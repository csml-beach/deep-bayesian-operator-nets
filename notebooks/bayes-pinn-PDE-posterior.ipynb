{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hBc_Jy8VUgOI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy.ma.core import outer\n",
    "from sys import stderr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import scatter, figure\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(4, 4)})\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "class Experiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Experiment, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.train_loss_history = []\n",
    "        self.w_ic = 1.5\n",
    "        self.w_int = 1\n",
    "        self.w_data = 1\n",
    "        self.w_bc = 1.5\n",
    "        self.w_param_std = 1.0\n",
    "        self.numInputs = 2\n",
    "        self.numParams = 2\n",
    "        self.numOutputs = 1\n",
    "        self.hidden_size = 25\n",
    "        \n",
    "        # Initialize history variables\n",
    "        self.total_loss_history = []\n",
    "        self.loss_ic_history = []\n",
    "        self.loss_interior_history = []\n",
    "        self.loss_data_history = []\n",
    "        self.loss_bc_history = []\n",
    "        self.loss_std_history = []\n",
    "\n",
    "        self.t0 = torch.tensor([0.0], requires_grad=True).to(self.device)\n",
    "        self.y0 = torch.tensor([1.0], requires_grad=True).to(self.device)\n",
    "\n",
    "        \n",
    "#         self.predicted_params = torch.zeros((self.numParams,1), requires_grad=True, device=self.device)\n",
    "        self.predicted_params = torch.zeros((100,2), requires_grad=True, device=self.device)\n",
    "#         print(\"predicted_params inside init\", self.predicted_params)\n",
    "\n",
    "        # We only have 1 input feature\n",
    "        self.b1 = nn.Linear(self.numInputs, self.hidden_size).to(self.device)\n",
    "        self.b2 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
    "        self.b3 = nn.Linear(self.hidden_size, self.numOutputs).to(self.device)\n",
    "\n",
    "        self.t1 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
    "        self.t2 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
    "        self.t3 = nn.Linear(self.hidden_size, self.numParams).to(self.device)\n",
    "\n",
    "\n",
    "    # make this static so that it can be called independently\n",
    "    @staticmethod\n",
    "    def exact_solution(t, x):\n",
    "        return torch.exp(-t) * torch.sin(torch.pi * x)\n",
    "\n",
    "    \n",
    "    def forward(self, t, x):\n",
    "        if not torch.is_tensor(t):\n",
    "            t = torch.from_numpy(t).float().to(self.device)\n",
    "\n",
    "        if not torch.is_tensor(x):\n",
    "\n",
    "            x = torch.from_numpy(x).float().to(self.device)\n",
    "\n",
    "\n",
    "        input = torch.cat((t, x), 1)\n",
    "\n",
    "        h1 = torch.tanh(self.b1(input))\n",
    "        h2 = torch.tanh(self.b2(h1))\n",
    "\n",
    "        prior_param_sample = torch.rand(1, self.hidden_size , device=self.device).requires_grad_(True)\n",
    "\n",
    "        t1 = torch.tanh(self.t1(prior_param_sample))\n",
    "        t2 = torch.tanh(self.t2(t1))\n",
    "        posterior_param_sample = self.t3(t2)\n",
    "        \n",
    "        print(\"posterior_param_sample: \", posterior_param_sample)\n",
    "        self.predicted_params = posterior_param_sample\n",
    "\n",
    "#         self.predicted_params = posterior_param_sample\n",
    "#         self.predicted_params = posterior_param_sample.view(-1)\n",
    "#         print(\"\\n predicted params inside forward\", self.predicted_params)\n",
    "\n",
    "        y = self.b3(torch.multiply(t2, h2))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    # General formula to compute the n-th order derivative of y = f(x) with respect to x\n",
    "    def compute_derivative(self, y, x, n):\n",
    "        if n == 0:\n",
    "            return y\n",
    "        else:\n",
    "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y, device= self.device),\n",
    "                                        create_graph=True, retain_graph=True,\n",
    "                                        allow_unused=True)[0]\n",
    "\n",
    "        return self.compute_derivative(dy_dx, x, n - 1)\n",
    "\n",
    "\n",
    "\n",
    "    def PDE_residual(self, t, x):\n",
    "        y = self.forward(t, x)\n",
    "        dy_dt = self.compute_derivative(y, t, 1)\n",
    "        dy_dx = self.compute_derivative(y, x, 1)\n",
    "        d2y_dx2 = self.compute_derivative(y, x, 2)\n",
    "        \n",
    "        print(\"\\n \\n\")\n",
    "        print(\"X shape:\" , x.shape)\n",
    "        print(\"Y shape:\" , y.shape)\n",
    "        print(\"dy_dt shape:\" , dy_dt.shape)\n",
    "        print(\"d2y_dx2 shape:\" , d2y_dx2.shape)\n",
    "        print(\"predicted params\", self.predicted_params.shape)\n",
    "#         print(\"self.predicted_params[:,[0]] shape:\" , self.predicted_params[:,[0]].shape)\n",
    "#         print(\"self.predicted_params[:,[0]]:\", self.predicted_params[:,[0]])\n",
    "     \n",
    "#         print(\"d2y_dx2 value :\", d2y_dx2.shape)\n",
    "#         print(\"predicted param :\", self.predicted_params[:,[0]].shape)\n",
    "        \n",
    "    \n",
    "        residual =  dy_dt - torch.multiply(self.predicted_params[:,[0]], d2y_dx2) \\\n",
    "            + torch.exp(- self.predicted_params[:,[1]] * t) * (torch.sin(torch.tensor(np.pi) * x) - torch.tensor(np.pi) ** 2 * torch.sin(torch.tensor(np.pi) * x))\n",
    "\n",
    "\n",
    "        \n",
    "        return residual\n",
    "\n",
    "    \n",
    "    \n",
    "    def loss_initial_condition(self, num_samples=100):\n",
    "        t0 = self.t0 * torch.ones((num_samples, 1), device = self.device)\n",
    "        x = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True)  - 1.0\n",
    "        y0_pred = self.forward(t0, x)\n",
    "        y0_true = self.exact_solution(t0, x)\n",
    "        loss_ic = torch.mean(torch.square(y0_true - y0_pred))\n",
    "        return loss_ic\n",
    "\n",
    "    \n",
    "    def loss_boundary_condition(self, num_samples=100):\n",
    "        x_low = -1\n",
    "        x_high = 1\n",
    "        xb_low   = x_low * torch.ones((num_samples, 1),  device = self.device)\n",
    "        xb_high  = x_high * torch.ones((num_samples, 1), device = self.device)\n",
    "\n",
    "        t = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True) - 1.0\n",
    "\n",
    "        yb_low = self.exact_solution(t, xb_low)\n",
    "        yb_high = self.exact_solution(t, xb_high)\n",
    "\n",
    "        yb_pred_low = self.forward(t, xb_low)\n",
    "        yb_pred_high = self.forward(t, xb_high)\n",
    "\n",
    "        loss_ic = torch.mean(torch.square(yb_pred_low - yb_low)) \\\n",
    "                + torch.mean(torch.square(yb_pred_high - yb_high))\n",
    "\n",
    "        return loss_ic\n",
    "\n",
    "    \n",
    "    def compute_losses(self):\n",
    "        loss_ic = self.loss_initial_condition()\n",
    "        loss_interior = self.loss_interior()\n",
    "        loss_data = self.loss_data()\n",
    "        loss_bc = self.loss_boundary_condition()\n",
    "        return loss_ic, loss_interior, loss_data, loss_bc\n",
    "\n",
    "    \n",
    "    def loss_data(self, num_samples=100):\n",
    "        t_data, x_data, y_data = next(iter(train_loader))\n",
    "        y_pred = self.forward(t_data, x_data)\n",
    "        loss = torch.mean(torch.square(y_pred - y_data))\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def loss_interior(self, num_samples=100):\n",
    "\n",
    "        interior_t_samples = torch.rand((num_samples, 1),   device=self.device).requires_grad_(True)\n",
    "        interior_x_samples = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True)  - 1.0\n",
    "        res = self.PDE_residual(interior_t_samples, interior_x_samples)\n",
    "        loss_residual = torch.mean(torch.square(res))\n",
    "        return loss_residual\n",
    "#         return res\n",
    "\n",
    "    \n",
    "    def sample_parameter_posterior(self, num_samples=100):\n",
    "        prior_param_samples = torch.rand(num_samples, self.hidden_size, device=self.device).requires_grad_(True)\n",
    "        t1 = torch.tanh(self.t1(prior_param_samples))\n",
    "        t2 = torch.tanh(self.t2(t1))\n",
    "        posterior_param_samples = self.t3(t2)\n",
    "#         print(\"\\n posterior samples before mean and std\", posterior_param_samples)\n",
    "#         print(\"\\n\")\n",
    "        return posterior_param_samples\n",
    "\n",
    "\n",
    "        \n",
    "    def update_predicted_params(self, posterior_samples):\n",
    "\n",
    "        mean = torch.mean(posterior_samples, dim=0)  # Compute the mean along the first axis\n",
    "        std = torch.std(posterior_samples, dim=0)    # Compute the standard deviation along the first axis\n",
    "\n",
    "        self.predicted_params = posterior_samples\n",
    "        self.mean_predicted_params = mean  # Store the mean\n",
    "        self.std_params = std  # Attach the standard deviation as an attribute\n",
    "        \n",
    "        \n",
    "    def closure(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_ic, loss_interior, loss_data, loss_bc = self.compute_losses()\n",
    "        total_loss = self.w_ic * loss_ic \\\n",
    "                   + self.w_int * loss_interior \\\n",
    "                   + self.w_data * loss_data \\\n",
    "                   + self.w_bc * loss_bc\n",
    "\n",
    "        # Sample the parameter posterior and update self.predicted_params and self.std_params\n",
    "        posterior_samples = self.sample_parameter_posterior(num_samples=100)\n",
    "        self.update_predicted_params(posterior_samples)\n",
    "        print(\"posterior samples in closure after update: \", self.predicted_params)\n",
    "\n",
    "        # Add the (Log(std))**2 term to the loss with the specified weight\n",
    "        self.log_std_squared_loss = self.w_param_std * torch.mean(torch.log(self.std_params)**2)\n",
    "        total_loss += self.log_std_squared_loss\n",
    "\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def train(self, epochs, optimizer='Adam', num_samples=100, **kwargs):\n",
    "        if optimizer == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "\n",
    "        elif optimizer == 'L-BFGS':\n",
    "            self.optimizer = torch.optim.LBFGS(self.parameters(), **kwargs)\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.step(self.closure)\n",
    "            if epoch % 1000 == 0:\n",
    "                loss_ic, loss_interior, loss_data, loss_bc = self.compute_losses()\n",
    "                total_loss = loss_ic + loss_interior + loss_data + loss_bc\n",
    "                \n",
    "                scheduler.step(total_loss)\n",
    "                \n",
    "                # Append losses to history\n",
    "                self.total_loss_history.append(total_loss.item())\n",
    "                self.loss_ic_history.append(loss_ic.item())\n",
    "                self.loss_interior_history.append(loss_interior.item())\n",
    "                self.loss_data_history.append(loss_data.item())\n",
    "                self.loss_bc_history.append(loss_bc.item())\n",
    "                self.loss_std_history.append(self.log_std_squared_loss.item())\n",
    "                \n",
    "#                 print(f'Epoch({optimizer}):{epoch},  Total Loss:{total_loss.item():.2f}  ' \\\n",
    "#                         f'PDE Loss:{loss_interior.item():.2f}  ' \\\n",
    "#                         f'BC Loss:{loss_bc.item():.2f}  ' \\\n",
    "#                         f'IC Loss: {loss_ic.item():.2f}  ' \\\n",
    "#                         f'Predicted Param:{self.predicted_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
    "#                         f'Std Params:{self.std_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
    "#                         f'Std Log Loss:{self.log_std_squared_loss.item():.2f}')\n",
    "                \n",
    "                print(f'Epoch({optimizer}):{epoch},  Total Loss:{total_loss.item():.2f}  ' \\\n",
    "                        f'PDE Loss:{loss_interior.item():.2f}  ' \\\n",
    "                        f'BC Loss:{loss_bc.item():.2f}  ' \\\n",
    "                        f'IC Loss: {loss_ic.item():.2f}  ' \\\n",
    "#                         f'Predicted Param:{self.predicted_params.detach().cpu().numpy()[0][0]:.2f}'\n",
    "                        f'Mean Predicted Param: {self.mean_predicted_params[0]:.2f} ' \\\n",
    "                        f'Std Params:{self.std_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
    "                        f'Std Log Loss:{self.log_std_squared_loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qktqyo9XUm1y"
   },
   "outputs": [],
   "source": [
    "def sample_dataset(noise=0.1, numSamples=100, device='cpu'):\n",
    "    t = torch.linspace(0, 1, numSamples, device=device) # Time domain\n",
    "    x = torch.linspace(-1, 1, numSamples, device=device) # Space domain\n",
    "\n",
    "    T, X   = torch.meshgrid(t, x) # Time-Space domain\n",
    "\n",
    "    y_true = Experiment.exact_solution(T, X)\n",
    "    T      = T.reshape(-1, 1) # Reshape to 2D to 1D\n",
    "    X      = X.reshape(-1, 1) # Resahpe to 2D to 1D\n",
    "\n",
    "    sample_mean = y_true.reshape(-1, 1)\n",
    "    sample_var  = noise * torch.ones_like(sample_mean)\n",
    "    Y_noisy     = torch.normal(sample_mean, sample_var)\n",
    "\n",
    "    return T, X, Y_noisy\n",
    "\n",
    "\n",
    "def create_train_test_datasets(device='cpu', batch_size = 32):\n",
    "    t_train, x_train, y_train = sample_dataset(noise=0.01, numSamples=10, device=device)\n",
    "    t_test, x_test, y_test = sample_dataset(noise=0.0, numSamples=100, device=device)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    size = int(t_test.size(0)**0.5)\n",
    "\n",
    "\n",
    "    plt.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
    "                 x_test.view(size, size).cpu().detach().numpy(),\n",
    "                 y_test.view(size, size).cpu().detach().numpy())\n",
    "    plt.xlabel('t'); plt.ylabel('x');\n",
    "    plt.colorbar()\n",
    "\n",
    "    train_dataset = TensorDataset(t_train, x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(t_test, x_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def plot_residuals(net, device, noise=0.0):\n",
    "    t_test, x_test, y_test = sample_dataset(noise=noise, device=device)\n",
    "    mu   = net.forward(t_test, x_test)\n",
    "\n",
    "    fig  = plt.figure(figsize=(6, 3))\n",
    "    ax1  = plt.subplot(121)\n",
    "    size = int(np.sqrt(x_test.size(0)))\n",
    "\n",
    "    # Calculate the residuals (difference between predictions and ground truth)\n",
    "    absDifference  = torch.abs(mu - y_test)\n",
    "    residuals      = absDifference.view(size, size).cpu().detach().numpy()\n",
    "\n",
    "    # Create a contour plot of the residuals\n",
    "    cax = ax1.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
    "                       x_test.view(size, size).cpu().detach().numpy(),\n",
    "                       residuals)\n",
    "\n",
    "    plt.colorbar(cax)\n",
    "    plt.xlabel('t');plt.ylabel('x')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_plot(net, device):\n",
    "    t_test, x_test, y_test = sample_dataset(noise=0.0, device=device)\n",
    "    mu = net.forward(t_test, x_test)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122, sharex=ax1)\n",
    "    size = int(np.sqrt(x_test.size(0)))\n",
    "\n",
    "    ax1.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
    "                 x_test.view(size, size).cpu().detach().numpy(),\n",
    "                 mu.view(size, size).cpu().detach().numpy())\n",
    "\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "\n",
    "    ax2.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
    "                 x_test.view(size, size).cpu().detach().numpy(),\n",
    "                 y_test.view(size, size).cpu().detach().numpy())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_history(net):\n",
    "    epochs = range(len(net.total_loss_history))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, net.total_loss_history, label='Total Loss')\n",
    "    plt.plot(epochs, net.loss_ic_history, label='IC Loss')\n",
    "    plt.plot(epochs, net.loss_interior_history, label='Interior Loss')\n",
    "    plt.plot(epochs, net.loss_data_history, label='Data Loss')\n",
    "    plt.plot(epochs, net.loss_bc_history, label='BC Loss')\n",
    "    plt.plot(epochs, net.loss_std_history, label='STD Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_loss_history_log(net):\n",
    "    epochs = range(len(net.total_loss_history))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.semilogy(epochs, net.total_loss_history, label='Total Loss')\n",
    "    plt.semilogy(epochs, net.loss_ic_history, label='IC Loss')\n",
    "    plt.semilogy(epochs, net.loss_interior_history, label='Interior Loss')\n",
    "    plt.semilogy(epochs, net.loss_data_history, label='Data Loss')\n",
    "    plt.semilogy(epochs, net.loss_bc_history, label='BC Loss')\n",
    "    plt.semilogy(epochs, net.loss_std_history, label='STD Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Loss Over Epochs (log scale)')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxdtBGzUUphv",
    "outputId": "5778e169-bbae-4f41-fbdc-1f96ed1951a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Params: 2103\n"
     ]
    }
   ],
   "source": [
    "net = Experiment()\n",
    "net.to(net.device)\n",
    "print(\"Params:\", sum(p.numel() for p in net.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "yvco7AmeUrOF",
    "outputId": "91677b60-7faf-4934-bb3e-964376527fab",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, test_loader = create_train_test_datasets(device = net.device, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ivyI-KUlUsBV"
   },
   "outputs": [],
   "source": [
    "net.w_ic = 1\n",
    "net.w_data = 10\n",
    "net.w_bc   = 1\n",
    "net.w_int = 5\n",
    "net.w_param_std = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pXJ12CAPUvSp",
    "outputId": "6d35b2c1-f71d-49dd-e5e5-4cb2c47d3ab5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior_param_sample:  tensor([[-0.0707, -0.1063]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[-0.1400, -0.1606]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[-0.1601, -0.2161]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[-0.0924, -0.1621]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[-0.0389, -0.0316]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[-0.1035, -0.1066],\n",
      "        [-0.1307, -0.1603],\n",
      "        [-0.0443, -0.0467],\n",
      "        [-0.1025, -0.1642],\n",
      "        [-0.0836, -0.1216],\n",
      "        [-0.1100, -0.1710],\n",
      "        [-0.1146, -0.1726],\n",
      "        [-0.1242, -0.1772],\n",
      "        [-0.1666, -0.2101],\n",
      "        [-0.1721, -0.2200],\n",
      "        [-0.1083, -0.1501],\n",
      "        [-0.0785, -0.1182],\n",
      "        [-0.1050, -0.1664],\n",
      "        [-0.1308, -0.2008],\n",
      "        [-0.0969, -0.1870],\n",
      "        [-0.1678, -0.1658],\n",
      "        [-0.2175, -0.2820],\n",
      "        [-0.1148, -0.1426],\n",
      "        [-0.1803, -0.2097],\n",
      "        [-0.1456, -0.1723],\n",
      "        [-0.1563, -0.1202],\n",
      "        [-0.1990, -0.2220],\n",
      "        [-0.0914, -0.1614],\n",
      "        [-0.1292, -0.1052],\n",
      "        [-0.1370, -0.1559],\n",
      "        [-0.1743, -0.2232],\n",
      "        [ 0.0085, -0.0292],\n",
      "        [-0.2203, -0.2804],\n",
      "        [-0.1937, -0.2070],\n",
      "        [-0.1769, -0.1937],\n",
      "        [-0.1327, -0.1685],\n",
      "        [-0.1692, -0.1607],\n",
      "        [-0.0938, -0.1889],\n",
      "        [-0.1183, -0.1412],\n",
      "        [-0.1738, -0.1817],\n",
      "        [-0.1657, -0.1870],\n",
      "        [-0.1643, -0.2025],\n",
      "        [-0.1596, -0.1886],\n",
      "        [-0.1844, -0.2050],\n",
      "        [-0.1573, -0.2025],\n",
      "        [-0.0314, -0.1049],\n",
      "        [-0.0124, -0.0169],\n",
      "        [-0.1710, -0.1925],\n",
      "        [-0.1792, -0.2051],\n",
      "        [-0.1064, -0.1490],\n",
      "        [-0.1479, -0.1573],\n",
      "        [-0.1099, -0.1481],\n",
      "        [-0.1677, -0.1946],\n",
      "        [-0.0467, -0.1048],\n",
      "        [-0.0666, -0.0681],\n",
      "        [-0.1436, -0.1315],\n",
      "        [-0.1016, -0.1555],\n",
      "        [-0.2050, -0.1957],\n",
      "        [-0.1128, -0.2037],\n",
      "        [-0.2162, -0.2494],\n",
      "        [-0.0515, -0.1392],\n",
      "        [-0.1148, -0.1273],\n",
      "        [-0.0237, -0.0117],\n",
      "        [-0.0423, -0.0714],\n",
      "        [-0.1214, -0.1606],\n",
      "        [-0.1729, -0.2240],\n",
      "        [-0.1705, -0.1878],\n",
      "        [-0.1694, -0.2179],\n",
      "        [-0.1233, -0.1771],\n",
      "        [-0.0892, -0.0987],\n",
      "        [-0.1243, -0.2110],\n",
      "        [-0.1220, -0.1505],\n",
      "        [-0.0950, -0.1512],\n",
      "        [-0.1349, -0.1419],\n",
      "        [-0.1816, -0.2015],\n",
      "        [-0.1452, -0.1596],\n",
      "        [-0.0998, -0.1181],\n",
      "        [-0.0632, -0.1404],\n",
      "        [-0.0472, -0.0899],\n",
      "        [-0.1076, -0.1406],\n",
      "        [-0.1019, -0.1351],\n",
      "        [-0.1341, -0.1209],\n",
      "        [-0.1320, -0.1477],\n",
      "        [-0.1974, -0.2027],\n",
      "        [-0.1870, -0.2563],\n",
      "        [-0.0805, -0.1378],\n",
      "        [-0.2288, -0.2653],\n",
      "        [-0.0999, -0.1139],\n",
      "        [-0.1130, -0.1714],\n",
      "        [-0.1140, -0.1392],\n",
      "        [-0.1592, -0.1829],\n",
      "        [-0.1306, -0.1283],\n",
      "        [-0.1810, -0.2329],\n",
      "        [-0.2109, -0.2253],\n",
      "        [-0.1193, -0.1228],\n",
      "        [-0.2001, -0.2177],\n",
      "        [-0.2508, -0.2508],\n",
      "        [-0.0506, -0.0476],\n",
      "        [-0.1341, -0.1691],\n",
      "        [-0.0752, -0.1136],\n",
      "        [-0.1350, -0.1457],\n",
      "        [-0.0824, -0.1324],\n",
      "        [-0.1256, -0.2100],\n",
      "        [-0.0902, -0.1500],\n",
      "        [-0.1016, -0.1359]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[-0.0289,  0.0493]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[-0.0110,  0.1020]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[0.0289, 0.0669]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.0811, 0.2172]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.0709, 0.1401]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "Epoch(Adam):0,  Total Loss:39.22  PDE Loss:38.42  BC Loss:0.05  IC Loss: 0.51  Mean Predicted Param: -0.13 Std Params:0.05  Std Log Loss:8.74\n",
      "posterior_param_sample:  tensor([[0.0097, 0.1698]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.0975, 0.1560]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[0.0253, 0.1057]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[-0.0338,  0.0440]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.0543, 0.1117]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[-0.0093,  0.0529],\n",
      "        [ 0.0269,  0.0987],\n",
      "        [ 0.0769,  0.2583],\n",
      "        [ 0.0106,  0.0825],\n",
      "        [ 0.0408,  0.1129],\n",
      "        [ 0.0799,  0.0790],\n",
      "        [ 0.1145,  0.1748],\n",
      "        [ 0.0704,  0.1003],\n",
      "        [-0.0436,  0.0503],\n",
      "        [ 0.0811,  0.1182],\n",
      "        [-0.0536,  0.0649],\n",
      "        [ 0.0600,  0.1265],\n",
      "        [ 0.1352,  0.1571],\n",
      "        [ 0.0488,  0.1570],\n",
      "        [ 0.1168,  0.2061],\n",
      "        [ 0.0377,  0.1064],\n",
      "        [ 0.0180,  0.1177],\n",
      "        [ 0.0197,  0.1004],\n",
      "        [-0.0357,  0.0353],\n",
      "        [-0.0014,  0.1369],\n",
      "        [-0.0432,  0.0418],\n",
      "        [-0.0489,  0.0173],\n",
      "        [-0.0512,  0.0157],\n",
      "        [ 0.0096,  0.1313],\n",
      "        [ 0.0789,  0.1876],\n",
      "        [ 0.0649,  0.1461],\n",
      "        [ 0.0876,  0.1375],\n",
      "        [ 0.0687,  0.1567],\n",
      "        [-0.0177,  0.0154],\n",
      "        [ 0.0745,  0.1563],\n",
      "        [ 0.0029,  0.0902],\n",
      "        [ 0.0350,  0.1057],\n",
      "        [ 0.0155,  0.1477],\n",
      "        [ 0.0098,  0.1102],\n",
      "        [ 0.0668,  0.0361],\n",
      "        [ 0.0982,  0.1540],\n",
      "        [ 0.0954,  0.1348],\n",
      "        [ 0.0934,  0.1464],\n",
      "        [ 0.1394,  0.2172],\n",
      "        [ 0.0587,  0.1526],\n",
      "        [ 0.0335,  0.0917],\n",
      "        [ 0.1155,  0.1327],\n",
      "        [-0.0442,  0.0208],\n",
      "        [ 0.0757,  0.0896],\n",
      "        [ 0.1333,  0.2028],\n",
      "        [-0.0579,  0.0356],\n",
      "        [ 0.1252,  0.1456],\n",
      "        [ 0.1207,  0.1417],\n",
      "        [ 0.0203,  0.1173],\n",
      "        [ 0.1428,  0.1772],\n",
      "        [-0.0267,  0.1189],\n",
      "        [ 0.1195,  0.1371],\n",
      "        [ 0.0887,  0.1646],\n",
      "        [ 0.0549,  0.1301],\n",
      "        [-0.0033,  0.1415],\n",
      "        [ 0.0691,  0.1552],\n",
      "        [ 0.0527,  0.1422],\n",
      "        [-0.0016,  0.1153],\n",
      "        [ 0.0241,  0.1048],\n",
      "        [ 0.0886,  0.1659],\n",
      "        [ 0.1163,  0.1380],\n",
      "        [ 0.0384,  0.1622],\n",
      "        [ 0.0455,  0.1577],\n",
      "        [ 0.0689,  0.1714],\n",
      "        [-0.0124,  0.0606],\n",
      "        [ 0.0713,  0.1691],\n",
      "        [ 0.0761,  0.1494],\n",
      "        [ 0.0659,  0.1627],\n",
      "        [ 0.1362,  0.1773],\n",
      "        [ 0.0284,  0.1144],\n",
      "        [ 0.0297,  0.1103],\n",
      "        [ 0.0415,  0.1125],\n",
      "        [-0.0392,  0.0416],\n",
      "        [ 0.0644,  0.1292],\n",
      "        [ 0.0609,  0.1075],\n",
      "        [ 0.0693,  0.1808],\n",
      "        [-0.0217,  0.0897],\n",
      "        [ 0.0928,  0.1696],\n",
      "        [ 0.0850,  0.1466],\n",
      "        [ 0.0551,  0.1207],\n",
      "        [-0.0155,  0.1119],\n",
      "        [ 0.0705,  0.0970],\n",
      "        [ 0.0792,  0.1068],\n",
      "        [ 0.0480,  0.1534],\n",
      "        [ 0.0355,  0.1674],\n",
      "        [ 0.0995,  0.1540],\n",
      "        [ 0.0898,  0.1756],\n",
      "        [ 0.0714,  0.2080],\n",
      "        [ 0.1457,  0.2014],\n",
      "        [ 0.1150,  0.2553],\n",
      "        [-0.0087,  0.1147],\n",
      "        [ 0.1464,  0.2190],\n",
      "        [-0.0111,  0.0601],\n",
      "        [-0.0264,  0.0709],\n",
      "        [ 0.0841,  0.1710],\n",
      "        [ 0.1038,  0.1800],\n",
      "        [ 0.0856,  0.1732],\n",
      "        [ 0.0225,  0.1481],\n",
      "        [-0.0170,  0.0231],\n",
      "        [ 0.0156,  0.0646]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.3289, 0.5380]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.3339, 0.4383]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[0.2822, 0.4894]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.2282, 0.4623]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.2697, 0.4167]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[0.3177, 0.5007],\n",
      "        [0.3096, 0.5223],\n",
      "        [0.3696, 0.5173],\n",
      "        [0.3059, 0.4488],\n",
      "        [0.2314, 0.4886],\n",
      "        [0.2555, 0.3910],\n",
      "        [0.2793, 0.4424],\n",
      "        [0.2144, 0.3755],\n",
      "        [0.3105, 0.4430],\n",
      "        [0.1020, 0.2878],\n",
      "        [0.2197, 0.4437],\n",
      "        [0.1804, 0.4166],\n",
      "        [0.2645, 0.4205],\n",
      "        [0.3667, 0.5246],\n",
      "        [0.2218, 0.4006],\n",
      "        [0.1406, 0.3549],\n",
      "        [0.3378, 0.5052],\n",
      "        [0.3118, 0.5503],\n",
      "        [0.1891, 0.3289],\n",
      "        [0.2670, 0.3840],\n",
      "        [0.2157, 0.4216],\n",
      "        [0.3727, 0.5511],\n",
      "        [0.3169, 0.5031],\n",
      "        [0.2014, 0.3828],\n",
      "        [0.1943, 0.4319],\n",
      "        [0.2559, 0.4550],\n",
      "        [0.0850, 0.3084],\n",
      "        [0.3225, 0.4714],\n",
      "        [0.3456, 0.4846],\n",
      "        [0.2696, 0.3973],\n",
      "        [0.2075, 0.3167],\n",
      "        [0.3203, 0.5043],\n",
      "        [0.1897, 0.3964],\n",
      "        [0.2545, 0.4400],\n",
      "        [0.2320, 0.4144],\n",
      "        [0.1990, 0.3866],\n",
      "        [0.3392, 0.5800],\n",
      "        [0.2484, 0.4609],\n",
      "        [0.2948, 0.5115],\n",
      "        [0.1771, 0.3662],\n",
      "        [0.2505, 0.4048],\n",
      "        [0.1015, 0.3102],\n",
      "        [0.3158, 0.4346],\n",
      "        [0.3469, 0.4983],\n",
      "        [0.3218, 0.5136],\n",
      "        [0.3219, 0.4413],\n",
      "        [0.2077, 0.4593],\n",
      "        [0.1830, 0.4517],\n",
      "        [0.2640, 0.4372],\n",
      "        [0.2956, 0.5012],\n",
      "        [0.2291, 0.3738],\n",
      "        [0.1818, 0.3705],\n",
      "        [0.2105, 0.3956],\n",
      "        [0.2842, 0.4574],\n",
      "        [0.1288, 0.3753],\n",
      "        [0.2426, 0.4054],\n",
      "        [0.3088, 0.5116],\n",
      "        [0.1323, 0.3544],\n",
      "        [0.2941, 0.4620],\n",
      "        [0.3799, 0.5160],\n",
      "        [0.2420, 0.4244],\n",
      "        [0.3144, 0.4427],\n",
      "        [0.3192, 0.4550],\n",
      "        [0.2747, 0.4278],\n",
      "        [0.3291, 0.4872],\n",
      "        [0.3670, 0.5359],\n",
      "        [0.2183, 0.4185],\n",
      "        [0.2092, 0.4550],\n",
      "        [0.3730, 0.5642],\n",
      "        [0.3728, 0.5575],\n",
      "        [0.2087, 0.4264],\n",
      "        [0.2047, 0.4266],\n",
      "        [0.1242, 0.3509],\n",
      "        [0.3563, 0.5131],\n",
      "        [0.2231, 0.4410],\n",
      "        [0.2729, 0.5571],\n",
      "        [0.1817, 0.3270],\n",
      "        [0.3532, 0.5320],\n",
      "        [0.3794, 0.5475],\n",
      "        [0.2100, 0.4512],\n",
      "        [0.2679, 0.5028],\n",
      "        [0.2877, 0.4481],\n",
      "        [0.3075, 0.5646],\n",
      "        [0.2264, 0.4120],\n",
      "        [0.3327, 0.5225],\n",
      "        [0.2023, 0.4545],\n",
      "        [0.2710, 0.4251],\n",
      "        [0.2161, 0.3979],\n",
      "        [0.3296, 0.4360],\n",
      "        [0.2083, 0.4158],\n",
      "        [0.2517, 0.4214],\n",
      "        [0.2548, 0.4253],\n",
      "        [0.2076, 0.3683],\n",
      "        [0.3264, 0.5150],\n",
      "        [0.1151, 0.3024],\n",
      "        [0.1694, 0.3675],\n",
      "        [0.2978, 0.4742],\n",
      "        [0.2215, 0.4017],\n",
      "        [0.3285, 0.5266],\n",
      "        [0.3172, 0.4728]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.5603, 0.8066]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.4206, 0.7068]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior_param_sample:  tensor([[0.4310, 0.6335]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.5836, 0.8437]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.4380, 0.7111]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[0.4870, 0.6832],\n",
      "        [0.4704, 0.7375],\n",
      "        [0.5332, 0.7953],\n",
      "        [0.4194, 0.6727],\n",
      "        [0.5121, 0.7786],\n",
      "        [0.6125, 0.9129],\n",
      "        [0.4917, 0.8106],\n",
      "        [0.5812, 0.8976],\n",
      "        [0.4414, 0.6974],\n",
      "        [0.4617, 0.6918],\n",
      "        [0.4597, 0.6875],\n",
      "        [0.5470, 0.8135],\n",
      "        [0.5099, 0.8191],\n",
      "        [0.5879, 0.8445],\n",
      "        [0.5068, 0.7429],\n",
      "        [0.4438, 0.7657],\n",
      "        [0.4916, 0.7646],\n",
      "        [0.4664, 0.7911],\n",
      "        [0.3511, 0.6774],\n",
      "        [0.4568, 0.6790],\n",
      "        [0.4086, 0.6894],\n",
      "        [0.5224, 0.8149],\n",
      "        [0.5206, 0.7374],\n",
      "        [0.4705, 0.7834],\n",
      "        [0.5133, 0.7221],\n",
      "        [0.4272, 0.7237],\n",
      "        [0.4395, 0.7132],\n",
      "        [0.6075, 0.7996],\n",
      "        [0.5544, 0.7908],\n",
      "        [0.4725, 0.7821],\n",
      "        [0.4797, 0.7553],\n",
      "        [0.4251, 0.6760],\n",
      "        [0.5855, 0.8467],\n",
      "        [0.5194, 0.8121],\n",
      "        [0.5700, 0.8100],\n",
      "        [0.4953, 0.8142],\n",
      "        [0.4573, 0.7755],\n",
      "        [0.3622, 0.7115],\n",
      "        [0.5200, 0.8094],\n",
      "        [0.3268, 0.5657],\n",
      "        [0.4384, 0.6894],\n",
      "        [0.6450, 0.9156],\n",
      "        [0.4267, 0.7691],\n",
      "        [0.3447, 0.6823],\n",
      "        [0.3954, 0.7421],\n",
      "        [0.6160, 0.8349],\n",
      "        [0.4321, 0.7764],\n",
      "        [0.5334, 0.8387],\n",
      "        [0.4246, 0.6982],\n",
      "        [0.3762, 0.5844],\n",
      "        [0.4728, 0.7754],\n",
      "        [0.4996, 0.7940],\n",
      "        [0.4291, 0.7895],\n",
      "        [0.4272, 0.6686],\n",
      "        [0.2683, 0.5355],\n",
      "        [0.5027, 0.7609],\n",
      "        [0.5066, 0.8224],\n",
      "        [0.4030, 0.7257],\n",
      "        [0.3780, 0.7176],\n",
      "        [0.5325, 0.8452],\n",
      "        [0.4937, 0.7934],\n",
      "        [0.4920, 0.7891],\n",
      "        [0.4310, 0.7059],\n",
      "        [0.3222, 0.6509],\n",
      "        [0.5010, 0.8029],\n",
      "        [0.3383, 0.6079],\n",
      "        [0.5348, 0.7925],\n",
      "        [0.3968, 0.6540],\n",
      "        [0.4841, 0.7509],\n",
      "        [0.5462, 0.7625],\n",
      "        [0.3240, 0.6106],\n",
      "        [0.4249, 0.7085],\n",
      "        [0.4834, 0.8023],\n",
      "        [0.5514, 0.8399],\n",
      "        [0.4418, 0.7500],\n",
      "        [0.6182, 0.8862],\n",
      "        [0.5650, 0.8012],\n",
      "        [0.4621, 0.7495],\n",
      "        [0.6232, 0.9114],\n",
      "        [0.5931, 0.8790],\n",
      "        [0.4214, 0.7113],\n",
      "        [0.3782, 0.6504],\n",
      "        [0.4623, 0.7588],\n",
      "        [0.4434, 0.7289],\n",
      "        [0.5189, 0.7669],\n",
      "        [0.5516, 0.8657],\n",
      "        [0.6047, 0.8923],\n",
      "        [0.6249, 0.8206],\n",
      "        [0.4021, 0.6474],\n",
      "        [0.4437, 0.7260],\n",
      "        [0.4981, 0.7696],\n",
      "        [0.5469, 0.9055],\n",
      "        [0.5624, 0.7704],\n",
      "        [0.5201, 0.7965],\n",
      "        [0.5079, 0.7913],\n",
      "        [0.5163, 0.8225],\n",
      "        [0.5237, 0.8165],\n",
      "        [0.5637, 0.7898],\n",
      "        [0.6562, 0.9128],\n",
      "        [0.4602, 0.7422]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.7840, 1.1265]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.7690, 1.1151]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[0.7763, 1.1415]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.7324, 1.1776]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.7906, 1.1327]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[0.7521, 1.1745],\n",
      "        [0.7157, 1.1394],\n",
      "        [0.7925, 1.2122],\n",
      "        [0.8338, 1.1606],\n",
      "        [0.7568, 1.1510],\n",
      "        [0.8210, 1.1920],\n",
      "        [0.7325, 1.1671],\n",
      "        [0.6617, 1.0368],\n",
      "        [0.7717, 1.1776],\n",
      "        [0.8085, 1.1683],\n",
      "        [0.6752, 1.0173],\n",
      "        [0.8348, 1.1150],\n",
      "        [0.6557, 0.9023],\n",
      "        [0.7200, 1.0942],\n",
      "        [0.6765, 1.0263],\n",
      "        [0.6628, 1.0405],\n",
      "        [0.8013, 1.2030],\n",
      "        [0.7840, 1.0542],\n",
      "        [0.7798, 1.0585],\n",
      "        [0.7510, 1.1659],\n",
      "        [0.8263, 1.2166],\n",
      "        [0.8458, 1.2460],\n",
      "        [0.6347, 0.9615],\n",
      "        [0.7653, 1.1183],\n",
      "        [0.8428, 1.1793],\n",
      "        [0.7530, 1.1458],\n",
      "        [0.6324, 1.0187],\n",
      "        [0.7852, 1.1326],\n",
      "        [0.6764, 0.9404],\n",
      "        [0.8117, 1.1993],\n",
      "        [0.7704, 1.1836],\n",
      "        [0.7665, 1.1136],\n",
      "        [0.7125, 1.1048],\n",
      "        [0.6700, 1.1020],\n",
      "        [0.7034, 1.1402],\n",
      "        [0.8681, 1.2370],\n",
      "        [0.7721, 1.1472],\n",
      "        [0.8472, 1.1609],\n",
      "        [0.8211, 1.2390],\n",
      "        [0.7630, 1.0994],\n",
      "        [0.7182, 1.0412],\n",
      "        [0.7316, 1.1626],\n",
      "        [0.7975, 1.1278],\n",
      "        [0.8645, 1.2326],\n",
      "        [0.7410, 1.1599],\n",
      "        [0.8632, 1.2618],\n",
      "        [0.6974, 1.0296],\n",
      "        [0.7375, 1.1365],\n",
      "        [0.7508, 1.1409],\n",
      "        [0.8292, 1.1942],\n",
      "        [0.7064, 0.9536],\n",
      "        [0.8493, 1.2262],\n",
      "        [0.7314, 1.0891],\n",
      "        [0.7091, 1.1226],\n",
      "        [0.8028, 1.1937],\n",
      "        [0.7100, 1.1226],\n",
      "        [0.7558, 1.0514],\n",
      "        [0.8124, 1.1547],\n",
      "        [0.7603, 1.1537],\n",
      "        [0.8291, 1.2154],\n",
      "        [0.8651, 1.1945],\n",
      "        [0.7596, 1.1978],\n",
      "        [0.7213, 1.1492],\n",
      "        [0.8827, 1.3009],\n",
      "        [0.9096, 1.2653],\n",
      "        [0.7711, 1.1563],\n",
      "        [0.6818, 1.0794],\n",
      "        [0.8550, 1.2396],\n",
      "        [0.7359, 1.1021],\n",
      "        [0.7266, 1.0613],\n",
      "        [0.7933, 1.1872],\n",
      "        [0.6191, 1.0288],\n",
      "        [0.7529, 1.1625],\n",
      "        [0.7587, 1.1399],\n",
      "        [0.6499, 0.9949],\n",
      "        [0.7586, 1.1851],\n",
      "        [0.8062, 1.0965],\n",
      "        [0.7558, 1.1032],\n",
      "        [0.7835, 1.1459],\n",
      "        [0.8559, 1.2053],\n",
      "        [0.8420, 1.2427],\n",
      "        [0.7282, 0.9991],\n",
      "        [0.8301, 1.2439],\n",
      "        [0.7535, 1.1692],\n",
      "        [0.5994, 0.8894],\n",
      "        [0.6882, 1.0780],\n",
      "        [0.6563, 0.9933],\n",
      "        [0.7057, 1.0998],\n",
      "        [0.8068, 1.1700],\n",
      "        [0.6849, 0.9368],\n",
      "        [0.8499, 1.2752],\n",
      "        [0.8415, 1.2711],\n",
      "        [0.6619, 1.0654],\n",
      "        [0.7587, 1.1279],\n",
      "        [0.7818, 1.1904],\n",
      "        [0.8652, 1.3157],\n",
      "        [0.8129, 1.1834],\n",
      "        [0.7146, 1.1582],\n",
      "        [0.8395, 1.1724],\n",
      "        [0.8879, 1.2989]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.1184, 1.6053]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[0.8523, 1.2852]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[1.0695, 1.5168]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.0449, 1.5140]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.0084, 1.4180]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[0.9563, 1.4661],\n",
      "        [1.0729, 1.5904],\n",
      "        [1.0516, 1.5051],\n",
      "        [1.0314, 1.4851],\n",
      "        [0.9711, 1.4501],\n",
      "        [1.0941, 1.5392],\n",
      "        [1.1496, 1.6022],\n",
      "        [1.0451, 1.5043],\n",
      "        [0.9920, 1.4860],\n",
      "        [0.8522, 1.2990],\n",
      "        [1.0140, 1.5028],\n",
      "        [1.0993, 1.6241],\n",
      "        [1.0707, 1.4654],\n",
      "        [0.9672, 1.4540],\n",
      "        [0.9924, 1.4739],\n",
      "        [1.0923, 1.5130],\n",
      "        [0.9513, 1.3776],\n",
      "        [1.0034, 1.4212],\n",
      "        [1.0517, 1.5638],\n",
      "        [0.9834, 1.4920],\n",
      "        [1.0936, 1.6024],\n",
      "        [1.1713, 1.7030],\n",
      "        [1.0218, 1.4615],\n",
      "        [1.0886, 1.5120],\n",
      "        [0.9891, 1.5049],\n",
      "        [1.0480, 1.5164],\n",
      "        [1.0347, 1.4242],\n",
      "        [1.1102, 1.5725],\n",
      "        [1.0082, 1.4259],\n",
      "        [1.1187, 1.5849],\n",
      "        [1.1060, 1.5323],\n",
      "        [1.1914, 1.5748],\n",
      "        [0.9858, 1.4414],\n",
      "        [1.0965, 1.6064],\n",
      "        [1.0987, 1.5998],\n",
      "        [1.0361, 1.5126],\n",
      "        [1.2068, 1.6539],\n",
      "        [1.0681, 1.5964],\n",
      "        [1.1086, 1.5004],\n",
      "        [1.1576, 1.5759],\n",
      "        [1.1147, 1.5048],\n",
      "        [0.9610, 1.4158],\n",
      "        [1.0015, 1.4920],\n",
      "        [1.1558, 1.5931],\n",
      "        [1.0287, 1.5127],\n",
      "        [1.0957, 1.5298],\n",
      "        [1.0795, 1.4364],\n",
      "        [1.0940, 1.5673],\n",
      "        [1.1078, 1.5924],\n",
      "        [1.1104, 1.5427],\n",
      "        [1.1322, 1.6241],\n",
      "        [1.0109, 1.4476],\n",
      "        [1.0150, 1.4182],\n",
      "        [1.0347, 1.5242],\n",
      "        [1.1267, 1.5943],\n",
      "        [1.0286, 1.3724],\n",
      "        [1.0790, 1.5784],\n",
      "        [1.1023, 1.5548],\n",
      "        [1.1448, 1.6042],\n",
      "        [0.9475, 1.3752],\n",
      "        [1.0235, 1.5090],\n",
      "        [0.9671, 1.4341],\n",
      "        [0.8300, 1.2243],\n",
      "        [0.9852, 1.4664],\n",
      "        [0.9835, 1.4377],\n",
      "        [1.0377, 1.5363],\n",
      "        [0.9524, 1.4300],\n",
      "        [1.0242, 1.4873],\n",
      "        [1.0988, 1.6090],\n",
      "        [0.8992, 1.3844],\n",
      "        [0.9599, 1.4198],\n",
      "        [0.9535, 1.4494],\n",
      "        [0.9781, 1.4324],\n",
      "        [1.0744, 1.5883],\n",
      "        [0.9863, 1.4199],\n",
      "        [1.0185, 1.5140],\n",
      "        [1.1303, 1.5809],\n",
      "        [1.0007, 1.4875],\n",
      "        [1.1487, 1.6057],\n",
      "        [1.1555, 1.5627],\n",
      "        [1.0312, 1.4532],\n",
      "        [1.1129, 1.5914],\n",
      "        [0.8916, 1.4262],\n",
      "        [1.0180, 1.4201],\n",
      "        [0.9563, 1.4250],\n",
      "        [1.1197, 1.5508],\n",
      "        [1.0314, 1.4237],\n",
      "        [1.0997, 1.5147],\n",
      "        [0.9645, 1.4720],\n",
      "        [1.1131, 1.5757],\n",
      "        [1.0133, 1.4777],\n",
      "        [1.1729, 1.6251],\n",
      "        [0.9737, 1.4254],\n",
      "        [1.0711, 1.5705],\n",
      "        [0.8305, 1.3460],\n",
      "        [1.0709, 1.5177],\n",
      "        [1.0631, 1.5195],\n",
      "        [1.1389, 1.5956],\n",
      "        [0.9490, 1.4081],\n",
      "        [0.9724, 1.4501]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior_param_sample:  tensor([[1.4383, 1.9785]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.4024, 1.9114]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[1.4467, 1.9663]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.3858, 1.8774]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.3138, 1.7322]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[1.3762, 1.9345],\n",
      "        [1.2387, 1.8114],\n",
      "        [1.1787, 1.7494],\n",
      "        [1.3627, 1.8564],\n",
      "        [1.3281, 1.8626],\n",
      "        [1.3522, 1.9205],\n",
      "        [1.3472, 1.8782],\n",
      "        [1.3505, 1.8425],\n",
      "        [1.3083, 1.8019],\n",
      "        [1.3188, 1.8383],\n",
      "        [1.4558, 1.9810],\n",
      "        [1.3534, 1.9213],\n",
      "        [1.3357, 1.8490],\n",
      "        [1.4051, 1.8917],\n",
      "        [1.4169, 1.9200],\n",
      "        [1.3655, 1.9281],\n",
      "        [1.4301, 1.9497],\n",
      "        [1.3555, 1.8923],\n",
      "        [1.4404, 1.9488],\n",
      "        [1.2922, 1.8040],\n",
      "        [1.2517, 1.6614],\n",
      "        [1.4841, 2.0723],\n",
      "        [1.3860, 1.9055],\n",
      "        [1.2944, 1.8213],\n",
      "        [1.3416, 1.8475],\n",
      "        [1.2647, 1.7452],\n",
      "        [1.4561, 2.0367],\n",
      "        [1.3687, 1.8933],\n",
      "        [1.4141, 1.9696],\n",
      "        [1.4393, 1.9624],\n",
      "        [1.3403, 1.8997],\n",
      "        [1.3870, 1.9067],\n",
      "        [1.3648, 1.8345],\n",
      "        [1.3417, 1.8923],\n",
      "        [1.3405, 1.8822],\n",
      "        [1.4467, 1.9880],\n",
      "        [1.1912, 1.7166],\n",
      "        [1.4444, 1.9930],\n",
      "        [1.4186, 1.8837],\n",
      "        [1.3979, 1.9010],\n",
      "        [1.2907, 1.8111],\n",
      "        [1.3944, 1.9036],\n",
      "        [1.3763, 1.9111],\n",
      "        [1.3762, 1.8919],\n",
      "        [1.4241, 1.9136],\n",
      "        [1.3040, 1.7799],\n",
      "        [1.2397, 1.7560],\n",
      "        [1.2366, 1.7386],\n",
      "        [1.2858, 1.8322],\n",
      "        [1.3753, 1.8525],\n",
      "        [1.3740, 1.8075],\n",
      "        [1.3678, 1.9255],\n",
      "        [1.4047, 1.9047],\n",
      "        [1.4250, 1.9372],\n",
      "        [1.3676, 1.8292],\n",
      "        [1.2094, 1.6901],\n",
      "        [1.4330, 1.9792],\n",
      "        [1.2976, 1.7434],\n",
      "        [1.4538, 1.9904],\n",
      "        [1.2909, 1.7650],\n",
      "        [1.2965, 1.8061],\n",
      "        [1.3621, 1.9022],\n",
      "        [1.4046, 1.8474],\n",
      "        [1.3766, 1.9062],\n",
      "        [1.2405, 1.7864],\n",
      "        [1.3458, 1.8025],\n",
      "        [1.3812, 1.9569],\n",
      "        [1.4664, 2.0221],\n",
      "        [1.3205, 1.8444],\n",
      "        [1.3365, 1.8370],\n",
      "        [1.3205, 1.8539],\n",
      "        [1.3994, 1.9231],\n",
      "        [1.2748, 1.8416],\n",
      "        [1.3540, 1.8793],\n",
      "        [1.4158, 1.9266],\n",
      "        [1.4454, 1.9825],\n",
      "        [1.4180, 1.9542],\n",
      "        [1.4368, 1.9827],\n",
      "        [1.4193, 1.9424],\n",
      "        [1.2545, 1.6926],\n",
      "        [1.4166, 1.8987],\n",
      "        [1.3436, 1.8836],\n",
      "        [1.4090, 1.8920],\n",
      "        [1.4282, 1.9298],\n",
      "        [1.2767, 1.8545],\n",
      "        [1.4429, 1.9947],\n",
      "        [1.3662, 1.8856],\n",
      "        [1.2631, 1.8178],\n",
      "        [1.3427, 1.8378],\n",
      "        [1.3807, 1.8457],\n",
      "        [1.4527, 2.0180],\n",
      "        [1.2645, 1.8224],\n",
      "        [1.2808, 1.6686],\n",
      "        [1.2273, 1.8148],\n",
      "        [1.3529, 1.8854],\n",
      "        [1.3474, 1.8668],\n",
      "        [1.3957, 1.9093],\n",
      "        [1.3514, 1.8433],\n",
      "        [1.4104, 1.9264],\n",
      "        [1.3857, 1.9380]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.6927, 2.2064]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.7577, 2.3348]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[1.7263, 2.2908]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.6890, 2.2274]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.6616, 2.2174]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[1.6915, 2.2269],\n",
      "        [1.7579, 2.2855],\n",
      "        [1.5414, 2.0211],\n",
      "        [1.5855, 2.1510],\n",
      "        [1.7031, 2.2334],\n",
      "        [1.6609, 2.1900],\n",
      "        [1.6367, 2.1644],\n",
      "        [1.6648, 2.2864],\n",
      "        [1.5164, 2.0339],\n",
      "        [1.7212, 2.3119],\n",
      "        [1.6892, 2.2352],\n",
      "        [1.4213, 1.9689],\n",
      "        [1.6670, 2.1652],\n",
      "        [1.6472, 2.1958],\n",
      "        [1.6599, 2.1676],\n",
      "        [1.5087, 2.0344],\n",
      "        [1.6725, 2.2117],\n",
      "        [1.5551, 2.0032],\n",
      "        [1.6523, 2.1530],\n",
      "        [1.6297, 2.1603],\n",
      "        [1.7504, 2.2934],\n",
      "        [1.7149, 2.3234],\n",
      "        [1.7515, 2.2914],\n",
      "        [1.5031, 2.0488],\n",
      "        [1.7033, 2.2224],\n",
      "        [1.6851, 2.2143],\n",
      "        [1.6177, 2.1627],\n",
      "        [1.6889, 2.2837],\n",
      "        [1.6688, 2.1576],\n",
      "        [1.7256, 2.2547],\n",
      "        [1.6901, 2.2441],\n",
      "        [1.7066, 2.2852],\n",
      "        [1.6720, 2.2070],\n",
      "        [1.4897, 2.0377],\n",
      "        [1.6584, 2.2285],\n",
      "        [1.7085, 2.2836],\n",
      "        [1.5961, 2.1016],\n",
      "        [1.7178, 2.2653],\n",
      "        [1.7033, 2.2682],\n",
      "        [1.7239, 2.2695],\n",
      "        [1.7113, 2.2345],\n",
      "        [1.6089, 2.2037],\n",
      "        [1.5463, 2.1401],\n",
      "        [1.6039, 2.1826],\n",
      "        [1.6845, 2.2454],\n",
      "        [1.7423, 2.2897],\n",
      "        [1.6618, 2.2165],\n",
      "        [1.6389, 2.2226],\n",
      "        [1.6144, 2.1811],\n",
      "        [1.6050, 2.1355],\n",
      "        [1.6108, 2.1639],\n",
      "        [1.5273, 2.0496],\n",
      "        [1.7163, 2.2926],\n",
      "        [1.7283, 2.2591],\n",
      "        [1.6753, 2.2422],\n",
      "        [1.6992, 2.2446],\n",
      "        [1.5774, 2.1682],\n",
      "        [1.5767, 2.1591],\n",
      "        [1.6152, 2.1719],\n",
      "        [1.5979, 2.1397],\n",
      "        [1.7417, 2.2409],\n",
      "        [1.6919, 2.2542],\n",
      "        [1.6778, 2.2484],\n",
      "        [1.5697, 2.1023],\n",
      "        [1.7146, 2.3275],\n",
      "        [1.6082, 2.1326],\n",
      "        [1.6533, 2.1455],\n",
      "        [1.6659, 2.2195],\n",
      "        [1.5089, 2.0578],\n",
      "        [1.6718, 2.2222],\n",
      "        [1.6277, 2.1998],\n",
      "        [1.6814, 2.2220],\n",
      "        [1.6948, 2.1929],\n",
      "        [1.6493, 2.2035],\n",
      "        [1.7043, 2.2587],\n",
      "        [1.6975, 2.2500],\n",
      "        [1.5324, 1.9853],\n",
      "        [1.6955, 2.2619],\n",
      "        [1.6547, 2.2293],\n",
      "        [1.6387, 2.2412],\n",
      "        [1.7146, 2.2560],\n",
      "        [1.6795, 2.2353],\n",
      "        [1.5652, 2.1288],\n",
      "        [1.7184, 2.2910],\n",
      "        [1.6415, 2.0896],\n",
      "        [1.6626, 2.2522],\n",
      "        [1.6319, 2.1111],\n",
      "        [1.6756, 2.2203],\n",
      "        [1.6235, 2.1482],\n",
      "        [1.6020, 2.1896],\n",
      "        [1.5775, 2.0921],\n",
      "        [1.6590, 2.2049],\n",
      "        [1.4829, 2.0733],\n",
      "        [1.6544, 2.1805],\n",
      "        [1.6335, 2.2082],\n",
      "        [1.7305, 2.2457],\n",
      "        [1.5793, 2.1215],\n",
      "        [1.7151, 2.2888],\n",
      "        [1.6511, 2.1795],\n",
      "        [1.5053, 2.0714]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.9352, 2.4353]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.8782, 2.4053]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[2.0099, 2.5486]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.0138, 2.5465]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[1.8982, 2.4820]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[1.9922, 2.5108],\n",
      "        [1.9585, 2.4967],\n",
      "        [1.9760, 2.4940],\n",
      "        [1.9999, 2.5617],\n",
      "        [1.8785, 2.4099],\n",
      "        [1.9784, 2.5102],\n",
      "        [1.8825, 2.3657],\n",
      "        [2.0151, 2.5561],\n",
      "        [1.9477, 2.5417],\n",
      "        [1.9377, 2.5055],\n",
      "        [1.9858, 2.5071],\n",
      "        [1.8469, 2.3675],\n",
      "        [1.9191, 2.4793],\n",
      "        [1.8927, 2.4693],\n",
      "        [1.9619, 2.4633],\n",
      "        [1.9018, 2.4178],\n",
      "        [1.8354, 2.3690],\n",
      "        [2.0186, 2.6040],\n",
      "        [1.9260, 2.4542],\n",
      "        [1.8428, 2.3833],\n",
      "        [1.8952, 2.4047],\n",
      "        [1.8209, 2.4109],\n",
      "        [2.0242, 2.5913],\n",
      "        [1.7919, 2.3451],\n",
      "        [2.0050, 2.5277],\n",
      "        [1.9914, 2.5266],\n",
      "        [1.9601, 2.5287],\n",
      "        [2.0053, 2.5422],\n",
      "        [1.9617, 2.4711],\n",
      "        [2.0184, 2.5822],\n",
      "        [2.0218, 2.5608],\n",
      "        [1.9556, 2.4956],\n",
      "        [1.9890, 2.5617],\n",
      "        [2.0050, 2.5657],\n",
      "        [1.9777, 2.5365],\n",
      "        [1.9265, 2.4912],\n",
      "        [1.9995, 2.5590],\n",
      "        [1.8755, 2.3995],\n",
      "        [1.9473, 2.5027],\n",
      "        [2.0227, 2.5507],\n",
      "        [1.9350, 2.5085],\n",
      "        [1.9840, 2.4863],\n",
      "        [2.0739, 2.6277],\n",
      "        [1.9764, 2.5009],\n",
      "        [1.9981, 2.4833],\n",
      "        [1.9500, 2.4931],\n",
      "        [1.8981, 2.4409],\n",
      "        [1.9259, 2.4630],\n",
      "        [2.0008, 2.5533],\n",
      "        [1.8698, 2.4454],\n",
      "        [2.0237, 2.5744],\n",
      "        [1.7959, 2.3462],\n",
      "        [1.9336, 2.4973],\n",
      "        [1.9558, 2.4558],\n",
      "        [1.8824, 2.4355],\n",
      "        [1.9952, 2.5241],\n",
      "        [1.8644, 2.4049],\n",
      "        [2.0165, 2.5984],\n",
      "        [2.0135, 2.5549],\n",
      "        [2.0021, 2.5701],\n",
      "        [1.9637, 2.5101],\n",
      "        [1.9563, 2.5065],\n",
      "        [1.9593, 2.4732],\n",
      "        [2.0157, 2.5141],\n",
      "        [1.9499, 2.4942],\n",
      "        [2.0068, 2.5872],\n",
      "        [1.9342, 2.4532],\n",
      "        [1.9608, 2.5223],\n",
      "        [1.9837, 2.5333],\n",
      "        [2.0086, 2.5418],\n",
      "        [2.0250, 2.5722],\n",
      "        [1.8578, 2.4205],\n",
      "        [1.9861, 2.5377],\n",
      "        [1.8791, 2.4600],\n",
      "        [2.0274, 2.5853],\n",
      "        [2.0041, 2.5881],\n",
      "        [1.9850, 2.5518],\n",
      "        [1.9998, 2.5036],\n",
      "        [2.0077, 2.5933],\n",
      "        [1.9880, 2.5340],\n",
      "        [1.8607, 2.4230],\n",
      "        [1.9248, 2.4206],\n",
      "        [1.9973, 2.5542],\n",
      "        [1.9200, 2.5052],\n",
      "        [1.8989, 2.4181],\n",
      "        [1.9493, 2.4875],\n",
      "        [1.9637, 2.4738],\n",
      "        [1.9254, 2.4805],\n",
      "        [1.8854, 2.4580],\n",
      "        [2.0342, 2.5867],\n",
      "        [1.9668, 2.5269],\n",
      "        [2.0662, 2.5648],\n",
      "        [1.9764, 2.4706],\n",
      "        [1.9074, 2.3623],\n",
      "        [2.0717, 2.5781],\n",
      "        [1.9840, 2.5298],\n",
      "        [2.0248, 2.5829],\n",
      "        [1.6880, 2.1707],\n",
      "        [1.8793, 2.4274],\n",
      "        [1.8929, 2.3465]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.2506, 2.6964]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.3078, 2.8211]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[2.2732, 2.7903]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.2970, 2.8339]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.2985, 2.8535]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[2.2536, 2.7825],\n",
      "        [2.3125, 2.8278],\n",
      "        [2.2894, 2.8097],\n",
      "        [2.2498, 2.7834],\n",
      "        [2.1768, 2.6984],\n",
      "        [2.2378, 2.7482],\n",
      "        [2.2882, 2.7960],\n",
      "        [2.2553, 2.7788],\n",
      "        [2.0785, 2.5755],\n",
      "        [2.2870, 2.8353],\n",
      "        [2.2861, 2.7999],\n",
      "        [2.2941, 2.8265],\n",
      "        [2.0710, 2.6334],\n",
      "        [2.1497, 2.6656],\n",
      "        [2.1712, 2.6870],\n",
      "        [2.1699, 2.7178],\n",
      "        [2.2755, 2.7741],\n",
      "        [2.3292, 2.8565],\n",
      "        [2.2812, 2.8209],\n",
      "        [2.2369, 2.7822],\n",
      "        [2.2708, 2.8448],\n",
      "        [2.2193, 2.7612],\n",
      "        [2.2760, 2.8230],\n",
      "        [2.1742, 2.7037],\n",
      "        [2.2280, 2.7866],\n",
      "        [2.2791, 2.8057],\n",
      "        [2.2586, 2.7699],\n",
      "        [2.3277, 2.8096],\n",
      "        [2.1767, 2.6966],\n",
      "        [2.2980, 2.8176],\n",
      "        [2.2705, 2.8050],\n",
      "        [2.2116, 2.7412],\n",
      "        [2.2723, 2.7915],\n",
      "        [2.2672, 2.7565],\n",
      "        [2.2018, 2.7526],\n",
      "        [2.1705, 2.6929],\n",
      "        [2.2848, 2.7701],\n",
      "        [2.2082, 2.7522],\n",
      "        [2.2914, 2.7931],\n",
      "        [2.2687, 2.7890],\n",
      "        [2.3014, 2.8282],\n",
      "        [2.2531, 2.6835],\n",
      "        [2.2882, 2.8180],\n",
      "        [2.2415, 2.7872],\n",
      "        [2.2874, 2.7888],\n",
      "        [2.2878, 2.8006],\n",
      "        [2.2728, 2.8217],\n",
      "        [2.2317, 2.7890],\n",
      "        [2.3385, 2.8404],\n",
      "        [2.3275, 2.8326],\n",
      "        [2.3152, 2.7933],\n",
      "        [2.2958, 2.8315],\n",
      "        [2.2323, 2.7232],\n",
      "        [2.2528, 2.7656],\n",
      "        [2.3222, 2.8421],\n",
      "        [2.2579, 2.8174],\n",
      "        [2.2364, 2.7785],\n",
      "        [2.2670, 2.7578],\n",
      "        [2.2186, 2.7731],\n",
      "        [2.3249, 2.8580],\n",
      "        [2.0285, 2.5898],\n",
      "        [2.2659, 2.8181],\n",
      "        [2.1991, 2.6901],\n",
      "        [2.2278, 2.7537],\n",
      "        [2.2747, 2.7379],\n",
      "        [2.2455, 2.7588],\n",
      "        [2.2377, 2.7592],\n",
      "        [2.1675, 2.7137],\n",
      "        [2.1664, 2.7203],\n",
      "        [2.2847, 2.8224],\n",
      "        [2.2974, 2.8275],\n",
      "        [2.2790, 2.7647],\n",
      "        [2.3182, 2.8556],\n",
      "        [2.2234, 2.7617],\n",
      "        [2.1661, 2.6918],\n",
      "        [2.2452, 2.7950],\n",
      "        [2.1524, 2.6603],\n",
      "        [2.3060, 2.8207],\n",
      "        [2.2427, 2.7589],\n",
      "        [2.1678, 2.7218],\n",
      "        [2.1602, 2.7078],\n",
      "        [2.2729, 2.8257],\n",
      "        [2.2606, 2.7936],\n",
      "        [2.2414, 2.7595],\n",
      "        [2.1182, 2.6512],\n",
      "        [2.1846, 2.7214],\n",
      "        [2.2032, 2.6891],\n",
      "        [2.2312, 2.7840],\n",
      "        [2.3283, 2.8565],\n",
      "        [2.2076, 2.7133],\n",
      "        [2.2675, 2.8230],\n",
      "        [2.1970, 2.7366],\n",
      "        [2.2693, 2.7721],\n",
      "        [2.1333, 2.6663],\n",
      "        [2.2699, 2.7991],\n",
      "        [2.2898, 2.8257],\n",
      "        [2.2572, 2.7924],\n",
      "        [2.3152, 2.8468],\n",
      "        [2.1963, 2.7336],\n",
      "        [2.2020, 2.7684]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior_param_sample:  tensor([[2.5955, 3.0810]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.5231, 2.9737]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[2.6032, 3.0685]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.6054, 3.0819]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.5551, 2.9687]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[2.6042, 3.0857],\n",
      "        [2.5564, 3.0276],\n",
      "        [2.5461, 2.9967],\n",
      "        [2.5807, 3.0333],\n",
      "        [2.5266, 2.9821],\n",
      "        [2.5448, 3.0064],\n",
      "        [2.6108, 3.0842],\n",
      "        [2.5701, 3.0450],\n",
      "        [2.6125, 3.0734],\n",
      "        [2.4296, 2.9412],\n",
      "        [2.5822, 3.0173],\n",
      "        [2.4454, 2.9559],\n",
      "        [2.4450, 2.9333],\n",
      "        [2.5379, 3.0481],\n",
      "        [2.5719, 3.0713],\n",
      "        [2.5933, 3.0476],\n",
      "        [2.5974, 3.0785],\n",
      "        [2.6107, 3.0924],\n",
      "        [2.5436, 3.0363],\n",
      "        [2.5432, 3.0027],\n",
      "        [2.6212, 3.0860],\n",
      "        [2.6015, 3.0245],\n",
      "        [2.3764, 2.7800],\n",
      "        [2.4901, 2.9795],\n",
      "        [2.5436, 3.0326],\n",
      "        [2.5387, 2.9787],\n",
      "        [2.5759, 3.0509],\n",
      "        [2.4744, 2.9564],\n",
      "        [2.5493, 3.0288],\n",
      "        [2.5605, 3.0198],\n",
      "        [2.6041, 3.0789],\n",
      "        [2.5153, 2.9678],\n",
      "        [2.5247, 3.0375],\n",
      "        [2.5780, 3.0411],\n",
      "        [2.5493, 3.0497],\n",
      "        [2.5705, 3.0604],\n",
      "        [2.5465, 2.9732],\n",
      "        [2.5860, 3.0449],\n",
      "        [2.4497, 2.9092],\n",
      "        [2.5654, 3.0318],\n",
      "        [2.5767, 3.0615],\n",
      "        [2.4962, 2.9758],\n",
      "        [2.5036, 2.9779],\n",
      "        [2.5407, 3.0302],\n",
      "        [2.5888, 3.0975],\n",
      "        [2.5637, 3.0392],\n",
      "        [2.5341, 2.9467],\n",
      "        [2.5845, 3.0873],\n",
      "        [2.5723, 3.0092],\n",
      "        [2.5332, 2.9975],\n",
      "        [2.5951, 3.0512],\n",
      "        [2.4807, 2.9545],\n",
      "        [2.5471, 3.0443],\n",
      "        [2.5290, 2.9690],\n",
      "        [2.4559, 2.9235],\n",
      "        [2.5407, 3.0100],\n",
      "        [2.5326, 2.9631],\n",
      "        [2.6279, 3.1049],\n",
      "        [2.5360, 2.9934],\n",
      "        [2.5601, 3.0303],\n",
      "        [2.4760, 2.9709],\n",
      "        [2.5502, 3.0306],\n",
      "        [2.6007, 3.0671],\n",
      "        [2.5721, 3.0576],\n",
      "        [2.5409, 2.9966],\n",
      "        [2.5426, 3.0030],\n",
      "        [2.4997, 3.0052],\n",
      "        [2.4241, 2.8976],\n",
      "        [2.4881, 2.9554],\n",
      "        [2.6411, 3.0614],\n",
      "        [2.6097, 3.0503],\n",
      "        [2.4843, 2.9600],\n",
      "        [2.5618, 3.0255],\n",
      "        [2.5864, 3.0695],\n",
      "        [2.4123, 2.8881],\n",
      "        [2.5516, 3.0544],\n",
      "        [2.5285, 2.9853],\n",
      "        [2.5106, 2.9802],\n",
      "        [2.5805, 3.0342],\n",
      "        [2.4957, 2.9628],\n",
      "        [2.6336, 3.0927],\n",
      "        [2.5625, 3.0557],\n",
      "        [2.6285, 3.0692],\n",
      "        [2.5759, 3.0707],\n",
      "        [2.5180, 3.0227],\n",
      "        [2.6123, 3.0482],\n",
      "        [2.5268, 2.9839],\n",
      "        [2.5745, 3.0169],\n",
      "        [2.5656, 3.0306],\n",
      "        [2.5949, 3.0371],\n",
      "        [2.6158, 3.0701],\n",
      "        [2.6084, 3.0640],\n",
      "        [2.4796, 2.9776],\n",
      "        [2.6045, 3.0455],\n",
      "        [2.5605, 3.0442],\n",
      "        [2.5706, 3.0451],\n",
      "        [2.5092, 3.0121],\n",
      "        [2.6122, 3.0896],\n",
      "        [2.5868, 3.0720],\n",
      "        [2.5226, 2.9985]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.7672, 3.1695]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.8115, 3.2412]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[2.8566, 3.2102]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.8452, 3.2385]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[2.8542, 3.2493]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[2.8871, 3.3070],\n",
      "        [2.8054, 3.1960],\n",
      "        [2.6877, 3.1573],\n",
      "        [2.8179, 3.1891],\n",
      "        [2.8494, 3.2598],\n",
      "        [2.8117, 3.2616],\n",
      "        [2.8477, 3.2394],\n",
      "        [2.7863, 3.1892],\n",
      "        [2.8069, 3.2512],\n",
      "        [2.7868, 3.1751],\n",
      "        [2.9151, 3.3087],\n",
      "        [2.8068, 3.2223],\n",
      "        [2.8775, 3.2559],\n",
      "        [2.8822, 3.2885],\n",
      "        [2.8312, 3.2290],\n",
      "        [2.8254, 3.2555],\n",
      "        [2.8187, 3.2159],\n",
      "        [2.8707, 3.2733],\n",
      "        [2.7869, 3.1585],\n",
      "        [2.8450, 3.2129],\n",
      "        [2.8734, 3.2742],\n",
      "        [2.7401, 3.1730],\n",
      "        [2.7394, 3.1579],\n",
      "        [2.8339, 3.2414],\n",
      "        [2.8509, 3.2567],\n",
      "        [2.8710, 3.2423],\n",
      "        [2.8797, 3.2775],\n",
      "        [2.7372, 3.1570],\n",
      "        [2.8247, 3.2463],\n",
      "        [2.8821, 3.2757],\n",
      "        [2.8753, 3.2678],\n",
      "        [2.7042, 3.1153],\n",
      "        [2.8574, 3.2553],\n",
      "        [2.8660, 3.2709],\n",
      "        [2.8448, 3.2099],\n",
      "        [2.8574, 3.2386],\n",
      "        [2.9036, 3.2642],\n",
      "        [2.8620, 3.2574],\n",
      "        [2.7767, 3.1986],\n",
      "        [2.8128, 3.2244],\n",
      "        [2.8063, 3.2215],\n",
      "        [2.8975, 3.2853],\n",
      "        [2.8527, 3.2546],\n",
      "        [2.8834, 3.2950],\n",
      "        [2.8409, 3.2676],\n",
      "        [2.7841, 3.1623],\n",
      "        [2.8254, 3.2250],\n",
      "        [2.8881, 3.2767],\n",
      "        [2.7611, 3.2008],\n",
      "        [2.8655, 3.2696],\n",
      "        [2.8757, 3.2650],\n",
      "        [2.8390, 3.2300],\n",
      "        [2.8215, 3.2371],\n",
      "        [2.7983, 3.2086],\n",
      "        [2.8945, 3.2836],\n",
      "        [2.8533, 3.2399],\n",
      "        [2.8547, 3.2791],\n",
      "        [2.8983, 3.2779],\n",
      "        [2.8754, 3.2548],\n",
      "        [2.7557, 3.1661],\n",
      "        [2.8932, 3.2671],\n",
      "        [2.8518, 3.2583],\n",
      "        [2.8113, 3.1991],\n",
      "        [2.6579, 3.0612],\n",
      "        [2.8039, 3.2151],\n",
      "        [2.8910, 3.2953],\n",
      "        [2.8189, 3.1910],\n",
      "        [2.8617, 3.2727],\n",
      "        [2.8389, 3.2349],\n",
      "        [2.8614, 3.2760],\n",
      "        [2.7953, 3.2270],\n",
      "        [2.6738, 3.0788],\n",
      "        [2.8667, 3.2606],\n",
      "        [2.8579, 3.2182],\n",
      "        [2.8680, 3.2692],\n",
      "        [2.8093, 3.2543],\n",
      "        [2.8493, 3.2446],\n",
      "        [2.8582, 3.2520],\n",
      "        [2.8237, 3.2258],\n",
      "        [2.8709, 3.3045],\n",
      "        [2.8585, 3.2638],\n",
      "        [2.9144, 3.2931],\n",
      "        [2.8244, 3.1932],\n",
      "        [2.8033, 3.1577],\n",
      "        [2.5579, 2.9765],\n",
      "        [2.8370, 3.2552],\n",
      "        [2.7935, 3.2168],\n",
      "        [2.7627, 3.2011],\n",
      "        [2.8613, 3.2541],\n",
      "        [2.8167, 3.2435],\n",
      "        [2.8584, 3.2574],\n",
      "        [2.8569, 3.2675],\n",
      "        [2.8688, 3.2137],\n",
      "        [2.8338, 3.2525],\n",
      "        [2.7317, 3.1658],\n",
      "        [2.8241, 3.2288],\n",
      "        [2.8284, 3.2415],\n",
      "        [2.8961, 3.2919],\n",
      "        [2.8939, 3.2729],\n",
      "        [2.8043, 3.2107]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.1773, 3.4705]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.1327, 3.4635]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.1544, 3.4361]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.1487, 3.4664]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.0946, 3.4077]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.1238, 3.4161],\n",
      "        [3.1225, 3.4006],\n",
      "        [3.1260, 3.4266],\n",
      "        [3.0689, 3.3784],\n",
      "        [3.0619, 3.3971],\n",
      "        [3.1350, 3.4375],\n",
      "        [3.1627, 3.4712],\n",
      "        [3.1131, 3.4444],\n",
      "        [3.0767, 3.3720],\n",
      "        [3.1476, 3.4332],\n",
      "        [3.0494, 3.4071],\n",
      "        [3.1161, 3.4003],\n",
      "        [3.1866, 3.4822],\n",
      "        [3.1247, 3.4506],\n",
      "        [3.1051, 3.4190],\n",
      "        [3.0761, 3.3667],\n",
      "        [3.1172, 3.3955],\n",
      "        [3.1562, 3.4619],\n",
      "        [3.1236, 3.4338],\n",
      "        [3.1294, 3.4446],\n",
      "        [3.0581, 3.4036],\n",
      "        [2.9961, 3.3389],\n",
      "        [3.1685, 3.4770],\n",
      "        [3.0904, 3.4423],\n",
      "        [3.1797, 3.4760],\n",
      "        [3.0848, 3.4047],\n",
      "        [3.0902, 3.3980],\n",
      "        [3.1108, 3.4094],\n",
      "        [3.0742, 3.3785],\n",
      "        [3.1583, 3.4510],\n",
      "        [3.1574, 3.4422],\n",
      "        [3.1173, 3.4355],\n",
      "        [3.1338, 3.4307],\n",
      "        [3.1741, 3.4432],\n",
      "        [3.2142, 3.5030],\n",
      "        [3.1087, 3.4107],\n",
      "        [3.0790, 3.3713],\n",
      "        [3.1338, 3.4523],\n",
      "        [3.1046, 3.4224],\n",
      "        [3.0908, 3.3888],\n",
      "        [3.1438, 3.4578],\n",
      "        [3.0498, 3.3658],\n",
      "        [3.1606, 3.4422],\n",
      "        [3.1226, 3.4115],\n",
      "        [3.0445, 3.3690],\n",
      "        [3.0954, 3.4369],\n",
      "        [3.1757, 3.4837],\n",
      "        [3.0202, 3.3666],\n",
      "        [3.0301, 3.3237],\n",
      "        [3.1354, 3.4414],\n",
      "        [3.1652, 3.4552],\n",
      "        [3.0751, 3.4286],\n",
      "        [3.0613, 3.3751],\n",
      "        [3.1799, 3.4652],\n",
      "        [3.0353, 3.3619],\n",
      "        [3.1872, 3.4897],\n",
      "        [3.1455, 3.4560],\n",
      "        [3.1664, 3.4425],\n",
      "        [3.1080, 3.4157],\n",
      "        [3.0753, 3.3618],\n",
      "        [3.1216, 3.4300],\n",
      "        [3.1431, 3.4529],\n",
      "        [3.1623, 3.4498],\n",
      "        [3.1115, 3.4252],\n",
      "        [3.1198, 3.4424],\n",
      "        [3.1336, 3.4510],\n",
      "        [3.0607, 3.3885],\n",
      "        [3.1545, 3.4563],\n",
      "        [3.1729, 3.4572],\n",
      "        [3.1555, 3.4175],\n",
      "        [3.1712, 3.4676],\n",
      "        [3.0714, 3.4056],\n",
      "        [3.1104, 3.4222],\n",
      "        [3.1681, 3.4929],\n",
      "        [3.0771, 3.4023],\n",
      "        [3.0614, 3.4025],\n",
      "        [3.1560, 3.4399],\n",
      "        [3.1146, 3.4219],\n",
      "        [3.1381, 3.4101],\n",
      "        [3.1589, 3.4578],\n",
      "        [3.1476, 3.4545],\n",
      "        [3.1709, 3.4595],\n",
      "        [3.1890, 3.4741],\n",
      "        [3.0389, 3.3510],\n",
      "        [3.1541, 3.4687],\n",
      "        [3.1941, 3.4956],\n",
      "        [3.1519, 3.4165],\n",
      "        [3.1295, 3.4387],\n",
      "        [3.1527, 3.4727],\n",
      "        [3.1774, 3.4829],\n",
      "        [3.1648, 3.4620],\n",
      "        [3.0886, 3.3959],\n",
      "        [3.1617, 3.4580],\n",
      "        [3.1484, 3.4470],\n",
      "        [3.1506, 3.4449],\n",
      "        [3.1528, 3.4593],\n",
      "        [3.1366, 3.4146],\n",
      "        [3.0793, 3.3985],\n",
      "        [3.1714, 3.4593],\n",
      "        [3.1175, 3.4271]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.3702, 3.5977]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4128, 3.5957]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.4429, 3.6221]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.3805, 3.5927]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4036, 3.5956]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.4136, 3.5956],\n",
      "        [3.3650, 3.5823],\n",
      "        [3.4305, 3.6477],\n",
      "        [3.3882, 3.5872],\n",
      "        [3.4426, 3.6592],\n",
      "        [3.3907, 3.6081],\n",
      "        [3.4593, 3.6481],\n",
      "        [3.4791, 3.6548],\n",
      "        [3.4056, 3.6091],\n",
      "        [3.2901, 3.5583],\n",
      "        [3.3987, 3.6310],\n",
      "        [3.4378, 3.6431],\n",
      "        [3.4836, 3.6551],\n",
      "        [3.4155, 3.6398],\n",
      "        [3.3919, 3.5825],\n",
      "        [3.4065, 3.6092],\n",
      "        [3.3948, 3.6015],\n",
      "        [3.4558, 3.6360],\n",
      "        [3.4335, 3.6400],\n",
      "        [3.3893, 3.5669],\n",
      "        [3.3933, 3.6030],\n",
      "        [3.4006, 3.6000],\n",
      "        [3.3873, 3.5900],\n",
      "        [3.3484, 3.5552],\n",
      "        [3.3921, 3.6140],\n",
      "        [3.4242, 3.6244],\n",
      "        [3.4209, 3.5966],\n",
      "        [3.4652, 3.6594],\n",
      "        [3.4518, 3.6444],\n",
      "        [3.4867, 3.6622],\n",
      "        [3.3942, 3.6067],\n",
      "        [3.4388, 3.6255],\n",
      "        [3.3719, 3.6022],\n",
      "        [3.4759, 3.6765],\n",
      "        [3.3720, 3.5952],\n",
      "        [3.4361, 3.5840],\n",
      "        [3.4506, 3.6355],\n",
      "        [3.3774, 3.5901],\n",
      "        [3.3232, 3.5407],\n",
      "        [3.3464, 3.5555],\n",
      "        [3.4402, 3.6319],\n",
      "        [3.4029, 3.6071],\n",
      "        [3.4329, 3.6271],\n",
      "        [3.4485, 3.6219],\n",
      "        [3.2431, 3.5011],\n",
      "        [3.4256, 3.5904],\n",
      "        [3.4236, 3.6177],\n",
      "        [3.3569, 3.5621],\n",
      "        [3.3583, 3.5792],\n",
      "        [3.4286, 3.6166],\n",
      "        [3.4052, 3.6093],\n",
      "        [3.4663, 3.6555],\n",
      "        [3.3520, 3.5648],\n",
      "        [3.3692, 3.5503],\n",
      "        [3.4144, 3.5906],\n",
      "        [3.4109, 3.6118],\n",
      "        [3.2448, 3.4743],\n",
      "        [3.4421, 3.6143],\n",
      "        [3.4497, 3.6603],\n",
      "        [3.3830, 3.6105],\n",
      "        [3.4107, 3.6056],\n",
      "        [3.3906, 3.5717],\n",
      "        [3.4594, 3.6461],\n",
      "        [3.4099, 3.6150],\n",
      "        [3.4281, 3.6320],\n",
      "        [3.4141, 3.6086],\n",
      "        [3.4492, 3.6415],\n",
      "        [3.3653, 3.5669],\n",
      "        [3.4921, 3.6629],\n",
      "        [3.4116, 3.6196],\n",
      "        [3.4337, 3.6089],\n",
      "        [3.3651, 3.5958],\n",
      "        [3.4458, 3.6333],\n",
      "        [3.3826, 3.5660],\n",
      "        [3.3628, 3.5897],\n",
      "        [3.4358, 3.6242],\n",
      "        [3.4645, 3.6440],\n",
      "        [3.3884, 3.5821],\n",
      "        [3.3751, 3.5947],\n",
      "        [3.4492, 3.6373],\n",
      "        [3.4182, 3.6066],\n",
      "        [3.4567, 3.6563],\n",
      "        [3.3592, 3.5624],\n",
      "        [3.3973, 3.6082],\n",
      "        [3.4347, 3.6001],\n",
      "        [3.3575, 3.5780],\n",
      "        [3.3799, 3.5879],\n",
      "        [3.3495, 3.5872],\n",
      "        [3.4147, 3.6060],\n",
      "        [3.3834, 3.6023],\n",
      "        [3.3583, 3.5658],\n",
      "        [3.3796, 3.5999],\n",
      "        [3.4251, 3.6447],\n",
      "        [3.4344, 3.6326],\n",
      "        [3.3882, 3.6103],\n",
      "        [3.4112, 3.5944],\n",
      "        [3.4444, 3.6196],\n",
      "        [3.4064, 3.5925],\n",
      "        [3.4582, 3.6435],\n",
      "        [3.4478, 3.6335]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior_param_sample:  tensor([[3.6991, 3.8090]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6602, 3.7421]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.7009, 3.7954]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.7099, 3.7964]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6696, 3.7939]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.6958, 3.7594],\n",
      "        [3.6282, 3.7132],\n",
      "        [3.6741, 3.7617],\n",
      "        [3.6571, 3.7600],\n",
      "        [3.6160, 3.6939],\n",
      "        [3.5501, 3.6577],\n",
      "        [3.6626, 3.7878],\n",
      "        [3.6967, 3.7827],\n",
      "        [3.6703, 3.7563],\n",
      "        [3.5792, 3.7215],\n",
      "        [3.5172, 3.6995],\n",
      "        [3.6637, 3.7675],\n",
      "        [3.6848, 3.7843],\n",
      "        [3.5967, 3.7308],\n",
      "        [3.7017, 3.8125],\n",
      "        [3.6795, 3.7734],\n",
      "        [3.6690, 3.7885],\n",
      "        [3.6461, 3.7535],\n",
      "        [3.6875, 3.7853],\n",
      "        [3.6381, 3.7247],\n",
      "        [3.6307, 3.7588],\n",
      "        [3.7105, 3.8111],\n",
      "        [3.6722, 3.7700],\n",
      "        [3.5517, 3.6875],\n",
      "        [3.7382, 3.8156],\n",
      "        [3.6424, 3.7414],\n",
      "        [3.6764, 3.7617],\n",
      "        [3.6834, 3.7932],\n",
      "        [3.6657, 3.7674],\n",
      "        [3.6744, 3.7899],\n",
      "        [3.6535, 3.7348],\n",
      "        [3.6046, 3.7575],\n",
      "        [3.6100, 3.6980],\n",
      "        [3.6326, 3.7397],\n",
      "        [3.6438, 3.7584],\n",
      "        [3.6500, 3.7604],\n",
      "        [3.5669, 3.6650],\n",
      "        [3.6654, 3.7827],\n",
      "        [3.5844, 3.7144],\n",
      "        [3.6048, 3.7308],\n",
      "        [3.6918, 3.7661],\n",
      "        [3.7121, 3.8030],\n",
      "        [3.5982, 3.7380],\n",
      "        [3.7058, 3.8086],\n",
      "        [3.5724, 3.6468],\n",
      "        [3.5894, 3.7454],\n",
      "        [3.6432, 3.7553],\n",
      "        [3.6506, 3.7755],\n",
      "        [3.6451, 3.7642],\n",
      "        [3.6444, 3.7505],\n",
      "        [3.7132, 3.8230],\n",
      "        [3.6065, 3.7486],\n",
      "        [3.7076, 3.7980],\n",
      "        [3.6369, 3.7343],\n",
      "        [3.6581, 3.7560],\n",
      "        [3.6401, 3.7471],\n",
      "        [3.7042, 3.7976],\n",
      "        [3.7185, 3.8019],\n",
      "        [3.6899, 3.7748],\n",
      "        [3.6471, 3.7508],\n",
      "        [3.7052, 3.7681],\n",
      "        [3.6304, 3.7693],\n",
      "        [3.6789, 3.7969],\n",
      "        [3.6797, 3.7920],\n",
      "        [3.7164, 3.8257],\n",
      "        [3.6809, 3.7893],\n",
      "        [3.6907, 3.7826],\n",
      "        [3.7341, 3.8204],\n",
      "        [3.7145, 3.8156],\n",
      "        [3.6920, 3.7729],\n",
      "        [3.6271, 3.7554],\n",
      "        [3.6610, 3.7699],\n",
      "        [3.7237, 3.8001],\n",
      "        [3.6532, 3.7625],\n",
      "        [3.7259, 3.8127],\n",
      "        [3.6565, 3.7857],\n",
      "        [3.6982, 3.7536],\n",
      "        [3.6439, 3.7634],\n",
      "        [3.6560, 3.7426],\n",
      "        [3.6921, 3.7754],\n",
      "        [3.5932, 3.7035],\n",
      "        [3.6533, 3.7589],\n",
      "        [3.6919, 3.7845],\n",
      "        [3.6985, 3.7876],\n",
      "        [3.6699, 3.7623],\n",
      "        [3.6932, 3.7818],\n",
      "        [3.6523, 3.7387],\n",
      "        [3.6327, 3.7523],\n",
      "        [3.6980, 3.7999],\n",
      "        [3.6137, 3.7211],\n",
      "        [3.6564, 3.7674],\n",
      "        [3.7048, 3.8153],\n",
      "        [3.6684, 3.7698],\n",
      "        [3.6960, 3.7993],\n",
      "        [3.6789, 3.7785],\n",
      "        [3.6968, 3.8076],\n",
      "        [3.6609, 3.7681],\n",
      "        [3.6299, 3.7777],\n",
      "        [3.6268, 3.7073],\n",
      "        [3.7427, 3.8157]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8233, 3.9038]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8084, 3.9066]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.8629, 3.9280]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.7806, 3.8684]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8503, 3.9084]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.8106, 3.8967],\n",
      "        [3.7677, 3.8764],\n",
      "        [3.8639, 3.9352],\n",
      "        [3.8226, 3.9089],\n",
      "        [3.7727, 3.8604],\n",
      "        [3.8215, 3.9008],\n",
      "        [3.8259, 3.9081],\n",
      "        [3.8197, 3.8971],\n",
      "        [3.8078, 3.9110],\n",
      "        [3.7906, 3.8439],\n",
      "        [3.7209, 3.8621],\n",
      "        [3.8129, 3.9239],\n",
      "        [3.8850, 3.9543],\n",
      "        [3.8336, 3.9142],\n",
      "        [3.8102, 3.9044],\n",
      "        [3.7941, 3.8700],\n",
      "        [3.7850, 3.8759],\n",
      "        [3.8721, 3.9278],\n",
      "        [3.8453, 3.9148],\n",
      "        [3.7540, 3.8646],\n",
      "        [3.8490, 3.9138],\n",
      "        [3.7728, 3.8757],\n",
      "        [3.8184, 3.9055],\n",
      "        [3.8236, 3.8958],\n",
      "        [3.8652, 3.9132],\n",
      "        [3.8409, 3.8992],\n",
      "        [3.7392, 3.8262],\n",
      "        [3.8200, 3.8983],\n",
      "        [3.8831, 3.9285],\n",
      "        [3.8165, 3.9034],\n",
      "        [3.8377, 3.8954],\n",
      "        [3.8431, 3.9083],\n",
      "        [3.8940, 3.9553],\n",
      "        [3.8371, 3.8899],\n",
      "        [3.8430, 3.9218],\n",
      "        [3.7973, 3.8915],\n",
      "        [3.8297, 3.8772],\n",
      "        [3.8637, 3.9348],\n",
      "        [3.8018, 3.8707],\n",
      "        [3.8175, 3.8967],\n",
      "        [3.7849, 3.8942],\n",
      "        [3.8064, 3.8878],\n",
      "        [3.8104, 3.8844],\n",
      "        [3.8499, 3.9353],\n",
      "        [3.8522, 3.9278],\n",
      "        [3.7972, 3.8950],\n",
      "        [3.8243, 3.9007],\n",
      "        [3.8631, 3.9116],\n",
      "        [3.8261, 3.9076],\n",
      "        [3.8663, 3.9352],\n",
      "        [3.8520, 3.9304],\n",
      "        [3.7382, 3.8654],\n",
      "        [3.7936, 3.9144],\n",
      "        [3.8589, 3.9370],\n",
      "        [3.7825, 3.8579],\n",
      "        [3.8173, 3.8820],\n",
      "        [3.7663, 3.8433],\n",
      "        [3.7701, 3.8428],\n",
      "        [3.8191, 3.8914],\n",
      "        [3.8573, 3.8921],\n",
      "        [3.8495, 3.9256],\n",
      "        [3.8251, 3.9080],\n",
      "        [3.8555, 3.9266],\n",
      "        [3.7983, 3.8671],\n",
      "        [3.7631, 3.8678],\n",
      "        [3.7954, 3.9172],\n",
      "        [3.8469, 3.9286],\n",
      "        [3.7236, 3.8127],\n",
      "        [3.8804, 3.9404],\n",
      "        [3.6732, 3.7806],\n",
      "        [3.8037, 3.8884],\n",
      "        [3.7908, 3.8995],\n",
      "        [3.8092, 3.8907],\n",
      "        [3.8130, 3.9064],\n",
      "        [3.8811, 3.9485],\n",
      "        [3.7930, 3.8681],\n",
      "        [3.8621, 3.9298],\n",
      "        [3.8193, 3.8857],\n",
      "        [3.8789, 3.9404],\n",
      "        [3.8216, 3.9017],\n",
      "        [3.8258, 3.9087],\n",
      "        [3.7867, 3.8924],\n",
      "        [3.8408, 3.9230],\n",
      "        [3.8465, 3.9181],\n",
      "        [3.7822, 3.8968],\n",
      "        [3.8500, 3.8973],\n",
      "        [3.8325, 3.9216],\n",
      "        [3.8359, 3.9203],\n",
      "        [3.8795, 3.9435],\n",
      "        [3.8232, 3.8820],\n",
      "        [3.7313, 3.8357],\n",
      "        [3.7912, 3.8631],\n",
      "        [3.8085, 3.8964],\n",
      "        [3.7289, 3.8673],\n",
      "        [3.8674, 3.9350],\n",
      "        [3.8480, 3.9061],\n",
      "        [3.8460, 3.9021],\n",
      "        [3.7876, 3.9130],\n",
      "        [3.8106, 3.8781],\n",
      "        [3.7705, 3.8453]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8467, 4.0009]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8399, 4.0163]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.7452, 3.9519]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8155, 4.0147]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8045, 3.9949]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.7448, 3.9849],\n",
      "        [3.8465, 4.0005],\n",
      "        [3.8746, 4.0156],\n",
      "        [3.8394, 4.0127],\n",
      "        [3.8314, 4.0250],\n",
      "        [3.7772, 3.9887],\n",
      "        [3.8900, 4.0426],\n",
      "        [3.8783, 4.0302],\n",
      "        [3.8582, 4.0444],\n",
      "        [3.7660, 3.9501],\n",
      "        [3.8182, 3.9884],\n",
      "        [3.8957, 4.0363],\n",
      "        [3.8190, 4.0038],\n",
      "        [3.8771, 4.0425],\n",
      "        [3.8895, 4.0352],\n",
      "        [3.9007, 4.0424],\n",
      "        [3.8196, 3.9880],\n",
      "        [3.8390, 4.0251],\n",
      "        [3.8887, 4.0588],\n",
      "        [3.8568, 4.0358],\n",
      "        [3.8079, 4.0036],\n",
      "        [3.8128, 4.0042],\n",
      "        [3.7490, 3.9334],\n",
      "        [3.7066, 3.9657],\n",
      "        [3.8432, 4.0306],\n",
      "        [3.8777, 4.0282],\n",
      "        [3.8547, 4.0169],\n",
      "        [3.8714, 4.0416],\n",
      "        [3.8782, 4.0571],\n",
      "        [3.7758, 3.9758],\n",
      "        [3.8207, 3.9885],\n",
      "        [3.8573, 4.0274],\n",
      "        [3.8185, 4.0070],\n",
      "        [3.8186, 3.9947],\n",
      "        [3.8099, 3.9825],\n",
      "        [3.8686, 4.0322],\n",
      "        [3.8439, 4.0210],\n",
      "        [3.8283, 4.0018],\n",
      "        [3.8865, 4.0329],\n",
      "        [3.7660, 3.9755],\n",
      "        [3.8425, 4.0337],\n",
      "        [3.8978, 4.0544],\n",
      "        [3.8711, 4.0315],\n",
      "        [3.8315, 3.9767],\n",
      "        [3.7512, 3.9472],\n",
      "        [3.8630, 4.0294],\n",
      "        [3.7625, 3.9849],\n",
      "        [3.8173, 3.9892],\n",
      "        [3.8628, 4.0057],\n",
      "        [3.8809, 4.0518],\n",
      "        [3.7791, 3.9690],\n",
      "        [3.8607, 4.0398],\n",
      "        [3.9133, 4.0370],\n",
      "        [3.8332, 4.0152],\n",
      "        [3.8752, 4.0226],\n",
      "        [3.8072, 4.0083],\n",
      "        [3.7469, 3.9410],\n",
      "        [3.8757, 4.0454],\n",
      "        [3.8422, 4.0331],\n",
      "        [3.9006, 4.0532],\n",
      "        [3.8499, 4.0438],\n",
      "        [3.7851, 3.9750],\n",
      "        [3.8720, 4.0269],\n",
      "        [3.8576, 4.0120],\n",
      "        [3.7722, 3.9456],\n",
      "        [3.8470, 4.0179],\n",
      "        [3.8500, 4.0052],\n",
      "        [3.8205, 4.0089],\n",
      "        [3.8487, 4.0196],\n",
      "        [3.8716, 4.0448],\n",
      "        [3.8784, 4.0422],\n",
      "        [3.8310, 4.0175],\n",
      "        [3.9253, 4.0553],\n",
      "        [3.6991, 3.9096],\n",
      "        [3.8953, 4.0499],\n",
      "        [3.8783, 4.0404],\n",
      "        [3.8534, 4.0285],\n",
      "        [3.7966, 3.9954],\n",
      "        [3.8388, 4.0361],\n",
      "        [3.7940, 3.9780],\n",
      "        [3.8028, 4.0126],\n",
      "        [3.8489, 4.0199],\n",
      "        [3.8683, 4.0452],\n",
      "        [3.8929, 4.0411],\n",
      "        [3.7051, 3.9592],\n",
      "        [3.9143, 4.0537],\n",
      "        [3.8111, 4.0060],\n",
      "        [3.7407, 3.9585],\n",
      "        [3.7902, 3.9782],\n",
      "        [3.7796, 3.9728],\n",
      "        [3.7281, 3.9712],\n",
      "        [3.8159, 4.0037],\n",
      "        [3.8668, 4.0401],\n",
      "        [3.8850, 4.0369],\n",
      "        [3.8679, 4.0193],\n",
      "        [3.8066, 3.9700],\n",
      "        [3.8734, 4.0294],\n",
      "        [3.8714, 4.0128],\n",
      "        [3.8303, 4.0044],\n",
      "        [3.8429, 4.0218]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.7672, 4.1111]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6923, 4.0654]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.7671, 4.0901]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8780, 4.1656]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.8211, 4.1218]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.8364, 4.1257],\n",
      "        [3.8161, 4.1048],\n",
      "        [3.8191, 4.1132],\n",
      "        [3.8290, 4.1222],\n",
      "        [3.8382, 4.1343],\n",
      "        [3.8229, 4.1210],\n",
      "        [3.8334, 4.1257],\n",
      "        [3.8398, 4.1255],\n",
      "        [3.8132, 4.1332],\n",
      "        [3.7928, 4.0903],\n",
      "        [3.8452, 4.1438],\n",
      "        [3.8024, 4.1128],\n",
      "        [3.8029, 4.1371],\n",
      "        [3.8585, 4.1189],\n",
      "        [3.7988, 4.1176],\n",
      "        [3.8277, 4.1476],\n",
      "        [3.8411, 4.1281],\n",
      "        [3.8519, 4.1519],\n",
      "        [3.8594, 4.1486],\n",
      "        [3.7743, 4.1047],\n",
      "        [3.8042, 4.0977],\n",
      "        [3.6793, 4.0449],\n",
      "        [3.7612, 4.0986],\n",
      "        [3.8135, 4.1317],\n",
      "        [3.8121, 4.1295],\n",
      "        [3.8920, 4.1556],\n",
      "        [3.8034, 4.1182],\n",
      "        [3.8724, 4.1566],\n",
      "        [3.8407, 4.1360],\n",
      "        [3.8003, 4.1175],\n",
      "        [3.7435, 4.0612],\n",
      "        [3.8243, 4.1394],\n",
      "        [3.7939, 4.1308],\n",
      "        [3.8004, 4.1111],\n",
      "        [3.8334, 4.1330],\n",
      "        [3.7793, 4.0465],\n",
      "        [3.8505, 4.1367],\n",
      "        [3.8659, 4.1515],\n",
      "        [3.8268, 4.1375],\n",
      "        [3.8468, 4.1473],\n",
      "        [3.7644, 4.0730],\n",
      "        [3.8138, 4.1295],\n",
      "        [3.7458, 4.0920],\n",
      "        [3.8091, 4.1116],\n",
      "        [3.8031, 4.1049],\n",
      "        [3.8324, 4.1214],\n",
      "        [3.8347, 4.1315],\n",
      "        [3.8348, 4.1342],\n",
      "        [3.6165, 4.0002],\n",
      "        [3.8411, 4.1220],\n",
      "        [3.7480, 4.0831],\n",
      "        [3.7981, 4.1092],\n",
      "        [3.8725, 4.1549],\n",
      "        [3.7816, 4.1111],\n",
      "        [3.7218, 4.0652],\n",
      "        [3.8708, 4.1486],\n",
      "        [3.8161, 4.1121],\n",
      "        [3.8801, 4.1462],\n",
      "        [3.7879, 4.1130],\n",
      "        [3.8370, 4.1341],\n",
      "        [3.7468, 4.1103],\n",
      "        [3.8340, 4.1441],\n",
      "        [3.8124, 4.1218],\n",
      "        [3.8910, 4.1566],\n",
      "        [3.8498, 4.1269],\n",
      "        [3.7583, 4.0865],\n",
      "        [3.8431, 4.1233],\n",
      "        [3.8065, 4.1037],\n",
      "        [3.8328, 4.1302],\n",
      "        [3.7721, 4.0872],\n",
      "        [3.8306, 4.1358],\n",
      "        [3.8418, 4.1493],\n",
      "        [3.7720, 4.1133],\n",
      "        [3.8663, 4.1392],\n",
      "        [3.8147, 4.1161],\n",
      "        [3.7244, 4.0684],\n",
      "        [3.8289, 4.1441],\n",
      "        [3.8379, 4.1367],\n",
      "        [3.8276, 4.1391],\n",
      "        [3.7369, 4.0942],\n",
      "        [3.8401, 4.1398],\n",
      "        [3.8769, 4.1530],\n",
      "        [3.7796, 4.1130],\n",
      "        [3.8665, 4.1619],\n",
      "        [3.8020, 4.1102],\n",
      "        [3.8190, 4.1319],\n",
      "        [3.8348, 4.1451],\n",
      "        [3.8429, 4.1493],\n",
      "        [3.8545, 4.1449],\n",
      "        [3.8639, 4.1512],\n",
      "        [3.7743, 4.1116],\n",
      "        [3.7253, 4.0764],\n",
      "        [3.7853, 4.0921],\n",
      "        [3.8072, 4.1187],\n",
      "        [3.8229, 4.1061],\n",
      "        [3.8207, 4.1381],\n",
      "        [3.8583, 4.1566],\n",
      "        [3.7929, 4.1053],\n",
      "        [3.8046, 4.1130],\n",
      "        [3.7711, 4.1019]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior_param_sample:  tensor([[3.7578, 4.2143]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.7226, 4.2001]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.7997, 4.2247]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.7483, 4.1890]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6857, 4.1664]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.8188, 4.2376],\n",
      "        [3.7592, 4.2193],\n",
      "        [3.6897, 4.1742],\n",
      "        [3.7861, 4.2142],\n",
      "        [3.8016, 4.2351],\n",
      "        [3.8163, 4.2343],\n",
      "        [3.7606, 4.1986],\n",
      "        [3.6704, 4.1647],\n",
      "        [3.7785, 4.2304],\n",
      "        [3.7534, 4.2105],\n",
      "        [3.7876, 4.2055],\n",
      "        [3.7546, 4.2079],\n",
      "        [3.7672, 4.2069],\n",
      "        [3.6972, 4.1413],\n",
      "        [3.6625, 4.1690],\n",
      "        [3.7522, 4.2007],\n",
      "        [3.8036, 4.2309],\n",
      "        [3.7244, 4.1659],\n",
      "        [3.7484, 4.2032],\n",
      "        [3.8017, 4.2276],\n",
      "        [3.7200, 4.2084],\n",
      "        [3.7999, 4.2414],\n",
      "        [3.7872, 4.2184],\n",
      "        [3.7809, 4.2094],\n",
      "        [3.6859, 4.1870],\n",
      "        [3.7160, 4.1516],\n",
      "        [3.6380, 4.1530],\n",
      "        [3.7964, 4.2330],\n",
      "        [3.7281, 4.2121],\n",
      "        [3.7544, 4.2076],\n",
      "        [3.7443, 4.2025],\n",
      "        [3.7822, 4.2262],\n",
      "        [3.7539, 4.2041],\n",
      "        [3.7777, 4.2222],\n",
      "        [3.7127, 4.1675],\n",
      "        [3.7820, 4.2195],\n",
      "        [3.7539, 4.2184],\n",
      "        [3.6626, 4.1741],\n",
      "        [3.7376, 4.2040],\n",
      "        [3.7984, 4.2230],\n",
      "        [3.7086, 4.1983],\n",
      "        [3.7252, 4.1743],\n",
      "        [3.8122, 4.2336],\n",
      "        [3.7583, 4.2225],\n",
      "        [3.7293, 4.1751],\n",
      "        [3.6227, 4.1502],\n",
      "        [3.7302, 4.1922],\n",
      "        [3.7121, 4.1948],\n",
      "        [3.7588, 4.2096],\n",
      "        [3.7748, 4.2157],\n",
      "        [3.7302, 4.1747],\n",
      "        [3.7646, 4.2041],\n",
      "        [3.7847, 4.2389],\n",
      "        [3.8417, 4.2537],\n",
      "        [3.6964, 4.1625],\n",
      "        [3.7790, 4.2115],\n",
      "        [3.7524, 4.2134],\n",
      "        [3.7651, 4.2162],\n",
      "        [3.7670, 4.2143],\n",
      "        [3.7693, 4.2085],\n",
      "        [3.6805, 4.1528],\n",
      "        [3.6840, 4.1806],\n",
      "        [3.8058, 4.2231],\n",
      "        [3.7642, 4.2326],\n",
      "        [3.6442, 4.1453],\n",
      "        [3.7045, 4.1980],\n",
      "        [3.6477, 4.1551],\n",
      "        [3.7862, 4.2301],\n",
      "        [3.7353, 4.2001],\n",
      "        [3.6936, 4.1542],\n",
      "        [3.6110, 4.1516],\n",
      "        [3.7443, 4.2148],\n",
      "        [3.7084, 4.2093],\n",
      "        [3.7323, 4.2076],\n",
      "        [3.7106, 4.1529],\n",
      "        [3.7362, 4.2122],\n",
      "        [3.7811, 4.2422],\n",
      "        [3.7338, 4.2074],\n",
      "        [3.6507, 4.1448],\n",
      "        [3.7306, 4.1991],\n",
      "        [3.7323, 4.1787],\n",
      "        [3.7174, 4.2031],\n",
      "        [3.7342, 4.2023],\n",
      "        [3.7165, 4.2002],\n",
      "        [3.7453, 4.1920],\n",
      "        [3.7264, 4.2082],\n",
      "        [3.8273, 4.2455],\n",
      "        [3.7244, 4.1795],\n",
      "        [3.8053, 4.2297],\n",
      "        [3.8091, 4.2313],\n",
      "        [3.7611, 4.1895],\n",
      "        [3.6863, 4.1908],\n",
      "        [3.8090, 4.2454],\n",
      "        [3.8074, 4.2171],\n",
      "        [3.7170, 4.1920],\n",
      "        [3.6537, 4.1610],\n",
      "        [3.8290, 4.2400],\n",
      "        [3.7412, 4.1961],\n",
      "        [3.7736, 4.2197],\n",
      "        [3.7506, 4.2225]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6764, 4.2859]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5418, 4.2231]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.6461, 4.2917]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6371, 4.2780]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6834, 4.2879]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.6282, 4.2809],\n",
      "        [3.6823, 4.2794],\n",
      "        [3.6322, 4.2525],\n",
      "        [3.7244, 4.3101],\n",
      "        [3.6930, 4.2963],\n",
      "        [3.7145, 4.3088],\n",
      "        [3.5987, 4.2470],\n",
      "        [3.5823, 4.2353],\n",
      "        [3.6273, 4.2829],\n",
      "        [3.6943, 4.2806],\n",
      "        [3.7287, 4.2995],\n",
      "        [3.7063, 4.3013],\n",
      "        [3.5396, 4.2413],\n",
      "        [3.6615, 4.2813],\n",
      "        [3.6904, 4.2904],\n",
      "        [3.7263, 4.3036],\n",
      "        [3.5387, 4.2452],\n",
      "        [3.6230, 4.2657],\n",
      "        [3.5613, 4.2268],\n",
      "        [3.6775, 4.2989],\n",
      "        [3.6041, 4.2507],\n",
      "        [3.7203, 4.3135],\n",
      "        [3.6523, 4.2809],\n",
      "        [3.5594, 4.2012],\n",
      "        [3.6666, 4.2717],\n",
      "        [3.6058, 4.2743],\n",
      "        [3.6870, 4.2806],\n",
      "        [3.7037, 4.3053],\n",
      "        [3.6587, 4.2897],\n",
      "        [3.7129, 4.3066],\n",
      "        [3.7167, 4.3017],\n",
      "        [3.5810, 4.2006],\n",
      "        [3.6980, 4.2767],\n",
      "        [3.6645, 4.2868],\n",
      "        [3.6165, 4.2793],\n",
      "        [3.6528, 4.2902],\n",
      "        [3.5954, 4.2015],\n",
      "        [3.6265, 4.2576],\n",
      "        [3.6794, 4.2957],\n",
      "        [3.6752, 4.3049],\n",
      "        [3.6947, 4.3165],\n",
      "        [3.6871, 4.2927],\n",
      "        [3.6626, 4.2424],\n",
      "        [3.6274, 4.2476],\n",
      "        [3.7380, 4.3268],\n",
      "        [3.6699, 4.2844],\n",
      "        [3.6751, 4.2956],\n",
      "        [3.6758, 4.2613],\n",
      "        [3.6849, 4.2746],\n",
      "        [3.6700, 4.2799],\n",
      "        [3.4602, 4.1472],\n",
      "        [3.7139, 4.3057],\n",
      "        [3.6855, 4.3151],\n",
      "        [3.5867, 4.2245],\n",
      "        [3.6796, 4.2842],\n",
      "        [3.6211, 4.2697],\n",
      "        [3.6576, 4.2541],\n",
      "        [3.7096, 4.3010],\n",
      "        [3.6525, 4.2884],\n",
      "        [3.6582, 4.2866],\n",
      "        [3.6345, 4.2804],\n",
      "        [3.5874, 4.2295],\n",
      "        [3.6993, 4.2951],\n",
      "        [3.6207, 4.2743],\n",
      "        [3.5685, 4.2322],\n",
      "        [3.7465, 4.3122],\n",
      "        [3.6836, 4.2756],\n",
      "        [3.5453, 4.1943],\n",
      "        [3.6695, 4.2760],\n",
      "        [3.6925, 4.2796],\n",
      "        [3.6890, 4.2975],\n",
      "        [3.4651, 4.1918],\n",
      "        [3.6761, 4.2641],\n",
      "        [3.6573, 4.2405],\n",
      "        [3.7405, 4.3257],\n",
      "        [3.7587, 4.3219],\n",
      "        [3.6693, 4.2880],\n",
      "        [3.6809, 4.2947],\n",
      "        [3.6804, 4.2852],\n",
      "        [3.7172, 4.3237],\n",
      "        [3.6523, 4.2929],\n",
      "        [3.6281, 4.2700],\n",
      "        [3.6683, 4.2644],\n",
      "        [3.5476, 4.2084],\n",
      "        [3.7057, 4.2735],\n",
      "        [3.6708, 4.2879],\n",
      "        [3.6479, 4.2575],\n",
      "        [3.6867, 4.3086],\n",
      "        [3.7200, 4.3049],\n",
      "        [3.6241, 4.2656],\n",
      "        [3.6322, 4.2752],\n",
      "        [3.7217, 4.3210],\n",
      "        [3.7249, 4.3113],\n",
      "        [3.7285, 4.3111],\n",
      "        [3.5888, 4.2448],\n",
      "        [3.7002, 4.2627],\n",
      "        [3.6899, 4.2945],\n",
      "        [3.6778, 4.2826],\n",
      "        [3.7038, 4.2971],\n",
      "        [3.6233, 4.2585]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6106, 4.3753]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5957, 4.3593]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.4888, 4.2854]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6416, 4.3787]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6306, 4.3792]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.6560, 4.3914],\n",
      "        [3.6224, 4.3705],\n",
      "        [3.5550, 4.3466],\n",
      "        [3.6751, 4.3757],\n",
      "        [3.6529, 4.3861],\n",
      "        [3.6850, 4.4045],\n",
      "        [3.6351, 4.3676],\n",
      "        [3.6222, 4.3700],\n",
      "        [3.4971, 4.2950],\n",
      "        [3.6948, 4.3992],\n",
      "        [3.5887, 4.3644],\n",
      "        [3.5640, 4.3398],\n",
      "        [3.5416, 4.3051],\n",
      "        [3.6366, 4.3618],\n",
      "        [3.6416, 4.3548],\n",
      "        [3.5699, 4.3431],\n",
      "        [3.5135, 4.2918],\n",
      "        [3.6324, 4.3516],\n",
      "        [3.6047, 4.3854],\n",
      "        [3.5511, 4.3194],\n",
      "        [3.6536, 4.3806],\n",
      "        [3.6275, 4.3780],\n",
      "        [3.6421, 4.3636],\n",
      "        [3.5884, 4.3365],\n",
      "        [3.5758, 4.3367],\n",
      "        [3.6070, 4.3629],\n",
      "        [3.5601, 4.3466],\n",
      "        [3.6549, 4.3639],\n",
      "        [3.6271, 4.3765],\n",
      "        [3.6177, 4.3726],\n",
      "        [3.6128, 4.3461],\n",
      "        [3.6139, 4.3041],\n",
      "        [3.5320, 4.2866],\n",
      "        [3.5813, 4.3563],\n",
      "        [3.6724, 4.3919],\n",
      "        [3.4299, 4.2733],\n",
      "        [3.5223, 4.3331],\n",
      "        [3.6406, 4.3754],\n",
      "        [3.5977, 4.3566],\n",
      "        [3.4868, 4.2722],\n",
      "        [3.6134, 4.3619],\n",
      "        [3.6474, 4.3681],\n",
      "        [3.5878, 4.3703],\n",
      "        [3.5367, 4.3183],\n",
      "        [3.4684, 4.2746],\n",
      "        [3.6830, 4.3766],\n",
      "        [3.5762, 4.3136],\n",
      "        [3.6424, 4.3899],\n",
      "        [3.4972, 4.2969],\n",
      "        [3.5918, 4.3651],\n",
      "        [3.6565, 4.3807],\n",
      "        [3.5451, 4.3221],\n",
      "        [3.6460, 4.3866],\n",
      "        [3.5811, 4.3045],\n",
      "        [3.6579, 4.3829],\n",
      "        [3.6542, 4.3833],\n",
      "        [3.5512, 4.3355],\n",
      "        [3.6411, 4.3831],\n",
      "        [3.6477, 4.3837],\n",
      "        [3.5187, 4.2568],\n",
      "        [3.5950, 4.3624],\n",
      "        [3.5940, 4.3450],\n",
      "        [3.5605, 4.3493],\n",
      "        [3.6591, 4.4019],\n",
      "        [3.5238, 4.3190],\n",
      "        [3.6235, 4.3654],\n",
      "        [3.6394, 4.3831],\n",
      "        [3.6354, 4.3865],\n",
      "        [3.6517, 4.3516],\n",
      "        [3.6927, 4.4063],\n",
      "        [3.6260, 4.3722],\n",
      "        [3.5481, 4.3339],\n",
      "        [3.5835, 4.3159],\n",
      "        [3.5553, 4.3522],\n",
      "        [3.6556, 4.3873],\n",
      "        [3.5658, 4.3437],\n",
      "        [3.6338, 4.3507],\n",
      "        [3.6546, 4.3962],\n",
      "        [3.5840, 4.3739],\n",
      "        [3.6988, 4.3912],\n",
      "        [3.5752, 4.3518],\n",
      "        [3.5771, 4.3184],\n",
      "        [3.5914, 4.3449],\n",
      "        [3.5743, 4.3087],\n",
      "        [3.6166, 4.3706],\n",
      "        [3.6067, 4.3679],\n",
      "        [3.6598, 4.3849],\n",
      "        [3.5059, 4.3270],\n",
      "        [3.5557, 4.3074],\n",
      "        [3.6712, 4.3839],\n",
      "        [3.6224, 4.3727],\n",
      "        [3.6244, 4.3573],\n",
      "        [3.6626, 4.3613],\n",
      "        [3.4639, 4.2931],\n",
      "        [3.4023, 4.2322],\n",
      "        [3.6032, 4.3471],\n",
      "        [3.6124, 4.3219],\n",
      "        [3.4797, 4.3132],\n",
      "        [3.6005, 4.3457],\n",
      "        [3.6700, 4.3846]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5778, 4.4015]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5345, 4.3900]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.6526, 4.4780]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5093, 4.4229]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5636, 4.4162]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.4138, 4.3142],\n",
      "        [3.5947, 4.4553],\n",
      "        [3.5438, 4.4332],\n",
      "        [3.2977, 4.3180],\n",
      "        [3.5645, 4.3931],\n",
      "        [3.5992, 4.4634],\n",
      "        [3.6332, 4.4530],\n",
      "        [3.5146, 4.3839],\n",
      "        [3.5239, 4.4068],\n",
      "        [3.5833, 4.4186],\n",
      "        [3.6086, 4.4433],\n",
      "        [3.5899, 4.4552],\n",
      "        [3.4763, 4.3735],\n",
      "        [3.3973, 4.3442],\n",
      "        [3.5124, 4.4186],\n",
      "        [3.5211, 4.4043],\n",
      "        [3.4859, 4.4066],\n",
      "        [3.4867, 4.3906],\n",
      "        [3.6035, 4.4415],\n",
      "        [3.5259, 4.4140],\n",
      "        [3.5889, 4.4586],\n",
      "        [3.5786, 4.4262],\n",
      "        [3.6025, 4.4418],\n",
      "        [3.5464, 4.4275],\n",
      "        [3.4022, 4.3422],\n",
      "        [3.6290, 4.4633],\n",
      "        [3.6383, 4.4669],\n",
      "        [3.6390, 4.4550],\n",
      "        [3.4344, 4.3526],\n",
      "        [3.5651, 4.4324],\n",
      "        [3.5310, 4.4155],\n",
      "        [3.5946, 4.4460],\n",
      "        [3.6169, 4.4477],\n",
      "        [3.5356, 4.4340],\n",
      "        [3.5722, 4.4461],\n",
      "        [3.5575, 4.4530],\n",
      "        [3.5170, 4.4277],\n",
      "        [3.5607, 4.4263],\n",
      "        [3.5748, 4.4433],\n",
      "        [3.5286, 4.3786],\n",
      "        [3.5031, 4.4050],\n",
      "        [3.5686, 4.4205],\n",
      "        [3.4587, 4.3608],\n",
      "        [3.4922, 4.4066],\n",
      "        [3.5346, 4.4188],\n",
      "        [3.5308, 4.4162],\n",
      "        [3.5575, 4.4342],\n",
      "        [3.5202, 4.4208],\n",
      "        [3.5177, 4.4200],\n",
      "        [3.4744, 4.3889],\n",
      "        [3.5597, 4.4471],\n",
      "        [3.5153, 4.4123],\n",
      "        [3.5668, 4.4167],\n",
      "        [3.5416, 4.4305],\n",
      "        [3.5895, 4.4120],\n",
      "        [3.4972, 4.4045],\n",
      "        [3.5924, 4.4405],\n",
      "        [3.4992, 4.4252],\n",
      "        [3.6338, 4.4528],\n",
      "        [3.5719, 4.4347],\n",
      "        [3.6165, 4.4435],\n",
      "        [3.5493, 4.4311],\n",
      "        [3.6054, 4.4560],\n",
      "        [3.5253, 4.4192],\n",
      "        [3.5646, 4.4355],\n",
      "        [3.5884, 4.4483],\n",
      "        [3.5953, 4.4612],\n",
      "        [3.5443, 4.4181],\n",
      "        [3.6329, 4.4410],\n",
      "        [3.6078, 4.4452],\n",
      "        [3.5446, 4.4056],\n",
      "        [3.6042, 4.4493],\n",
      "        [3.5184, 4.4116],\n",
      "        [3.5821, 4.4256],\n",
      "        [3.5458, 4.4477],\n",
      "        [3.4520, 4.4060],\n",
      "        [3.4713, 4.3981],\n",
      "        [3.5582, 4.4517],\n",
      "        [3.5128, 4.4211],\n",
      "        [3.4297, 4.3801],\n",
      "        [3.5509, 4.3783],\n",
      "        [3.5487, 4.4361],\n",
      "        [3.6280, 4.4392],\n",
      "        [3.5548, 4.4233],\n",
      "        [3.4512, 4.3826],\n",
      "        [3.4846, 4.4123],\n",
      "        [3.5857, 4.4413],\n",
      "        [3.6031, 4.4405],\n",
      "        [3.5522, 4.4146],\n",
      "        [3.5303, 4.3992],\n",
      "        [3.5640, 4.4510],\n",
      "        [3.5040, 4.4027],\n",
      "        [3.5439, 4.4173],\n",
      "        [3.4288, 4.3498],\n",
      "        [3.5600, 4.3722],\n",
      "        [3.5733, 4.3977],\n",
      "        [3.5966, 4.4323],\n",
      "        [3.4997, 4.4067],\n",
      "        [3.5980, 4.4437],\n",
      "        [3.5150, 4.4276]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior_param_sample:  tensor([[3.5720, 4.5042]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5428, 4.4862]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.4498, 4.4874]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4923, 4.4914]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5246, 4.4931]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.5576, 4.5151],\n",
      "        [3.6665, 4.5440],\n",
      "        [3.5028, 4.4555],\n",
      "        [3.4768, 4.4727],\n",
      "        [3.5225, 4.4985],\n",
      "        [3.5761, 4.5218],\n",
      "        [3.5950, 4.5233],\n",
      "        [3.5073, 4.4922],\n",
      "        [3.5079, 4.5000],\n",
      "        [3.4597, 4.4468],\n",
      "        [3.4728, 4.4614],\n",
      "        [3.4381, 4.4591],\n",
      "        [3.5275, 4.5123],\n",
      "        [3.5886, 4.5113],\n",
      "        [3.4611, 4.4679],\n",
      "        [3.6027, 4.5216],\n",
      "        [3.5381, 4.4846],\n",
      "        [3.4193, 4.4347],\n",
      "        [3.5212, 4.4734],\n",
      "        [3.5701, 4.5076],\n",
      "        [3.4042, 4.4124],\n",
      "        [3.4773, 4.4964],\n",
      "        [3.4696, 4.4724],\n",
      "        [3.5577, 4.5207],\n",
      "        [3.4973, 4.4766],\n",
      "        [3.4694, 4.4905],\n",
      "        [3.5875, 4.5120],\n",
      "        [3.4737, 4.4849],\n",
      "        [3.4763, 4.4929],\n",
      "        [3.5785, 4.4822],\n",
      "        [3.5357, 4.5181],\n",
      "        [3.5456, 4.5040],\n",
      "        [3.4647, 4.4829],\n",
      "        [3.6216, 4.5135],\n",
      "        [3.4664, 4.4463],\n",
      "        [3.4621, 4.4653],\n",
      "        [3.5054, 4.5015],\n",
      "        [3.5711, 4.5239],\n",
      "        [3.5611, 4.4900],\n",
      "        [3.5004, 4.4638],\n",
      "        [3.3322, 4.4058],\n",
      "        [3.5474, 4.5052],\n",
      "        [3.4389, 4.4642],\n",
      "        [3.5411, 4.5125],\n",
      "        [3.5537, 4.4855],\n",
      "        [3.5536, 4.4929],\n",
      "        [3.4169, 4.4651],\n",
      "        [3.5610, 4.5171],\n",
      "        [3.5834, 4.5347],\n",
      "        [3.5322, 4.5126],\n",
      "        [3.4491, 4.4561],\n",
      "        [3.4972, 4.4437],\n",
      "        [3.5858, 4.5255],\n",
      "        [3.6053, 4.5244],\n",
      "        [3.6358, 4.5369],\n",
      "        [3.3835, 4.4478],\n",
      "        [3.5596, 4.4993],\n",
      "        [3.3794, 4.4286],\n",
      "        [3.4200, 4.4593],\n",
      "        [3.5204, 4.5040],\n",
      "        [3.5442, 4.5112],\n",
      "        [3.5223, 4.5058],\n",
      "        [3.6409, 4.5462],\n",
      "        [3.5195, 4.4722],\n",
      "        [3.5243, 4.4834],\n",
      "        [3.4875, 4.4840],\n",
      "        [3.4530, 4.4729],\n",
      "        [3.3735, 4.4190],\n",
      "        [3.6167, 4.5248],\n",
      "        [3.5186, 4.4589],\n",
      "        [3.3630, 4.4332],\n",
      "        [3.5916, 4.5133],\n",
      "        [3.3907, 4.4662],\n",
      "        [3.6094, 4.5414],\n",
      "        [3.5674, 4.5175],\n",
      "        [3.6074, 4.5300],\n",
      "        [3.4872, 4.4959],\n",
      "        [3.5366, 4.4913],\n",
      "        [3.5137, 4.4813],\n",
      "        [3.5316, 4.4870],\n",
      "        [3.5149, 4.4851],\n",
      "        [3.5796, 4.5259],\n",
      "        [3.5591, 4.4984],\n",
      "        [3.5116, 4.5025],\n",
      "        [3.4955, 4.4584],\n",
      "        [3.4594, 4.4401],\n",
      "        [3.3450, 4.4387],\n",
      "        [3.5807, 4.5218],\n",
      "        [3.5404, 4.4966],\n",
      "        [3.5070, 4.4611],\n",
      "        [3.4832, 4.4995],\n",
      "        [3.6425, 4.5377],\n",
      "        [3.4291, 4.4242],\n",
      "        [3.4204, 4.4249],\n",
      "        [3.5658, 4.5197],\n",
      "        [3.4825, 4.4602],\n",
      "        [3.3945, 4.4283],\n",
      "        [3.5838, 4.5339],\n",
      "        [3.4627, 4.4725],\n",
      "        [3.4223, 4.4758]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4176, 4.5150]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.2532, 4.4402]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.4152, 4.4999]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4350, 4.5320]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4902, 4.5521]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.4610, 4.5115],\n",
      "        [3.5805, 4.5972],\n",
      "        [3.4410, 4.5377],\n",
      "        [3.5124, 4.5742],\n",
      "        [3.5830, 4.5838],\n",
      "        [3.6187, 4.5991],\n",
      "        [3.4491, 4.5431],\n",
      "        [3.5909, 4.5637],\n",
      "        [3.4895, 4.5479],\n",
      "        [3.5072, 4.5675],\n",
      "        [3.6185, 4.5965],\n",
      "        [3.4777, 4.5088],\n",
      "        [3.5754, 4.5520],\n",
      "        [3.5828, 4.5835],\n",
      "        [3.5487, 4.5372],\n",
      "        [3.4247, 4.4701],\n",
      "        [3.5095, 4.5573],\n",
      "        [3.5370, 4.5966],\n",
      "        [3.4646, 4.5458],\n",
      "        [3.4968, 4.5341],\n",
      "        [3.5131, 4.5691],\n",
      "        [3.5118, 4.5552],\n",
      "        [3.5244, 4.5712],\n",
      "        [3.4304, 4.4713],\n",
      "        [3.5420, 4.5517],\n",
      "        [3.4819, 4.5366],\n",
      "        [3.5260, 4.5645],\n",
      "        [3.4044, 4.4845],\n",
      "        [3.5460, 4.5656],\n",
      "        [3.5422, 4.5559],\n",
      "        [3.5702, 4.5881],\n",
      "        [3.4738, 4.5411],\n",
      "        [3.4899, 4.5563],\n",
      "        [3.5263, 4.5348],\n",
      "        [3.5026, 4.5487],\n",
      "        [3.5727, 4.5827],\n",
      "        [3.4153, 4.5208],\n",
      "        [3.5150, 4.5580],\n",
      "        [3.5485, 4.5675],\n",
      "        [3.4614, 4.5437],\n",
      "        [3.5141, 4.5572],\n",
      "        [3.4714, 4.5561],\n",
      "        [3.6318, 4.5864],\n",
      "        [3.4610, 4.5402],\n",
      "        [3.5495, 4.5554],\n",
      "        [3.4390, 4.5109],\n",
      "        [3.4819, 4.5272],\n",
      "        [3.4935, 4.5473],\n",
      "        [3.6031, 4.5842],\n",
      "        [3.5101, 4.5407],\n",
      "        [3.6133, 4.5987],\n",
      "        [3.3619, 4.4981],\n",
      "        [3.5814, 4.5843],\n",
      "        [3.5525, 4.5570],\n",
      "        [3.5521, 4.5650],\n",
      "        [3.6105, 4.6098],\n",
      "        [3.4970, 4.5405],\n",
      "        [3.5494, 4.5499],\n",
      "        [3.4924, 4.5438],\n",
      "        [3.5037, 4.5459],\n",
      "        [3.4777, 4.5337],\n",
      "        [3.4798, 4.5073],\n",
      "        [3.5677, 4.5748],\n",
      "        [3.3806, 4.4605],\n",
      "        [3.4311, 4.5121],\n",
      "        [3.4712, 4.5389],\n",
      "        [3.5592, 4.5758],\n",
      "        [3.5915, 4.5800],\n",
      "        [3.3686, 4.4910],\n",
      "        [3.5683, 4.5717],\n",
      "        [3.3507, 4.5076],\n",
      "        [3.5238, 4.5693],\n",
      "        [3.3020, 4.4733],\n",
      "        [3.6304, 4.5948],\n",
      "        [3.4809, 4.5458],\n",
      "        [3.4710, 4.5365],\n",
      "        [3.6275, 4.6092],\n",
      "        [3.3571, 4.4694],\n",
      "        [3.3894, 4.5154],\n",
      "        [3.5294, 4.5161],\n",
      "        [3.3818, 4.4952],\n",
      "        [3.5665, 4.5513],\n",
      "        [3.5885, 4.5924],\n",
      "        [3.5481, 4.5499],\n",
      "        [3.4413, 4.5158],\n",
      "        [3.5941, 4.5870],\n",
      "        [3.3353, 4.4844],\n",
      "        [3.3259, 4.4311],\n",
      "        [3.4436, 4.5388],\n",
      "        [3.6235, 4.6043],\n",
      "        [3.5291, 4.5717],\n",
      "        [3.6249, 4.5956],\n",
      "        [3.4271, 4.5360],\n",
      "        [3.5350, 4.5336],\n",
      "        [3.5750, 4.5980],\n",
      "        [3.3023, 4.4491],\n",
      "        [3.2829, 4.3849],\n",
      "        [3.4560, 4.5048],\n",
      "        [3.4249, 4.5135],\n",
      "        [3.4616, 4.5537]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4349, 4.5892]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5000, 4.6321]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.5348, 4.6187]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5398, 4.6256]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.3545, 4.5325]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.4246, 4.5256],\n",
      "        [3.5774, 4.6478],\n",
      "        [3.5483, 4.6309],\n",
      "        [3.5377, 4.6450],\n",
      "        [3.3765, 4.5091],\n",
      "        [3.3786, 4.5715],\n",
      "        [3.5873, 4.6531],\n",
      "        [3.4817, 4.5732],\n",
      "        [3.4270, 4.5906],\n",
      "        [3.3504, 4.5487],\n",
      "        [3.5475, 4.6079],\n",
      "        [3.5523, 4.6334],\n",
      "        [3.5566, 4.6239],\n",
      "        [3.4931, 4.6021],\n",
      "        [3.4655, 4.5872],\n",
      "        [3.6639, 4.6730],\n",
      "        [3.4763, 4.5992],\n",
      "        [3.5928, 4.6343],\n",
      "        [3.5065, 4.5648],\n",
      "        [3.4764, 4.6316],\n",
      "        [3.4941, 4.6153],\n",
      "        [3.6052, 4.6480],\n",
      "        [3.4863, 4.6010],\n",
      "        [3.4333, 4.5926],\n",
      "        [3.3600, 4.5433],\n",
      "        [3.6490, 4.6596],\n",
      "        [3.6034, 4.6519],\n",
      "        [3.3871, 4.5649],\n",
      "        [3.4862, 4.5924],\n",
      "        [3.3306, 4.5493],\n",
      "        [3.5018, 4.6261],\n",
      "        [3.3664, 4.5361],\n",
      "        [3.5718, 4.6485],\n",
      "        [3.6344, 4.6643],\n",
      "        [3.3546, 4.5234],\n",
      "        [3.5604, 4.6217],\n",
      "        [3.5099, 4.6099],\n",
      "        [3.5802, 4.6542],\n",
      "        [3.2912, 4.5373],\n",
      "        [3.4586, 4.5950],\n",
      "        [3.5514, 4.6413],\n",
      "        [3.3876, 4.5631],\n",
      "        [3.5904, 4.6048],\n",
      "        [3.4850, 4.5945],\n",
      "        [3.5907, 4.6497],\n",
      "        [3.4605, 4.5800],\n",
      "        [3.4705, 4.6100],\n",
      "        [3.5535, 4.6313],\n",
      "        [3.5100, 4.6070],\n",
      "        [3.4904, 4.6062],\n",
      "        [3.4653, 4.5873],\n",
      "        [3.4998, 4.5893],\n",
      "        [3.4528, 4.5650],\n",
      "        [3.4871, 4.6101],\n",
      "        [3.5034, 4.6123],\n",
      "        [3.4507, 4.5335],\n",
      "        [3.5010, 4.5876],\n",
      "        [3.5124, 4.5782],\n",
      "        [3.5269, 4.6300],\n",
      "        [3.4888, 4.5983],\n",
      "        [3.5160, 4.6376],\n",
      "        [3.3895, 4.5485],\n",
      "        [3.4568, 4.6076],\n",
      "        [3.4464, 4.5687],\n",
      "        [3.4974, 4.5830],\n",
      "        [3.4183, 4.5680],\n",
      "        [3.4001, 4.5302],\n",
      "        [3.5228, 4.6237],\n",
      "        [3.3908, 4.5519],\n",
      "        [3.5470, 4.6117],\n",
      "        [3.4350, 4.6020],\n",
      "        [3.3470, 4.5505],\n",
      "        [3.4418, 4.5752],\n",
      "        [3.5267, 4.6326],\n",
      "        [3.6097, 4.6576],\n",
      "        [3.5396, 4.5995],\n",
      "        [3.3434, 4.5339],\n",
      "        [3.4751, 4.5835],\n",
      "        [3.5847, 4.6448],\n",
      "        [3.4275, 4.5818],\n",
      "        [3.3219, 4.5244],\n",
      "        [3.6038, 4.6489],\n",
      "        [3.4946, 4.5950],\n",
      "        [3.5836, 4.6393],\n",
      "        [3.4491, 4.6003],\n",
      "        [3.4987, 4.5838],\n",
      "        [3.4995, 4.6082],\n",
      "        [3.4663, 4.6016],\n",
      "        [3.5474, 4.6334],\n",
      "        [3.6169, 4.6559],\n",
      "        [3.5324, 4.6011],\n",
      "        [3.6019, 4.6233],\n",
      "        [3.4294, 4.5456],\n",
      "        [3.1915, 4.4618],\n",
      "        [3.4489, 4.5845],\n",
      "        [3.5665, 4.6467],\n",
      "        [3.5087, 4.6311],\n",
      "        [3.4760, 4.5613],\n",
      "        [3.4890, 4.6116],\n",
      "        [3.3815, 4.5109]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5327, 4.6735]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5327, 4.6160]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.4710, 4.6639]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5672, 4.6721]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.6974, 4.7487]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.6161, 4.6853],\n",
      "        [3.6952, 4.7136],\n",
      "        [3.5305, 4.6825],\n",
      "        [3.5823, 4.6885],\n",
      "        [3.3324, 4.6014],\n",
      "        [3.4921, 4.6773],\n",
      "        [3.4328, 4.6338],\n",
      "        [3.1728, 4.4833],\n",
      "        [3.5437, 4.6773],\n",
      "        [3.6155, 4.7027],\n",
      "        [3.5242, 4.6836],\n",
      "        [3.6010, 4.7153],\n",
      "        [3.6461, 4.7270],\n",
      "        [3.5678, 4.6904],\n",
      "        [3.5989, 4.7013],\n",
      "        [3.4140, 4.5949],\n",
      "        [3.5482, 4.6662],\n",
      "        [3.3214, 4.4839],\n",
      "        [3.3785, 4.6052],\n",
      "        [3.5541, 4.6805],\n",
      "        [3.6118, 4.7030],\n",
      "        [3.5948, 4.6892],\n",
      "        [3.6032, 4.6954],\n",
      "        [3.4463, 4.6671],\n",
      "        [3.5916, 4.7033],\n",
      "        [3.5672, 4.6492],\n",
      "        [3.1822, 4.5363],\n",
      "        [3.6057, 4.7090],\n",
      "        [3.7093, 4.7450],\n",
      "        [3.5605, 4.6606],\n",
      "        [3.3849, 4.6083],\n",
      "        [3.5269, 4.6858],\n",
      "        [3.4494, 4.6309],\n",
      "        [3.5846, 4.6942],\n",
      "        [3.5394, 4.6842],\n",
      "        [3.5407, 4.6833],\n",
      "        [3.5945, 4.7010],\n",
      "        [3.6178, 4.7111],\n",
      "        [3.3531, 4.6169],\n",
      "        [3.4292, 4.6548],\n",
      "        [3.5661, 4.6966],\n",
      "        [3.5434, 4.6790],\n",
      "        [3.5217, 4.6697],\n",
      "        [3.3948, 4.6215],\n",
      "        [3.5674, 4.6679],\n",
      "        [3.4083, 4.5955],\n",
      "        [3.6534, 4.7180],\n",
      "        [3.4459, 4.6432],\n",
      "        [3.3276, 4.6046],\n",
      "        [3.5468, 4.6901],\n",
      "        [3.6369, 4.7231],\n",
      "        [3.4311, 4.6196],\n",
      "        [3.5585, 4.6908],\n",
      "        [3.5453, 4.6903],\n",
      "        [3.5460, 4.6642],\n",
      "        [3.6030, 4.7105],\n",
      "        [3.5839, 4.6849],\n",
      "        [3.6235, 4.6843],\n",
      "        [3.4754, 4.6254],\n",
      "        [3.5878, 4.6968],\n",
      "        [3.6433, 4.7284],\n",
      "        [3.5108, 4.6947],\n",
      "        [3.5359, 4.6819],\n",
      "        [3.6689, 4.7205],\n",
      "        [3.0775, 4.4721],\n",
      "        [3.4018, 4.6142],\n",
      "        [3.5421, 4.6764],\n",
      "        [3.5580, 4.6876],\n",
      "        [3.5254, 4.6635],\n",
      "        [3.5805, 4.6878],\n",
      "        [3.4936, 4.6488],\n",
      "        [3.4093, 4.6455],\n",
      "        [3.4594, 4.6599],\n",
      "        [3.5394, 4.6586],\n",
      "        [3.5936, 4.6958],\n",
      "        [3.4739, 4.6567],\n",
      "        [3.5671, 4.6955],\n",
      "        [3.4016, 4.5926],\n",
      "        [3.4623, 4.6361],\n",
      "        [3.6144, 4.7097],\n",
      "        [3.3899, 4.6111],\n",
      "        [3.5233, 4.6802],\n",
      "        [3.7025, 4.7446],\n",
      "        [3.5449, 4.6798],\n",
      "        [3.5513, 4.6558],\n",
      "        [3.6381, 4.7205],\n",
      "        [3.6066, 4.7179],\n",
      "        [3.4104, 4.6133],\n",
      "        [3.6008, 4.7050],\n",
      "        [3.6463, 4.7265],\n",
      "        [3.6752, 4.7311],\n",
      "        [3.5761, 4.6819],\n",
      "        [3.4423, 4.6438],\n",
      "        [3.3574, 4.6032],\n",
      "        [3.5948, 4.7022],\n",
      "        [3.5590, 4.6916],\n",
      "        [3.5356, 4.6814],\n",
      "        [3.4911, 4.6620],\n",
      "        [3.5260, 4.6876],\n",
      "        [3.6139, 4.7060]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posterior_param_sample:  tensor([[3.5721, 4.7482]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.3867, 4.6714]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.4912, 4.7343]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.1735, 4.5453]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5453, 4.7257]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.4559, 4.6873],\n",
      "        [3.6558, 4.7797],\n",
      "        [3.7111, 4.7895],\n",
      "        [3.6621, 4.7720],\n",
      "        [3.5844, 4.7531],\n",
      "        [3.2980, 4.6003],\n",
      "        [3.3494, 4.6546],\n",
      "        [3.4851, 4.6643],\n",
      "        [3.5774, 4.6994],\n",
      "        [3.5086, 4.7114],\n",
      "        [3.7506, 4.8058],\n",
      "        [3.5255, 4.7100],\n",
      "        [3.6378, 4.7136],\n",
      "        [3.5437, 4.7025],\n",
      "        [3.4704, 4.6571],\n",
      "        [3.4780, 4.6573],\n",
      "        [3.6762, 4.7617],\n",
      "        [3.6581, 4.7732],\n",
      "        [3.5601, 4.7407],\n",
      "        [3.3363, 4.6490],\n",
      "        [3.2881, 4.6320],\n",
      "        [3.4022, 4.6261],\n",
      "        [3.5832, 4.7494],\n",
      "        [3.4719, 4.6613],\n",
      "        [3.3690, 4.6584],\n",
      "        [3.4870, 4.7149],\n",
      "        [3.5244, 4.7385],\n",
      "        [3.5752, 4.7179],\n",
      "        [3.6373, 4.7456],\n",
      "        [3.6710, 4.7732],\n",
      "        [3.5221, 4.7199],\n",
      "        [3.4474, 4.6577],\n",
      "        [3.2683, 4.5505],\n",
      "        [3.7154, 4.7709],\n",
      "        [3.4220, 4.6554],\n",
      "        [3.7000, 4.7898],\n",
      "        [3.5083, 4.7080],\n",
      "        [3.5534, 4.7397],\n",
      "        [3.5162, 4.7011],\n",
      "        [3.6783, 4.7806],\n",
      "        [3.5949, 4.7498],\n",
      "        [3.6618, 4.7673],\n",
      "        [3.6093, 4.7473],\n",
      "        [3.7330, 4.7981],\n",
      "        [3.6970, 4.7903],\n",
      "        [3.5491, 4.6834],\n",
      "        [3.5143, 4.6867],\n",
      "        [3.7589, 4.7975],\n",
      "        [3.4046, 4.6275],\n",
      "        [3.6120, 4.7195],\n",
      "        [3.5526, 4.7372],\n",
      "        [3.6294, 4.7753],\n",
      "        [3.5302, 4.7309],\n",
      "        [3.4305, 4.6805],\n",
      "        [3.2885, 4.6039],\n",
      "        [3.5630, 4.6951],\n",
      "        [3.6874, 4.7710],\n",
      "        [3.6483, 4.7430],\n",
      "        [3.4412, 4.6783],\n",
      "        [3.6575, 4.7635],\n",
      "        [3.5876, 4.7496],\n",
      "        [3.6504, 4.7652],\n",
      "        [3.4472, 4.6752],\n",
      "        [3.5656, 4.7423],\n",
      "        [3.5418, 4.7234],\n",
      "        [3.5323, 4.7033],\n",
      "        [3.3998, 4.6465],\n",
      "        [3.5383, 4.7369],\n",
      "        [3.5284, 4.7065],\n",
      "        [3.5920, 4.7248],\n",
      "        [3.7022, 4.7715],\n",
      "        [3.3923, 4.6608],\n",
      "        [3.7262, 4.7817],\n",
      "        [3.6297, 4.7417],\n",
      "        [3.5640, 4.7128],\n",
      "        [3.4559, 4.6884],\n",
      "        [3.4262, 4.6457],\n",
      "        [3.6149, 4.7109],\n",
      "        [3.4274, 4.6440],\n",
      "        [3.4799, 4.6745],\n",
      "        [3.5027, 4.6887],\n",
      "        [3.6338, 4.7568],\n",
      "        [3.4936, 4.7216],\n",
      "        [3.5841, 4.7360],\n",
      "        [3.7352, 4.7760],\n",
      "        [3.4995, 4.7186],\n",
      "        [3.5570, 4.7077],\n",
      "        [3.2986, 4.5458],\n",
      "        [3.5431, 4.7281],\n",
      "        [3.5621, 4.7356],\n",
      "        [3.5037, 4.6922],\n",
      "        [3.4709, 4.7078],\n",
      "        [3.6544, 4.7396],\n",
      "        [3.6377, 4.7512],\n",
      "        [3.5267, 4.7370],\n",
      "        [3.4716, 4.7082],\n",
      "        [3.6629, 4.7547],\n",
      "        [3.2557, 4.6155],\n",
      "        [3.6317, 4.7635],\n",
      "        [3.7092, 4.7990]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4152, 4.6726]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.5405, 4.7575]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "\n",
      " \n",
      "\n",
      "X shape: torch.Size([100, 1])\n",
      "Y shape: torch.Size([100, 1])\n",
      "dy_dt shape: torch.Size([100, 1])\n",
      "d2y_dx2 shape: torch.Size([100, 1])\n",
      "predicted params torch.Size([1, 2])\n",
      "posterior_param_sample:  tensor([[3.5059, 4.7132]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.7334, 4.8173]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior_param_sample:  tensor([[3.4966, 4.7115]], device='mps:0', grad_fn=<LinearBackward0>)\n",
      "posterior samples in closure after update:  tensor([[3.5248, 4.7150],\n",
      "        [3.4886, 4.7072],\n",
      "        [3.7093, 4.8218],\n",
      "        [3.7636, 4.8151],\n",
      "        [3.7152, 4.8172],\n",
      "        [3.4387, 4.6905],\n",
      "        [3.5441, 4.7548],\n",
      "        [3.7498, 4.8184],\n",
      "        [3.5471, 4.7684],\n",
      "        [3.7903, 4.8455],\n",
      "        [3.5344, 4.7245],\n",
      "        [3.5128, 4.7427],\n",
      "        [3.3611, 4.6829],\n",
      "        [3.4636, 4.7458],\n",
      "        [3.7636, 4.8399],\n",
      "        [3.5662, 4.7689],\n",
      "        [3.7308, 4.8245],\n",
      "        [3.5008, 4.7058],\n",
      "        [3.4789, 4.7254],\n",
      "        [3.7077, 4.8119],\n",
      "        [3.3392, 4.6827],\n",
      "        [3.6405, 4.7999],\n",
      "        [3.4467, 4.6938],\n",
      "        [3.6195, 4.7807],\n",
      "        [3.6349, 4.7665],\n",
      "        [3.5815, 4.7727],\n",
      "        [3.8120, 4.8489],\n",
      "        [3.5067, 4.7344],\n",
      "        [3.6685, 4.8194],\n",
      "        [3.5335, 4.7422],\n",
      "        [3.4734, 4.7331],\n",
      "        [3.4813, 4.7384],\n",
      "        [3.6597, 4.7820],\n",
      "        [3.3754, 4.7117],\n",
      "        [3.6776, 4.8183],\n",
      "        [3.7387, 4.8441],\n",
      "        [3.6800, 4.8209],\n",
      "        [3.5353, 4.7589],\n",
      "        [3.6837, 4.8149],\n",
      "        [3.6086, 4.8024],\n",
      "        [3.6396, 4.7828],\n",
      "        [3.4525, 4.6661],\n",
      "        [3.5415, 4.7475],\n",
      "        [3.4042, 4.7243],\n",
      "        [3.5798, 4.7728],\n",
      "        [3.3860, 4.6932],\n",
      "        [3.7256, 4.7887],\n",
      "        [3.7191, 4.8267],\n",
      "        [3.5194, 4.7322],\n",
      "        [3.7486, 4.8372],\n",
      "        [3.6928, 4.8052],\n",
      "        [3.5776, 4.7708],\n",
      "        [3.4870, 4.6448],\n",
      "        [3.4224, 4.6921],\n",
      "        [3.5337, 4.7545],\n",
      "        [3.5677, 4.7547],\n",
      "        [3.6062, 4.7878],\n",
      "        [3.5876, 4.7442],\n",
      "        [3.6159, 4.7951],\n",
      "        [3.5274, 4.7722],\n",
      "        [3.7539, 4.8133],\n",
      "        [3.7025, 4.8104],\n",
      "        [3.6165, 4.7938],\n",
      "        [3.6035, 4.7824],\n",
      "        [3.5896, 4.7754],\n",
      "        [3.5103, 4.7357],\n",
      "        [3.7191, 4.8201],\n",
      "        [3.6369, 4.8099],\n",
      "        [3.7281, 4.8311],\n",
      "        [3.4819, 4.7406],\n",
      "        [3.6157, 4.7900],\n",
      "        [3.6360, 4.8000],\n",
      "        [3.7056, 4.8049],\n",
      "        [3.7223, 4.8242],\n",
      "        [3.5744, 4.7704],\n",
      "        [3.7260, 4.8323],\n",
      "        [3.5142, 4.7351],\n",
      "        [3.6348, 4.7866],\n",
      "        [3.6102, 4.7816],\n",
      "        [3.6932, 4.8206],\n",
      "        [3.0828, 4.4986],\n",
      "        [3.6300, 4.7804],\n",
      "        [3.6728, 4.7906],\n",
      "        [3.6101, 4.7863],\n",
      "        [3.5621, 4.7914],\n",
      "        [3.5580, 4.7576],\n",
      "        [3.4764, 4.5942],\n",
      "        [3.5877, 4.7880],\n",
      "        [3.7097, 4.8193],\n",
      "        [3.4995, 4.7191],\n",
      "        [3.6847, 4.8049],\n",
      "        [3.5614, 4.7656],\n",
      "        [3.5851, 4.7748],\n",
      "        [3.6619, 4.7831],\n",
      "        [3.4492, 4.7174],\n",
      "        [3.4965, 4.7033],\n",
      "        [3.4439, 4.6910],\n",
      "        [3.5740, 4.7070],\n",
      "        [3.6654, 4.8000],\n",
      "        [3.5221, 4.7238]], device='mps:0', grad_fn=<LinearBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m25000\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 255\u001b[0m, in \u001b[0;36mExperiment.train\u001b[0;34m(self, epochs, optimizer, num_samples, **kwargs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure)\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    257\u001b[0m         loss_ic, loss_interior, loss_data, loss_bc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_losses()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1], line 239\u001b[0m, in \u001b[0;36mExperiment.closure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std_squared_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_param_std \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_params)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    237\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std_squared_loss\n\u001b[0;32m--> 239\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "net.train(25000, optimizer='Adam', lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': net.state_dict(),\n",
    "    'optimizer_state_dict': net.optimizer.state_dict(),\n",
    "    'total_loss_history': net.total_loss_history,\n",
    "    'loss_ic_history': net.loss_ic_history,\n",
    "    'loss_interior_history': net.loss_interior_history,\n",
    "    'loss_data_history': net.loss_data_history,\n",
    "    'loss_bc_history': net.loss_bc_history,\n",
    "    'loss_std_history': net.loss_std_history\n",
    "    # Add other variables if needed\n",
    "}, f'model_checkpoint_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "jsZSJLXXUy0H",
    "outputId": "fc8f9235-4fe7-4164-a431-b9856984f1cc"
   },
   "outputs": [],
   "source": [
    "make_plot(net, device=net.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "oHjKd828U01l",
    "outputId": "8efc6f2e-7e8c-4061-ee3d-ab295ba79b47"
   },
   "outputs": [],
   "source": [
    "plot_residuals(net, device=net.device, noise=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history_log(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyC72D9mU3GD"
   },
   "outputs": [],
   "source": [
    "samples = net.sample_parameter_posterior(num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoeVq4xpU46Z",
    "outputId": "d8a24f4f-9c86-429e-d0c9-8e305b59f807"
   },
   "outputs": [],
   "source": [
    "samples.mean(axis=0), samples.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2 = samples.cpu().detach().unbind(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "wdGgVRYjU6yW",
    "outputId": "97451fa7-84ec-48fc-c960-c576a5fc0621"
   },
   "outputs": [],
   "source": [
    "g  = sns.kdeplot(p1, fill=True, palette=\"crest\",\n",
    "                 alpha=.5, linewidth=1)\n",
    "\n",
    "\n",
    "g  = sns.kdeplot(p2, fill=True, palette=\"crest\",\n",
    "                 alpha=.5, linewidth=1)\n",
    "g.legend( ['D', 'Alpha'])\n",
    "\n",
    "# Histograms of individual parameters with adjusted line color\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['p1'], kde=True, linewidth=2, edgecolor='black')  # Change edgecolor here\n",
    "plt.title('Parameter 1 Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['p2'], kde=True, linewidth=2, edgecolor='black')  # Change edgecolor here\n",
    "plt.title('Parameter 2 Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = net.sample_parameter_posterior(num_samples=100).cpu().detach().numpy()\n",
    "\n",
    "df = pd.DataFrame(samples, columns=['Parameter 1', 'Parameter 2'])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Parameter 1'], kde=True, linewidth=2, edgecolor='blue', color='black')\n",
    "plt.title('Parameter 1 Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['Parameter 2'], kde=True, linewidth=2, edgecolor='blue', color='black')\n",
    "plt.title('Parameter 2 Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYqtbvIPU8eG"
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "t = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "X, T = torch.meshgrid(x[:, 0], t[:, 0])\n",
    "\n",
    "y_true = Experiment.exact_solution(T, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the saved model from the checkpoint\n",
    "We can load the state of model from the saved checkpoint like shown below and also other necessary states like optimizer state, losses etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model_checkpoint_trained.pth')\n",
    "\n",
    "model = Experiment()  # Create an instance of your Experiment class\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.total_loss_history = checkpoint['total_loss_history']\n",
    "model.loss_ic_history = checkpoint['loss_ic_history']\n",
    "model.loss_interior_history = checkpoint['loss_interior_history']\n",
    "model.loss_data_history = checkpoint['loss_data_history']\n",
    "model.loss_bc_history = checkpoint['loss_bc_history']\n",
    "model.loss_std_history = checkpoint['loss_std_history']\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load Optimizer's state if necessary\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we load the model, we can call the plot functions by passing the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_plot(model, device)  # Call the make_plot method to plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_history_log(model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
