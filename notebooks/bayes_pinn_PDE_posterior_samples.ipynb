{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBc_Jy8VUgOI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from numpy.ma.core import outer\n",
        "from sys import stderr\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import scatter, figure\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set(rc={'figure.figsize':(4, 4)})\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "class Experiment(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Experiment, self).__init__()\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "        print(self.device)\n",
        "\n",
        "        self.optimizer = None\n",
        "        self.train_loss_history = []\n",
        "        self.w_ic = 1.5\n",
        "        self.w_int = 1\n",
        "        self.w_data = 1\n",
        "        self.w_bc = 1.5\n",
        "        self.w_param_std = 1.0\n",
        "        self.numInputs = 2\n",
        "        self.numParams = 100\n",
        "        self.numOutputs = 1\n",
        "        self.hidden_size = 25\n",
        "\n",
        "        # Initialize history variables\n",
        "        self.total_loss_history = []\n",
        "        self.loss_ic_history = []\n",
        "        self.loss_interior_history = []\n",
        "        self.loss_data_history = []\n",
        "        self.loss_bc_history = []\n",
        "        self.loss_std_history = []\n",
        "\n",
        "        self.t0 = torch.tensor([0.0], requires_grad=True).to(self.device)\n",
        "        self.y0 = torch.tensor([1.0], requires_grad=True).to(self.device)\n",
        "\n",
        "\n",
        "#         self.predicted_params = torch.zeros((self.numParams,1), requires_grad=True, device=self.device)\n",
        "        self.predicted_params = torch.zeros((100,2), requires_grad=True, device=self.device)\n",
        "#         print(\"predicted_params inside init\", self.predicted_params)\n",
        "\n",
        "        # We only have 1 input feature\n",
        "        self.b1 = nn.Linear(self.numInputs, self.hidden_size).to(self.device)\n",
        "        self.b2 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
        "        self.b3 = nn.Linear(self.hidden_size, self.numOutputs).to(self.device)\n",
        "\n",
        "        self.t1 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
        "        self.t2 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
        "        self.t3 = nn.Linear(self.hidden_size, self.numParams).to(self.device)\n",
        "\n",
        "\n",
        "    # make this static so that it can be called independently\n",
        "    @staticmethod\n",
        "    def exact_solution(t, x):\n",
        "        return torch.exp(-t) * torch.sin(torch.pi * x)\n",
        "\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        if not torch.is_tensor(t):\n",
        "            t = torch.from_numpy(t).float().to(self.device)\n",
        "\n",
        "        if not torch.is_tensor(x):\n",
        "\n",
        "            x = torch.from_numpy(x).float().to(self.device)\n",
        "\n",
        "\n",
        "        input = torch.cat((t, x), 1)\n",
        "\n",
        "        h1 = torch.tanh(self.b1(input))\n",
        "        h2 = torch.tanh(self.b2(h1))\n",
        "\n",
        "\n",
        "\n",
        "        prior_param_sample = torch.rand(1, self.hidden_size , device=self.device).requires_grad_(True)\n",
        "        print(\"\\n Forward prior param sample \\n\", prior_param_sample)\n",
        "\n",
        "        # prior_param_sample = self.sample_parameter_posterior(num_samples=100)\n",
        "\n",
        "\n",
        "        t1 = torch.tanh(self.t1(prior_param_sample))\n",
        "        t2 = torch.tanh(self.t2(t1))\n",
        "        posterior_param_sample = self.t3(t2)\n",
        "\n",
        "\n",
        "        print(\"\\n Forward posterior_param_sample: \\n\", posterior_param_sample)\n",
        "        self.predicted_params = posterior_param_sample\n",
        "\n",
        "#         self.predicted_params = posterior_param_sample\n",
        "#         self.predicted_params = posterior_param_sample.view(-1)\n",
        "#         print(\"\\n predicted params inside forward\", self.predicted_params)\n",
        "\n",
        "        y = self.b3(torch.multiply(t2, h2))\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "    # General formula to compute the n-th order derivative of y = f(x) with respect to x\n",
        "    def compute_derivative(self, y, x, n):\n",
        "        if n == 0:\n",
        "            return y\n",
        "        else:\n",
        "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y, device= self.device),\n",
        "                                        create_graph=True, retain_graph=True,\n",
        "                                        allow_unused=True)[0]\n",
        "\n",
        "        return self.compute_derivative(dy_dx, x, n - 1)\n",
        "\n",
        "\n",
        "\n",
        "    def PDE_residual(self, t, x):\n",
        "        y = self.forward(t, x)\n",
        "        dy_dt = self.compute_derivative(y, t, 1)\n",
        "        dy_dx = self.compute_derivative(y, x, 1)\n",
        "        d2y_dx2 = self.compute_derivative(y, x, 2)\n",
        "\n",
        "        print(\"\\n \\n\")\n",
        "        print(\"X shape:\" , x.shape)\n",
        "        print(\"Y shape:\" , y.shape)\n",
        "        print(\"dy_dt shape:\" , dy_dt.shape)\n",
        "        print(\"d2y_dx2 shape:\" , d2y_dx2.shape)\n",
        "        print(\"predicted params\", self.predicted_params.shape)\n",
        "#         print(\"self.predicted_params[:,[0]] shape:\" , self.predicted_params[:,[0]].shape)\n",
        "#         print(\"self.predicted_params[:,[0]]:\", self.predicted_params[:,[0]])\n",
        "\n",
        "#         print(\"d2y_dx2 value :\", d2y_dx2.shape)\n",
        "#         print(\"predicted param :\", self.predicted_params[:,[0]].shape)\n",
        "\n",
        "\n",
        "        residual =  dy_dt - torch.multiply(self.predicted_params[:,[0]], d2y_dx2) \\\n",
        "            + torch.exp(- self.predicted_params[:,[1]] * t) * (torch.sin(torch.tensor(np.pi) * x) - torch.tensor(np.pi) ** 2 * torch.sin(torch.tensor(np.pi) * x))\n",
        "\n",
        "\n",
        "\n",
        "        return residual\n",
        "\n",
        "\n",
        "\n",
        "    def loss_initial_condition(self, num_samples=100):\n",
        "        t0 = self.t0 * torch.ones((num_samples, 1), device = self.device)\n",
        "        x = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True)  - 1.0\n",
        "        y0_pred = self.forward(t0, x)\n",
        "        y0_true = self.exact_solution(t0, x)\n",
        "        loss_ic = torch.mean(torch.square(y0_true - y0_pred))\n",
        "        return loss_ic\n",
        "\n",
        "\n",
        "    def loss_boundary_condition(self, num_samples=100):\n",
        "        x_low = -1\n",
        "        x_high = 1\n",
        "        xb_low   = x_low * torch.ones((num_samples, 1),  device = self.device)\n",
        "        xb_high  = x_high * torch.ones((num_samples, 1), device = self.device)\n",
        "\n",
        "        t = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True) - 1.0\n",
        "\n",
        "        yb_low = self.exact_solution(t, xb_low)\n",
        "        yb_high = self.exact_solution(t, xb_high)\n",
        "\n",
        "        yb_pred_low = self.forward(t, xb_low)\n",
        "        yb_pred_high = self.forward(t, xb_high)\n",
        "\n",
        "        loss_ic = torch.mean(torch.square(yb_pred_low - yb_low)) \\\n",
        "                + torch.mean(torch.square(yb_pred_high - yb_high))\n",
        "\n",
        "        return loss_ic\n",
        "\n",
        "\n",
        "    def compute_losses(self):\n",
        "        loss_ic = self.loss_initial_condition()\n",
        "        loss_interior = self.loss_interior()\n",
        "        loss_data = self.loss_data()\n",
        "        loss_bc = self.loss_boundary_condition()\n",
        "        return loss_ic, loss_interior, loss_data, loss_bc\n",
        "\n",
        "\n",
        "    def loss_data(self, num_samples=100):\n",
        "        t_data, x_data, y_data = next(iter(train_loader))\n",
        "        y_pred = self.forward(t_data, x_data)\n",
        "        loss = torch.mean(torch.square(y_pred - y_data))\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def loss_interior(self, num_samples=100):\n",
        "\n",
        "        interior_t_samples = torch.rand((num_samples, 1),   device=self.device).requires_grad_(True)\n",
        "        interior_x_samples = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True)  - 1.0\n",
        "        res = self.PDE_residual(interior_t_samples, interior_x_samples)\n",
        "        loss_residual = torch.mean(torch.square(res))\n",
        "        return loss_residual\n",
        "#         return res\n",
        "\n",
        "\n",
        "    def sample_parameter_posterior(self, num_samples=100):\n",
        "        prior_param_samples = torch.rand(num_samples, self.hidden_size, device=self.device).requires_grad_(True)\n",
        "        t1 = torch.tanh(self.t1(prior_param_samples))\n",
        "        t2 = torch.tanh(self.t2(t1))\n",
        "        posterior_param_samples = self.t3(t2)\n",
        "#         print(\"\\n posterior samples before mean and std\", posterior_param_samples)\n",
        "#         print(\"\\n\")\n",
        "        return posterior_param_samples\n",
        "\n",
        "\n",
        "\n",
        "    def update_predicted_params(self, posterior_samples):\n",
        "\n",
        "        mean = torch.mean(posterior_samples, dim=0)  # Compute the mean along the first axis\n",
        "        std = torch.std(posterior_samples, dim=0)    # Compute the standard deviation along the first axis\n",
        "\n",
        "        self.predicted_params = posterior_samples\n",
        "        self.mean_predicted_params = mean  # Store the mean\n",
        "        self.std_params = std  # Attach the standard deviation as an attribute\n",
        "\n",
        "\n",
        "    def closure(self):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss_ic, loss_interior, loss_data, loss_bc = self.compute_losses()\n",
        "        total_loss = self.w_ic * loss_ic \\\n",
        "                   + self.w_int * loss_interior \\\n",
        "                   + self.w_data * loss_data \\\n",
        "                   + self.w_bc * loss_bc\n",
        "\n",
        "        # Sample the parameter posterior and update self.predicted_params and self.std_params\n",
        "        posterior_samples = self.sample_parameter_posterior(num_samples=100)\n",
        "        self.update_predicted_params(posterior_samples)\n",
        "        print(\"posterior samples in closure after update: \", self.predicted_params)\n",
        "\n",
        "        # Add the (Log(std))**2 term to the loss with the specified weight\n",
        "        self.log_std_squared_loss = self.w_param_std * torch.mean(torch.log(self.std_params)**2)\n",
        "        total_loss += self.log_std_squared_loss\n",
        "\n",
        "        total_loss.backward(retain_graph=True)\n",
        "        return total_loss\n",
        "\n",
        "\n",
        "    def train(self, epochs, optimizer='Adam', num_samples=100, **kwargs):\n",
        "        if optimizer == 'Adam':\n",
        "            self.optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
        "\n",
        "        elif optimizer == 'L-BFGS':\n",
        "            self.optimizer = torch.optim.LBFGS(self.parameters(), **kwargs)\n",
        "\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            self.optimizer.step(self.closure)\n",
        "            if epoch % 1000 == 0:\n",
        "                loss_ic, loss_interior, loss_data, loss_bc = self.compute_losses()\n",
        "                total_loss = loss_ic + loss_interior + loss_data + loss_bc\n",
        "\n",
        "                scheduler.step(total_loss)\n",
        "\n",
        "                # Append losses to history\n",
        "                self.total_loss_history.append(total_loss.item())\n",
        "                self.loss_ic_history.append(loss_ic.item())\n",
        "                self.loss_interior_history.append(loss_interior.item())\n",
        "                self.loss_data_history.append(loss_data.item())\n",
        "                self.loss_bc_history.append(loss_bc.item())\n",
        "                self.loss_std_history.append(self.log_std_squared_loss.item())\n",
        "\n",
        "#                 print(f'Epoch({optimizer}):{epoch},  Total Loss:{total_loss.item():.2f}  ' \\\n",
        "#                         f'PDE Loss:{loss_interior.item():.2f}  ' \\\n",
        "#                         f'BC Loss:{loss_bc.item():.2f}  ' \\\n",
        "#                         f'IC Loss: {loss_ic.item():.2f}  ' \\\n",
        "#                         f'Predicted Param:{self.predicted_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
        "#                         f'Std Params:{self.std_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
        "#                         f'Std Log Loss:{self.log_std_squared_loss.item():.2f}')\n",
        "\n",
        "                print(f'Epoch({optimizer}):{epoch},  Total Loss:{total_loss.item():.2f}  ' \\\n",
        "                        f'PDE Loss:{loss_interior.item():.2f}  ' \\\n",
        "                        f'BC Loss:{loss_bc.item():.2f}  ' \\\n",
        "                        f'IC Loss: {loss_ic.item():.2f}  ' \\\n",
        "#                         f'Predicted Param:{self.predicted_params.detach().cpu().numpy()[0][0]:.2f}'\n",
        "                        f'Mean Predicted Param: {self.mean_predicted_params[0]:.2f} ' \\\n",
        "                        f'Std Params:{self.std_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
        "                        f'Std Log Loss:{self.log_std_squared_loss.item():.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qktqyo9XUm1y"
      },
      "outputs": [],
      "source": [
        "def sample_dataset(noise=0.1, numSamples=100, device='cpu'):\n",
        "    t = torch.linspace(0, 1, numSamples, device=device) # Time domain\n",
        "    x = torch.linspace(-1, 1, numSamples, device=device) # Space domain\n",
        "\n",
        "    T, X   = torch.meshgrid(t, x) # Time-Space domain\n",
        "\n",
        "    y_true = Experiment.exact_solution(T, X)\n",
        "    T      = T.reshape(-1, 1) # Reshape to 2D to 1D\n",
        "    X      = X.reshape(-1, 1) # Resahpe to 2D to 1D\n",
        "\n",
        "    sample_mean = y_true.reshape(-1, 1)\n",
        "    sample_var  = noise * torch.ones_like(sample_mean)\n",
        "    Y_noisy     = torch.normal(sample_mean, sample_var)\n",
        "\n",
        "    return T, X, Y_noisy\n",
        "\n",
        "\n",
        "def create_train_test_datasets(device='cpu', batch_size = 32):\n",
        "    t_train, x_train, y_train = sample_dataset(noise=0.01, numSamples=10, device=device)\n",
        "    t_test, x_test, y_test = sample_dataset(noise=0.0, numSamples=100, device=device)\n",
        "\n",
        "    fig = plt.figure()\n",
        "    size = int(t_test.size(0)**0.5)\n",
        "\n",
        "\n",
        "    plt.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
        "                 x_test.view(size, size).cpu().detach().numpy(),\n",
        "                 y_test.view(size, size).cpu().detach().numpy())\n",
        "    plt.xlabel('t'); plt.ylabel('x');\n",
        "    plt.colorbar()\n",
        "\n",
        "    train_dataset = TensorDataset(t_train, x_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_dataset = TensorDataset(t_test, x_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "def plot_residuals(net, device, noise=0.0):\n",
        "    t_test, x_test, y_test = sample_dataset(noise=noise, device=device)\n",
        "    mu   = net.forward(t_test, x_test)\n",
        "\n",
        "    fig  = plt.figure(figsize=(6, 3))\n",
        "    ax1  = plt.subplot(121)\n",
        "    size = int(np.sqrt(x_test.size(0)))\n",
        "\n",
        "    # Calculate the residuals (difference between predictions and ground truth)\n",
        "    absDifference  = torch.abs(mu - y_test)\n",
        "    residuals      = absDifference.view(size, size).cpu().detach().numpy()\n",
        "\n",
        "    # Create a contour plot of the residuals\n",
        "    cax = ax1.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
        "                       x_test.view(size, size).cpu().detach().numpy(),\n",
        "                       residuals)\n",
        "\n",
        "    plt.colorbar(cax)\n",
        "    plt.xlabel('t');plt.ylabel('x')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def make_plot(net, device):\n",
        "    t_test, x_test, y_test = sample_dataset(noise=0.0, device=device)\n",
        "    mu = net.forward(t_test, x_test)\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 3))\n",
        "    ax1 = plt.subplot(121)\n",
        "    ax2 = plt.subplot(122, sharex=ax1)\n",
        "    size = int(np.sqrt(x_test.size(0)))\n",
        "\n",
        "    ax1.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
        "                 x_test.view(size, size).cpu().detach().numpy(),\n",
        "                 mu.view(size, size).cpu().detach().numpy())\n",
        "\n",
        "    plt.xlabel('t')\n",
        "    plt.ylabel('x')\n",
        "\n",
        "    ax2.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
        "                 x_test.view(size, size).cpu().detach().numpy(),\n",
        "                 y_test.view(size, size).cpu().detach().numpy())\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_history(net):\n",
        "    epochs = range(len(net.total_loss_history))\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(epochs, net.total_loss_history, label='Total Loss')\n",
        "    plt.plot(epochs, net.loss_ic_history, label='IC Loss')\n",
        "    plt.plot(epochs, net.loss_interior_history, label='Interior Loss')\n",
        "    plt.plot(epochs, net.loss_data_history, label='Data Loss')\n",
        "    plt.plot(epochs, net.loss_bc_history, label='BC Loss')\n",
        "    plt.plot(epochs, net.loss_std_history, label='STD Loss')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_loss_history_log(net):\n",
        "    epochs = range(len(net.total_loss_history))\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.semilogy(epochs, net.total_loss_history, label='Total Loss')\n",
        "    plt.semilogy(epochs, net.loss_ic_history, label='IC Loss')\n",
        "    plt.semilogy(epochs, net.loss_interior_history, label='Interior Loss')\n",
        "    plt.semilogy(epochs, net.loss_data_history, label='Data Loss')\n",
        "    plt.semilogy(epochs, net.loss_bc_history, label='BC Loss')\n",
        "    plt.semilogy(epochs, net.loss_std_history, label='STD Loss')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss (log scale)')\n",
        "    plt.title('Loss Over Epochs (log scale)')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxdtBGzUUphv",
        "outputId": "14c393d6-dcf6-43c9-b49d-3dbdfc1df607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            "Params: 2103\n"
          ]
        }
      ],
      "source": [
        "net = Experiment()\n",
        "net.to(net.device)\n",
        "print(\"Params:\", sum(p.numel() for p in net.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "yvco7AmeUrOF",
        "outputId": "99c9a061-d64e-49f1-96b2-2cb9df4bca8f",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train_loader, test_loader = create_train_test_datasets(device = net.device, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivyI-KUlUsBV"
      },
      "outputs": [],
      "source": [
        "net.w_ic = 1\n",
        "net.w_data = 10\n",
        "net.w_bc   = 1\n",
        "net.w_int = 5\n",
        "net.w_param_std = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pXJ12CAPUvSp",
        "outputId": "7f4b368d-7262-4abc-db04-515a740040c4",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.0629, 0.8923, 0.3254, 0.3429, 0.3332, 0.8253, 0.1426, 0.3888, 0.5278,\n",
            "         0.6449, 0.0664, 0.3658, 0.7806, 0.1936, 0.7163, 0.3322, 0.0281, 0.0269,\n",
            "         0.3582, 0.8673, 0.3864, 0.7005, 0.1134, 0.8960, 0.7116]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.4498, 4.3211]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.5929, 0.3416, 0.5274, 0.5450, 0.4172, 0.2588, 0.0671, 0.8973, 0.4737,\n",
            "         0.5819, 0.5332, 0.8389, 0.4414, 0.0454, 0.0856, 0.8063, 0.4781, 0.3913,\n",
            "         0.0226, 0.1473, 0.4219, 0.6401, 0.0634, 0.9055, 0.0246]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.1722, 4.2977]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " \n",
            "\n",
            "X shape: torch.Size([100, 1])\n",
            "Y shape: torch.Size([100, 1])\n",
            "dy_dt shape: torch.Size([100, 1])\n",
            "d2y_dx2 shape: torch.Size([100, 1])\n",
            "predicted params torch.Size([1, 2])\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.0659, 0.3747, 0.7258, 0.8703, 0.0734, 0.0419, 0.7708, 0.3337, 0.0506,\n",
            "         0.8097, 0.6652, 0.7160, 0.8463, 0.4825, 0.1961, 0.1038, 0.1800, 0.4912,\n",
            "         0.6576, 0.2411, 0.4930, 0.1337, 0.8322, 0.6084, 0.2005]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.3743, 4.3170]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.0280, 0.4671, 0.8947, 0.5314, 0.7639, 0.0482, 0.8996, 0.8350, 0.0386,\n",
            "         0.4615, 0.1350, 0.9057, 0.4867, 0.2616, 0.5017, 0.6946, 0.9752, 0.1333,\n",
            "         0.5120, 0.3362, 0.9245, 0.7302, 0.2497, 0.2549, 0.7784]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.3220, 4.3867]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.2417, 0.8585, 0.8652, 0.0518, 0.6856, 0.9691, 0.3883, 0.2376, 0.0643,\n",
            "         0.5625, 0.7473, 0.3398, 0.5165, 0.0067, 0.5407, 0.4137, 0.7392, 0.3106,\n",
            "         0.4318, 0.0750, 0.3231, 0.8595, 0.8002, 0.8708, 0.8514]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.4042, 4.3802]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "posterior samples in closure after update:  tensor([[2.3946, 4.4064],\n",
            "        [2.4815, 4.3849],\n",
            "        [2.4243, 4.3879],\n",
            "        [2.2536, 4.3722],\n",
            "        [2.4218, 4.3226],\n",
            "        [2.2510, 4.3637],\n",
            "        [2.4322, 4.3767],\n",
            "        [2.4699, 4.3741],\n",
            "        [2.4681, 4.3982],\n",
            "        [2.4949, 4.3913],\n",
            "        [2.4383, 4.4045],\n",
            "        [2.4449, 4.3670],\n",
            "        [2.4033, 4.3028],\n",
            "        [2.4072, 4.4081],\n",
            "        [2.3635, 4.3433],\n",
            "        [2.4102, 4.3678],\n",
            "        [2.3604, 4.3839],\n",
            "        [2.4922, 4.3921],\n",
            "        [2.2900, 4.3334],\n",
            "        [2.3798, 4.3659],\n",
            "        [2.4667, 4.3665],\n",
            "        [2.2974, 4.3019],\n",
            "        [2.3193, 4.3907],\n",
            "        [2.2947, 4.4078],\n",
            "        [2.4206, 4.3764],\n",
            "        [2.5372, 4.4280],\n",
            "        [2.3695, 4.4199],\n",
            "        [2.4687, 4.4002],\n",
            "        [2.3937, 4.3874],\n",
            "        [2.3773, 4.3123],\n",
            "        [2.3281, 4.3187],\n",
            "        [2.2713, 4.3222],\n",
            "        [2.4398, 4.4251],\n",
            "        [2.4252, 4.3771],\n",
            "        [2.3774, 4.3945],\n",
            "        [2.4438, 4.4202],\n",
            "        [2.3621, 4.3474],\n",
            "        [2.3789, 4.3877],\n",
            "        [2.4820, 4.4296],\n",
            "        [2.3571, 4.3999],\n",
            "        [2.2119, 4.2539],\n",
            "        [2.2149, 4.3424],\n",
            "        [2.5202, 4.4172],\n",
            "        [2.3188, 4.2755],\n",
            "        [2.4570, 4.4199],\n",
            "        [2.2145, 4.3055],\n",
            "        [2.4244, 4.3745],\n",
            "        [2.4561, 4.3522],\n",
            "        [2.3404, 4.3446],\n",
            "        [2.2527, 4.3601],\n",
            "        [2.3356, 4.3601],\n",
            "        [2.3886, 4.3713],\n",
            "        [2.4117, 4.2994],\n",
            "        [2.3306, 4.3402],\n",
            "        [2.2418, 4.3669],\n",
            "        [2.3519, 4.3573],\n",
            "        [2.3266, 4.3543],\n",
            "        [2.3476, 4.3009],\n",
            "        [2.4348, 4.4453],\n",
            "        [2.2547, 4.2817],\n",
            "        [2.2138, 4.3086],\n",
            "        [2.4482, 4.4106],\n",
            "        [2.4320, 4.4035],\n",
            "        [2.3792, 4.4040],\n",
            "        [2.3069, 4.3749],\n",
            "        [2.4985, 4.4200],\n",
            "        [2.3286, 4.3726],\n",
            "        [2.3215, 4.3283],\n",
            "        [2.3490, 4.2911],\n",
            "        [2.2564, 4.3354],\n",
            "        [2.4557, 4.3929],\n",
            "        [2.3772, 4.4029],\n",
            "        [2.3667, 4.3241],\n",
            "        [2.4249, 4.4029],\n",
            "        [2.3704, 4.3555],\n",
            "        [2.4105, 4.4047],\n",
            "        [2.2962, 4.3820],\n",
            "        [2.3906, 4.3838],\n",
            "        [2.3620, 4.3968],\n",
            "        [2.4623, 4.3203],\n",
            "        [2.3985, 4.3593],\n",
            "        [2.3878, 4.2030],\n",
            "        [2.4290, 4.3820],\n",
            "        [2.3607, 4.3563],\n",
            "        [2.3573, 4.4007],\n",
            "        [2.4636, 4.3892],\n",
            "        [2.3198, 4.2958],\n",
            "        [2.3720, 4.3376],\n",
            "        [2.2474, 4.3672],\n",
            "        [2.2434, 4.2808],\n",
            "        [2.4662, 4.4224],\n",
            "        [2.2389, 4.3617],\n",
            "        [2.2769, 4.3136],\n",
            "        [2.3494, 4.3455],\n",
            "        [2.2667, 4.2976],\n",
            "        [2.5063, 4.3865],\n",
            "        [2.3553, 4.2678],\n",
            "        [2.1950, 4.3426],\n",
            "        [2.3472, 4.4079],\n",
            "        [2.3882, 4.3925]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.7762, 0.7617, 0.0572, 0.6184, 0.4568, 0.2266, 0.3621, 0.7889, 0.2654,\n",
            "         0.0969, 0.3093, 0.1740, 0.7030, 0.5357, 0.8020, 0.6932, 0.7947, 0.1755,\n",
            "         0.5093, 0.3786, 0.2230, 0.1240, 0.5397, 0.7320, 0.6019]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.6631, 4.4579]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.3013, 0.4826, 0.9478, 0.6844, 0.1164, 0.0890, 0.2363, 0.2719, 0.0762,\n",
            "         0.8135, 0.5135, 0.0232, 0.8270, 0.1001, 0.9075, 0.1121, 0.9560, 0.4917,\n",
            "         0.3825, 0.6273, 0.3821, 0.5602, 0.2648, 0.1342, 0.8859]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.4901, 4.3252]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " \n",
            "\n",
            "X shape: torch.Size([100, 1])\n",
            "Y shape: torch.Size([100, 1])\n",
            "dy_dt shape: torch.Size([100, 1])\n",
            "d2y_dx2 shape: torch.Size([100, 1])\n",
            "predicted params torch.Size([1, 2])\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.8300, 0.8735, 0.8184, 0.6354, 0.0411, 0.5537, 0.9354, 0.4827, 0.5696,\n",
            "         0.3237, 0.3428, 0.3475, 0.2186, 0.8424, 0.5489, 0.7150, 0.1215, 0.7934,\n",
            "         0.7810, 0.9808, 0.4282, 0.9965, 0.8022, 0.5414, 0.1233]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.8499, 4.5518]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.0281, 0.0086, 0.4346, 0.7953, 0.4408, 0.3597, 0.7087, 0.4783, 0.2930,\n",
            "         0.0292, 0.8186, 0.0836, 0.2167, 0.3474, 0.0061, 0.6920, 0.7116, 0.6024,\n",
            "         0.8773, 0.4057, 0.4407, 0.9197, 0.1427, 0.9696, 0.0568]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.4873, 4.3432]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.1400, 0.7178, 0.0886, 0.3072, 0.6704, 0.8611, 0.3243, 0.1358, 0.1342,\n",
            "         0.5395, 0.9154, 0.3945, 0.7647, 0.5215, 0.0230, 0.1538, 0.4800, 0.1800,\n",
            "         0.4468, 0.6971, 0.1769, 0.6419, 0.7151, 0.7657, 0.4369]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.6934, 4.3933]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "Epoch(Adam):0,  Total Loss:2.15  PDE Loss:1.38  BC Loss:0.46  IC Loss: 0.18  Mean Predicted Param: 2.37 Std Params:0.08  Std Log Loss:7.99\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.5310, 0.0359, 0.0181, 0.5906, 0.7185, 0.5468, 0.6793, 0.8494, 0.6857,\n",
            "         0.8497, 0.1004, 0.2102, 0.5255, 0.7740, 0.8643, 0.5850, 0.2153, 0.4255,\n",
            "         0.4731, 0.0397, 0.4851, 0.5194, 0.8436, 0.2612, 0.6399]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.5950, 4.4748]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.4176, 0.7303, 0.5882, 0.0195, 0.6807, 0.3750, 0.9383, 0.5403, 0.7425,\n",
            "         0.8995, 0.2746, 0.2068, 0.6665, 0.8189, 0.2060, 0.8576, 0.0491, 0.4060,\n",
            "         0.4072, 0.0658, 0.4340, 0.3830, 0.7700, 0.8775, 0.0386]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.7652, 4.4813]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " \n",
            "\n",
            "X shape: torch.Size([100, 1])\n",
            "Y shape: torch.Size([100, 1])\n",
            "dy_dt shape: torch.Size([100, 1])\n",
            "d2y_dx2 shape: torch.Size([100, 1])\n",
            "predicted params torch.Size([1, 2])\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.6735, 0.8086, 0.3381, 0.9541, 0.4519, 0.2678, 0.0417, 0.5991, 0.3133,\n",
            "         0.8438, 0.3761, 0.7030, 0.9493, 0.5542, 0.1951, 0.0370, 0.6617, 0.8161,\n",
            "         0.3873, 0.5262, 0.7381, 0.5865, 0.0425, 0.7774, 0.8912]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.4282, 4.4493]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.4167, 0.6814, 0.2369, 0.0513, 0.3096, 0.1631, 0.9647, 0.2030, 0.6245,\n",
            "         0.9549, 0.5274, 0.4592, 0.5367, 0.5625, 0.8389, 0.3755, 0.7336, 0.2457,\n",
            "         0.6148, 0.3594, 0.5135, 0.6149, 0.8357, 0.2662, 0.0755]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.7104, 4.4681]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.2052, 0.2644, 0.0753, 0.5510, 0.8029, 0.9962, 0.7875, 0.4161, 0.8921,\n",
            "         0.5934, 0.7816, 0.5693, 0.1482, 0.2467, 0.3777, 0.2741, 0.9886, 0.8735,\n",
            "         0.6657, 0.6998, 0.5719, 0.7993, 0.8705, 0.8407, 0.6228]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.6375, 4.4947]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "posterior samples in closure after update:  tensor([[2.5543, 4.4084],\n",
            "        [2.6665, 4.4293],\n",
            "        [2.5156, 4.5022],\n",
            "        [2.6748, 4.4760],\n",
            "        [2.7029, 4.3679],\n",
            "        [2.6339, 4.5253],\n",
            "        [2.5042, 4.4163],\n",
            "        [2.6741, 4.4658],\n",
            "        [2.7087, 4.4982],\n",
            "        [2.5503, 4.5008],\n",
            "        [2.6177, 4.3792],\n",
            "        [2.4714, 4.4611],\n",
            "        [2.6638, 4.5624],\n",
            "        [2.6451, 4.4272],\n",
            "        [2.5932, 4.5176],\n",
            "        [2.4627, 4.4250],\n",
            "        [2.5550, 4.4754],\n",
            "        [2.7634, 4.5719],\n",
            "        [2.7404, 4.4376],\n",
            "        [2.6553, 4.4991],\n",
            "        [2.6403, 4.4476],\n",
            "        [2.6703, 4.5728],\n",
            "        [2.6424, 4.4948],\n",
            "        [2.4651, 4.3599],\n",
            "        [2.5009, 4.4492],\n",
            "        [2.4776, 4.4321],\n",
            "        [2.6550, 4.5709],\n",
            "        [2.7122, 4.5284],\n",
            "        [2.4941, 4.3920],\n",
            "        [2.4333, 4.3513],\n",
            "        [2.5738, 4.5235],\n",
            "        [2.5646, 4.4364],\n",
            "        [2.6786, 4.4982],\n",
            "        [2.4451, 4.3228],\n",
            "        [2.4364, 4.4871],\n",
            "        [2.3967, 4.2950],\n",
            "        [2.5865, 4.4922],\n",
            "        [2.6995, 4.4193],\n",
            "        [2.7581, 4.5516],\n",
            "        [2.4595, 4.4959],\n",
            "        [2.4207, 4.3939],\n",
            "        [2.6270, 4.5097],\n",
            "        [2.7473, 4.4914],\n",
            "        [2.4475, 4.4503],\n",
            "        [2.7308, 4.4541],\n",
            "        [2.3560, 4.4360],\n",
            "        [2.5238, 4.4209],\n",
            "        [2.5829, 4.4677],\n",
            "        [2.4082, 4.3493],\n",
            "        [2.6227, 4.3337],\n",
            "        [2.3615, 4.3117],\n",
            "        [2.5777, 4.5231],\n",
            "        [2.7698, 4.3996],\n",
            "        [2.5084, 4.4057],\n",
            "        [2.4231, 4.4900],\n",
            "        [2.4813, 4.4302],\n",
            "        [2.4308, 4.4768],\n",
            "        [2.6869, 4.4233],\n",
            "        [2.6749, 4.4511],\n",
            "        [2.5232, 4.4737],\n",
            "        [2.7844, 4.4943],\n",
            "        [2.6345, 4.3900],\n",
            "        [2.7379, 4.5557],\n",
            "        [2.6103, 4.5763],\n",
            "        [2.7783, 4.5122],\n",
            "        [2.7668, 4.5355],\n",
            "        [2.6292, 4.4971],\n",
            "        [2.7586, 4.5311],\n",
            "        [2.7249, 4.5444],\n",
            "        [2.5178, 4.4635],\n",
            "        [2.6978, 4.4359],\n",
            "        [2.4513, 4.4582],\n",
            "        [2.5784, 4.5041],\n",
            "        [2.6675, 4.4772],\n",
            "        [2.7200, 4.4305],\n",
            "        [2.4868, 4.3482],\n",
            "        [2.7599, 4.5705],\n",
            "        [2.3986, 4.1259],\n",
            "        [2.5351, 4.4670],\n",
            "        [2.5506, 4.4464],\n",
            "        [2.6633, 4.4351],\n",
            "        [2.6089, 4.4199],\n",
            "        [2.7637, 4.4938],\n",
            "        [2.7009, 4.5032],\n",
            "        [2.7268, 4.5187],\n",
            "        [2.6672, 4.4473],\n",
            "        [2.5188, 4.4436],\n",
            "        [2.4056, 4.4088],\n",
            "        [2.5190, 4.4944],\n",
            "        [2.6750, 4.5370],\n",
            "        [2.1913, 4.3612],\n",
            "        [2.3710, 4.3461],\n",
            "        [2.5253, 4.3719],\n",
            "        [2.6854, 4.4953],\n",
            "        [2.7988, 4.4926],\n",
            "        [2.6639, 4.4292],\n",
            "        [2.5355, 4.4598],\n",
            "        [2.6312, 4.5062],\n",
            "        [2.7074, 4.5415],\n",
            "        [2.6271, 4.4312]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.9304, 0.6346, 0.4760, 0.2196, 0.8544, 0.9905, 0.9732, 0.5447, 0.5504,\n",
            "         0.4525, 0.4143, 0.1473, 0.4639, 0.1721, 0.4357, 0.8872, 0.3954, 0.1483,\n",
            "         0.3081, 0.5305, 0.4677, 0.5327, 0.0779, 0.8109, 0.9776]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.7728, 4.5431]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.0174, 0.9224, 0.5335, 0.3692, 0.9031, 0.2836, 0.5517, 0.9222, 0.3938,\n",
            "         0.5803, 0.4929, 0.2751, 0.7702, 0.4020, 0.4605, 0.5343, 0.2178, 0.9526,\n",
            "         0.8726, 0.3981, 0.0599, 0.8907, 0.4240, 0.5314, 0.5454]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.5603, 4.4010]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " \n",
            "\n",
            "X shape: torch.Size([100, 1])\n",
            "Y shape: torch.Size([100, 1])\n",
            "dy_dt shape: torch.Size([100, 1])\n",
            "d2y_dx2 shape: torch.Size([100, 1])\n",
            "predicted params torch.Size([1, 2])\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.3718, 0.6171, 0.4605, 0.7813, 0.0135, 0.2134, 0.4854, 0.4494, 0.7402,\n",
            "         0.1952, 0.1143, 0.3631, 0.6096, 0.0641, 0.7866, 0.2452, 0.7505, 0.0517,\n",
            "         0.6320, 0.4671, 0.4672, 0.1302, 0.8130, 0.5474, 0.2374]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.6954, 4.3015]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.7311, 0.0153, 0.7479, 0.1058, 0.1820, 0.5990, 0.1833, 0.6874, 0.2279,\n",
            "         0.5643, 0.0582, 0.0435, 0.1722, 0.5546, 0.3732, 0.8899, 0.1518, 0.3548,\n",
            "         0.2286, 0.2815, 0.7142, 0.4759, 0.6502, 0.9097, 0.6079]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.5278, 4.4183]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.0564, 0.9155, 0.9297, 0.4995, 0.8082, 0.1994, 0.5182, 0.6288, 0.5123,\n",
            "         0.7579, 0.3116, 0.8718, 0.9059, 0.7160, 0.3047, 0.9503, 0.8363, 0.6525,\n",
            "         0.0818, 0.1457, 0.9412, 0.2235, 0.1202, 0.1509, 0.4398]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.4426, 4.5130]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "posterior samples in closure after update:  tensor([[2.8350, 4.5499],\n",
            "        [2.5752, 4.4049],\n",
            "        [2.7718, 4.4872],\n",
            "        [2.5354, 4.5350],\n",
            "        [2.4821, 4.4934],\n",
            "        [2.6587, 4.5050],\n",
            "        [2.6126, 4.4384],\n",
            "        [2.5384, 4.5801],\n",
            "        [2.4197, 4.3659],\n",
            "        [2.6957, 4.5195],\n",
            "        [2.0020, 4.2714],\n",
            "        [2.5300, 4.3647],\n",
            "        [2.5467, 4.3610],\n",
            "        [2.3380, 4.3759],\n",
            "        [2.4624, 4.3353],\n",
            "        [2.8100, 4.3389],\n",
            "        [2.4036, 4.2991],\n",
            "        [2.6662, 4.4324],\n",
            "        [2.3864, 4.4759],\n",
            "        [2.3456, 4.1153],\n",
            "        [2.2889, 4.2903],\n",
            "        [2.7040, 4.5803],\n",
            "        [2.3906, 4.3857],\n",
            "        [2.5949, 4.4109],\n",
            "        [2.3370, 4.1171],\n",
            "        [2.7710, 4.4815],\n",
            "        [2.6777, 4.4777],\n",
            "        [2.8447, 4.5877],\n",
            "        [2.3519, 4.3980],\n",
            "        [2.6544, 4.4830],\n",
            "        [2.4406, 4.5109],\n",
            "        [2.6229, 4.3879],\n",
            "        [2.8774, 4.5399],\n",
            "        [2.4092, 4.4217],\n",
            "        [2.4736, 4.1755],\n",
            "        [2.5771, 4.5836],\n",
            "        [2.7194, 4.5817],\n",
            "        [2.5174, 4.4405],\n",
            "        [2.4530, 4.3244],\n",
            "        [2.4713, 4.4248],\n",
            "        [2.5334, 4.3840],\n",
            "        [2.6094, 4.4800],\n",
            "        [2.6988, 4.4058],\n",
            "        [2.4119, 4.5193],\n",
            "        [2.6762, 4.4595],\n",
            "        [2.6357, 4.3213],\n",
            "        [2.4968, 4.3992],\n",
            "        [2.6212, 4.5523],\n",
            "        [2.2548, 4.4099],\n",
            "        [2.6901, 4.3931],\n",
            "        [2.6204, 4.4456],\n",
            "        [2.1809, 4.3988],\n",
            "        [2.6902, 4.4184],\n",
            "        [2.4267, 4.5095],\n",
            "        [2.5777, 4.4808],\n",
            "        [2.5452, 4.4816],\n",
            "        [2.6727, 4.4695],\n",
            "        [2.4736, 4.2896],\n",
            "        [2.7211, 4.3625],\n",
            "        [2.6468, 4.2938],\n",
            "        [2.4735, 4.4905],\n",
            "        [2.5323, 4.4067],\n",
            "        [2.7223, 4.4928],\n",
            "        [2.3852, 4.5070],\n",
            "        [2.1856, 4.2843],\n",
            "        [2.5308, 4.2498],\n",
            "        [2.6855, 4.4405],\n",
            "        [2.4829, 4.5067],\n",
            "        [2.8137, 4.4002],\n",
            "        [2.7437, 4.4664],\n",
            "        [2.6235, 4.4896],\n",
            "        [2.6475, 4.4257],\n",
            "        [2.6738, 4.5739],\n",
            "        [2.5716, 4.4910],\n",
            "        [2.8715, 4.5544],\n",
            "        [2.7671, 4.4302],\n",
            "        [2.7096, 4.4520],\n",
            "        [2.6115, 4.4138],\n",
            "        [2.2937, 4.3363],\n",
            "        [2.5431, 4.3438],\n",
            "        [2.6858, 4.3892],\n",
            "        [2.6016, 4.4878],\n",
            "        [2.6886, 4.3625],\n",
            "        [2.8601, 4.4293],\n",
            "        [2.6615, 4.5748],\n",
            "        [2.4347, 4.3470],\n",
            "        [2.5843, 4.4153],\n",
            "        [2.4861, 4.4233],\n",
            "        [2.1457, 4.2302],\n",
            "        [2.4632, 4.5275],\n",
            "        [2.4340, 4.3982],\n",
            "        [2.7847, 4.3997],\n",
            "        [2.3764, 4.3460],\n",
            "        [2.6339, 4.3842],\n",
            "        [2.6402, 4.5061],\n",
            "        [2.1768, 4.2614],\n",
            "        [2.5855, 4.4381],\n",
            "        [2.6297, 4.4224],\n",
            "        [2.7812, 4.5423],\n",
            "        [2.4645, 4.3434]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.2408, 0.7531, 0.0129, 0.0712, 0.3415, 0.0154, 0.2287, 0.5099, 0.7521,\n",
            "         0.9916, 0.3385, 0.9416, 0.9416, 0.9809, 0.2368, 0.4245, 0.3847, 0.7813,\n",
            "         0.1843, 0.5696, 0.9766, 0.2616, 0.3532, 0.5407, 0.6601]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.2294, 4.3246]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.5103, 0.5416, 0.2280, 0.5129, 0.4282, 0.5043, 0.4962, 0.0422, 0.2216,\n",
            "         0.1027, 0.4577, 0.2295, 0.0856, 0.0429, 0.4323, 0.4001, 0.0820, 0.3034,\n",
            "         0.6315, 0.4847, 0.9683, 0.6703, 0.0319, 0.0631, 0.3435]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.2448, 4.1421]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " \n",
            "\n",
            "X shape: torch.Size([100, 1])\n",
            "Y shape: torch.Size([100, 1])\n",
            "dy_dt shape: torch.Size([100, 1])\n",
            "d2y_dx2 shape: torch.Size([100, 1])\n",
            "predicted params torch.Size([1, 2])\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.9073, 0.9555, 0.8698, 0.2311, 0.1296, 0.3181, 0.6735, 0.2175, 0.2855,\n",
            "         0.2049, 0.1933, 0.6896, 0.2842, 0.6181, 0.3384, 0.2481, 0.6209, 0.3287,\n",
            "         0.6556, 0.9055, 0.9613, 0.2843, 0.8546, 0.6203, 0.7429]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.7907, 4.5038]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.6906, 0.6804, 0.7363, 0.1255, 0.9719, 0.2998, 0.9789, 0.2934, 0.6913,\n",
            "         0.2623, 0.6665, 0.8673, 0.0921, 0.1657, 0.5425, 0.5131, 0.9789, 0.8104,\n",
            "         0.2238, 0.1578, 0.5563, 0.0939, 0.4456, 0.3168, 0.7826]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.2883, 4.3917]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "\n",
            " Forward prior param sample \n",
            " tensor([[0.6088, 0.8092, 0.2848, 0.0098, 0.9410, 0.1164, 0.1963, 0.7601, 0.4143,\n",
            "         0.6513, 0.6202, 0.1050, 0.0326, 0.1699, 0.1036, 0.2810, 0.8762, 0.7565,\n",
            "         0.3441, 0.7183, 0.9606, 0.8116, 0.1547, 0.6851, 0.6494]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n",
            " Forward posterior_param_sample: \n",
            " tensor([[2.0582, 4.1620]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "posterior samples in closure after update:  tensor([[2.2559, 4.2491],\n",
            "        [2.5239, 4.5069],\n",
            "        [2.1300, 4.0258],\n",
            "        [2.5009, 4.2426],\n",
            "        [2.4647, 4.3912],\n",
            "        [2.2806, 3.9484],\n",
            "        [2.0682, 4.3779],\n",
            "        [2.4463, 4.3466],\n",
            "        [2.4325, 4.4083],\n",
            "        [2.3559, 4.1286],\n",
            "        [2.1659, 4.1523],\n",
            "        [2.1612, 4.1100],\n",
            "        [2.4572, 4.1964],\n",
            "        [2.7250, 4.3793],\n",
            "        [2.7768, 4.3756],\n",
            "        [2.5814, 4.3941],\n",
            "        [2.4081, 4.0636],\n",
            "        [2.1534, 4.1110],\n",
            "        [2.3411, 4.2820],\n",
            "        [2.5993, 4.3446],\n",
            "        [2.4672, 4.1477],\n",
            "        [2.6190, 4.3171],\n",
            "        [2.5662, 4.0690],\n",
            "        [2.2575, 4.4452],\n",
            "        [2.7177, 4.3164],\n",
            "        [2.6550, 4.4213],\n",
            "        [2.5738, 4.3309],\n",
            "        [2.0626, 4.2648],\n",
            "        [2.2708, 4.2264],\n",
            "        [2.8323, 4.3990],\n",
            "        [2.4229, 4.3477],\n",
            "        [2.5155, 4.4932],\n",
            "        [2.0068, 3.8584],\n",
            "        [2.2434, 4.3837],\n",
            "        [2.1926, 4.2344],\n",
            "        [2.3378, 4.0457],\n",
            "        [2.6814, 4.4613],\n",
            "        [2.3930, 4.3315],\n",
            "        [2.5355, 4.4070],\n",
            "        [2.4717, 4.4616],\n",
            "        [2.3888, 4.2722],\n",
            "        [2.2878, 4.1736],\n",
            "        [2.5185, 4.2089],\n",
            "        [2.3904, 4.3628],\n",
            "        [2.4954, 4.4330],\n",
            "        [2.5477, 4.4343],\n",
            "        [2.7283, 4.4309],\n",
            "        [2.4576, 4.3681],\n",
            "        [2.5872, 4.2093],\n",
            "        [2.3191, 4.3283],\n",
            "        [2.1631, 3.9103],\n",
            "        [2.5519, 4.2673],\n",
            "        [2.1473, 4.0788],\n",
            "        [2.5501, 4.4594],\n",
            "        [2.1617, 4.4324],\n",
            "        [2.0792, 4.2236],\n",
            "        [2.5033, 4.2551],\n",
            "        [2.3471, 4.4813],\n",
            "        [2.6914, 4.5082],\n",
            "        [2.1730, 4.1782],\n",
            "        [2.4856, 4.3724],\n",
            "        [2.5129, 4.4761],\n",
            "        [2.1601, 4.3159],\n",
            "        [2.4043, 4.4809],\n",
            "        [2.5713, 4.4264],\n",
            "        [2.6391, 4.5259],\n",
            "        [2.8275, 4.4313],\n",
            "        [2.1017, 4.0574],\n",
            "        [2.4072, 4.4028],\n",
            "        [2.5445, 4.1651],\n",
            "        [2.6316, 4.3655],\n",
            "        [2.5604, 4.1048],\n",
            "        [2.3438, 4.0682],\n",
            "        [2.1589, 4.2522],\n",
            "        [2.4644, 4.4545],\n",
            "        [2.6939, 4.4888],\n",
            "        [2.3250, 4.2065],\n",
            "        [2.5813, 4.3943],\n",
            "        [2.5940, 4.2838],\n",
            "        [2.6242, 4.2969],\n",
            "        [2.3740, 4.2604],\n",
            "        [2.1062, 4.1188],\n",
            "        [1.9877, 4.2658],\n",
            "        [2.4512, 4.1468],\n",
            "        [2.2294, 4.0568],\n",
            "        [2.3105, 4.2267],\n",
            "        [2.7940, 4.5573],\n",
            "        [2.5882, 4.3546],\n",
            "        [2.0490, 4.0025],\n",
            "        [2.2687, 4.1094],\n",
            "        [2.7419, 4.4236],\n",
            "        [2.5191, 4.4020],\n",
            "        [2.3184, 4.1608],\n",
            "        [2.1804, 4.3543],\n",
            "        [2.6785, 4.3872],\n",
            "        [2.4830, 4.1355],\n",
            "        [2.7356, 4.5928],\n",
            "        [2.7046, 4.5343],\n",
            "        [2.3595, 4.2723],\n",
            "        [2.0683, 3.9987]], device='cuda:0', grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-d1a5980f7476>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-314e590a4c13>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, optimizer, num_samples, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mloss_ic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_interior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_bc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclosure\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-314e590a4c13>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_std_squared_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "net.train(25000, optimizer='Adam', lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW09rE71cgx-"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'model_state_dict': net.state_dict(),\n",
        "    'optimizer_state_dict': net.optimizer.state_dict(),\n",
        "    'total_loss_history': net.total_loss_history,\n",
        "    'loss_ic_history': net.loss_ic_history,\n",
        "    'loss_interior_history': net.loss_interior_history,\n",
        "    'loss_data_history': net.loss_data_history,\n",
        "    'loss_bc_history': net.loss_bc_history,\n",
        "    'loss_std_history': net.loss_std_history\n",
        "    # Add other variables if needed\n",
        "}, f'model_checkpoint_trained.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsZSJLXXUy0H"
      },
      "outputs": [],
      "source": [
        "make_plot(net, device=net.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHjKd828U01l"
      },
      "outputs": [],
      "source": [
        "plot_residuals(net, device=net.device, noise=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCvV8otacgx_"
      },
      "outputs": [],
      "source": [
        "plot_loss_history(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1lf-DIxcgx_"
      },
      "outputs": [],
      "source": [
        "plot_loss_history_log(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyC72D9mU3GD"
      },
      "outputs": [],
      "source": [
        "samples = net.sample_parameter_posterior(num_samples=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoeVq4xpU46Z"
      },
      "outputs": [],
      "source": [
        "samples.mean(axis=0), samples.std(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwqkcv3vcgx_"
      },
      "outputs": [],
      "source": [
        "p1, p2 = samples.cpu().detach().unbind(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdGgVRYjU6yW"
      },
      "outputs": [],
      "source": [
        "g  = sns.kdeplot(p1, fill=True, palette=\"crest\",\n",
        "                 alpha=.5, linewidth=1)\n",
        "\n",
        "\n",
        "g  = sns.kdeplot(p2, fill=True, palette=\"crest\",\n",
        "                 alpha=.5, linewidth=1)\n",
        "g.legend( ['D', 'Alpha'])\n",
        "\n",
        "# Histograms of individual parameters with adjusted line color\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['p1'], kde=True, linewidth=2, edgecolor='black')  # Change edgecolor here\n",
        "plt.title('Parameter 1 Distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['p2'], kde=True, linewidth=2, edgecolor='black')  # Change edgecolor here\n",
        "plt.title('Parameter 2 Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jDv4qKQcgx_"
      },
      "outputs": [],
      "source": [
        "samples = net.sample_parameter_posterior(num_samples=100).cpu().detach().numpy()\n",
        "\n",
        "df = pd.DataFrame(samples, columns=['Parameter 1', 'Parameter 2'])\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df['Parameter 1'], kde=True, linewidth=2, edgecolor='blue', color='black')\n",
        "plt.title('Parameter 1 Distribution')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(df['Parameter 2'], kde=True, linewidth=2, edgecolor='blue', color='black')\n",
        "plt.title('Parameter 2 Distribution')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYqtbvIPU8eG"
      },
      "outputs": [],
      "source": [
        "x = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
        "t = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
        "\n",
        "X, T = torch.meshgrid(x[:, 0], t[:, 0])\n",
        "\n",
        "y_true = Experiment.exact_solution(T, X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDn5pHYpcgx_"
      },
      "source": [
        "#### Loading the saved model from the checkpoint\n",
        "We can load the state of model from the saved checkpoint like shown below and also other necessary states like optimizer state, losses etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfXKqV3ocgyA"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('model_checkpoint_trained.pth')\n",
        "\n",
        "model = Experiment()  # Create an instance of your Experiment class\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.total_loss_history = checkpoint['total_loss_history']\n",
        "model.loss_ic_history = checkpoint['loss_ic_history']\n",
        "model.loss_interior_history = checkpoint['loss_interior_history']\n",
        "model.loss_data_history = checkpoint['loss_data_history']\n",
        "model.loss_bc_history = checkpoint['loss_bc_history']\n",
        "model.loss_std_history = checkpoint['loss_std_history']\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Load Optimizer's state if necessary\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49VuKDPpcgyA"
      },
      "source": [
        "#### Once we load the model, we can call the plot functions by passing the new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0hotxS-dcgyB"
      },
      "outputs": [],
      "source": [
        "make_plot(model, device)  # Call the make_plot method to plot the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfP2W3ixcgyB"
      },
      "outputs": [],
      "source": [
        "plot_loss_history_log(model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}