{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "hBc_Jy8VUgOI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from numpy.ma.core import outer\n",
    "from sys import stderr\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import scatter, figure\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(4, 4)})\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "class Experiment(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Experiment, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.train_loss_history = []\n",
    "        self.w_ic = 1.5\n",
    "        self.w_int = 1\n",
    "        self.w_data = 1\n",
    "        self.w_bc = 1.5\n",
    "        self.w_param_std = 1.0\n",
    "        self.numInputs = 2\n",
    "        self.numParams = 2\n",
    "        self.numOutputs = 1\n",
    "        self.hidden_size = 25\n",
    "\n",
    "        # Initialize history variables\n",
    "        self.total_loss_history = []\n",
    "        self.loss_ic_history = []\n",
    "        self.loss_interior_history = []\n",
    "        self.loss_data_history = []\n",
    "        self.loss_bc_history = []\n",
    "        self.loss_std_history = []\n",
    "\n",
    "        self.t0 = torch.tensor([0.0], requires_grad=True).to(self.device)\n",
    "        self.y0 = torch.tensor([1.0], requires_grad=True).to(self.device)\n",
    "\n",
    "\n",
    "#         self.predicted_params = torch.zeros((self.numParams,1), requires_grad=True, device=self.device)\n",
    "        self.predicted_params = torch.zeros((batch_size,2), requires_grad=True, device=self.device)\n",
    "#         print(\"predicted_params inside init\", self.predicted_params)\n",
    "\n",
    "        # We only have 1 input feature\n",
    "        self.b1 = nn.Linear(self.numInputs, self.hidden_size).to(self.device)\n",
    "        self.b2 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
    "        self.b3 = nn.Linear(self.hidden_size, self.numOutputs).to(self.device)\n",
    "\n",
    "        self.t1 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
    "        self.t2 = nn.Linear(self.hidden_size, self.hidden_size).to(self.device)\n",
    "        self.t3 = nn.Linear(self.hidden_size, self.numParams).to(self.device)\n",
    "\n",
    "\n",
    "    # make this static so that it can be called independently\n",
    "    @staticmethod\n",
    "    def exact_solution(t, x):\n",
    "        return torch.exp(-t) * torch.sin(torch.pi * x)\n",
    "\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        if not torch.is_tensor(t):\n",
    "            t = torch.from_numpy(t).float().to(self.device)\n",
    "\n",
    "        if not torch.is_tensor(x):\n",
    "\n",
    "            x = torch.from_numpy(x).float().to(self.device)\n",
    "\n",
    "\n",
    "        input = torch.cat((t, x), 1)\n",
    "\n",
    "        h1 = torch.tanh(self.b1(input))\n",
    "        h2 = torch.tanh(self.b2(h1))\n",
    "\n",
    "\n",
    "\n",
    "        prior_param_sample = torch.rand(batch_size, self.hidden_size , device=self.device).requires_grad_(True)\n",
    "        # print(\"\\n Forward prior param sample shape\\n\", prior_param_sample.shape)\n",
    "\n",
    "        # prior_param_sample = self.sample_parameter_posterior(num_samples=100)\n",
    "\n",
    "\n",
    "        t1 = torch.tanh(self.t1(prior_param_sample))\n",
    "        t2 = torch.tanh(self.t2(t1))\n",
    "        posterior_param_sample = self.t3(t2)\n",
    "\n",
    "\n",
    "        # print(\"\\n Forward posterior_param_sample: \\n\", posterior_param_sample.shape)\n",
    "        self.predicted_params = posterior_param_sample\n",
    "\n",
    "#         self.predicted_params = posterior_param_sample\n",
    "#         self.predicted_params = posterior_param_sample.view(-1)\n",
    "#         print(\"\\n predicted params inside forward\", self.predicted_params)\n",
    "\n",
    "        y = self.b3(torch.multiply(t2, h2))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    # General formula to compute the n-th order derivative of y = f(x) with respect to x\n",
    "    def compute_derivative(self, y, x, n):\n",
    "        if n == 0:\n",
    "            return y\n",
    "        else:\n",
    "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y, device= self.device),\n",
    "                                        create_graph=True, retain_graph=True,\n",
    "                                        allow_unused=True)[0]\n",
    "\n",
    "        return self.compute_derivative(dy_dx, x, n - 1)\n",
    "\n",
    "\n",
    "\n",
    "    def PDE_residual(self, t, x):\n",
    "        y = self.forward(t, x)\n",
    "        dy_dt = self.compute_derivative(y, t, 1)\n",
    "        dy_dx = self.compute_derivative(y, x, 1)\n",
    "        d2y_dx2 = self.compute_derivative(y, x, 2)\n",
    "\n",
    "        # print(\"\\n \\n\")\n",
    "        # print(\"X shape:\" , x.shape)\n",
    "        # print(\"Y shape:\" , y.shape)\n",
    "        # print(\"dy_dt shape:\" , dy_dt.shape)\n",
    "        # print(\"d2y_dx2 shape:\" , d2y_dx2.shape)\n",
    "        # print(\"predicted params\", self.predicted_params.shape)\n",
    "\n",
    "\n",
    "\n",
    "        residual =  dy_dt - torch.multiply(self.predicted_params[:,[0]], d2y_dx2) \\\n",
    "            + torch.exp(- self.predicted_params[:,[1]] * t) * (torch.sin(torch.tensor(np.pi) * x) - torch.tensor(np.pi) ** 2 * torch.sin(torch.tensor(np.pi) * x))\n",
    "\n",
    "\n",
    "\n",
    "        return residual\n",
    "\n",
    "\n",
    "\n",
    "    def loss_initial_condition(self, num_samples=100):\n",
    "        t0 = self.t0 * torch.ones((num_samples, 1), device = self.device)\n",
    "        x = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True)  - 1.0\n",
    "        y0_pred = self.forward(t0, x)\n",
    "        y0_true = self.exact_solution(t0, x)\n",
    "        loss_ic = torch.mean(torch.square(y0_true - y0_pred))\n",
    "        return loss_ic\n",
    "\n",
    "\n",
    "    def loss_boundary_condition(self, num_samples=100):\n",
    "        x_low = -1\n",
    "        x_high = 1\n",
    "        xb_low   = x_low * torch.ones((num_samples, 1),  device = self.device)\n",
    "        xb_high  = x_high * torch.ones((num_samples, 1), device = self.device)\n",
    "\n",
    "        t = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True) - 1.0\n",
    "\n",
    "        yb_low = self.exact_solution(t, xb_low)\n",
    "        yb_high = self.exact_solution(t, xb_high)\n",
    "\n",
    "        yb_pred_low = self.forward(t, xb_low)\n",
    "        yb_pred_high = self.forward(t, xb_high)\n",
    "\n",
    "        loss_ic = torch.mean(torch.square(yb_pred_low - yb_low)) \\\n",
    "                + torch.mean(torch.square(yb_pred_high - yb_high))\n",
    "\n",
    "        return loss_ic\n",
    "\n",
    "\n",
    "    def compute_losses(self):\n",
    "        loss_ic = self.loss_initial_condition()\n",
    "        loss_interior = self.loss_interior()\n",
    "        loss_data = self.loss_data()\n",
    "        loss_bc = self.loss_boundary_condition()\n",
    "        return loss_ic, loss_interior, loss_data, loss_bc\n",
    "\n",
    "\n",
    "    def loss_data(self, num_samples=100):\n",
    "        t_data, x_data, y_data = next(iter(train_loader))\n",
    "        y_pred = self.forward(t_data, x_data)\n",
    "        loss = torch.mean(torch.square(y_pred - y_data))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def loss_interior(self, num_samples=100):\n",
    "\n",
    "        interior_t_samples = torch.rand((num_samples, 1),   device=self.device).requires_grad_(True)\n",
    "        interior_x_samples = 2*torch.rand((num_samples, 1), device=self.device).requires_grad_(True)  - 1.0\n",
    "        res = self.PDE_residual(interior_t_samples, interior_x_samples)\n",
    "        loss_residual = torch.mean(torch.square(res))\n",
    "        return loss_residual\n",
    "#         return res\n",
    "\n",
    "\n",
    "    def sample_parameter_posterior(self, num_samples=100):\n",
    "        prior_param_samples = torch.rand(num_samples, self.hidden_size, device=self.device).requires_grad_(True)\n",
    "        t1 = torch.tanh(self.t1(prior_param_samples))\n",
    "        t2 = torch.tanh(self.t2(t1))\n",
    "        posterior_param_samples = self.t3(t2)\n",
    "        # print(\"\\n posterior samples size in sample param post function\", posterior_param_samples.shape)\n",
    "#         print(\"\\n\")\n",
    "        return posterior_param_samples\n",
    "\n",
    "\n",
    "\n",
    "    def update_predicted_params(self, posterior_samples):\n",
    "\n",
    "        mean = torch.mean(posterior_samples, dim=0)  # Compute the mean along the first axis\n",
    "        std = torch.std(posterior_samples, dim=0)    # Compute the standard deviation along the first axis\n",
    "\n",
    "        self.predicted_params = posterior_samples\n",
    "        self.mean_predicted_params = mean  # Store the mean\n",
    "        self.std_params = std  # Attach the standard deviation as an attribute\n",
    "\n",
    "\n",
    "    def closure(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_ic, loss_interior, loss_data, loss_bc = self.compute_losses()\n",
    "        total_loss = self.w_ic * loss_ic \\\n",
    "                   + self.w_int * loss_interior \\\n",
    "                   + self.w_data * loss_data \\\n",
    "                   + self.w_bc * loss_bc\n",
    "\n",
    "        # Sample the parameter posterior and update self.predicted_params and self.std_params\n",
    "        posterior_samples = self.sample_parameter_posterior(num_samples=100)\n",
    "        self.update_predicted_params(posterior_samples)\n",
    "        # print(\"posterior samples in closure after update: \", self.predicted_params)\n",
    "\n",
    "        # Add the (Log(std))**2 term to the loss with the specified weight\n",
    "        self.log_std_squared_loss = self.w_param_std * torch.mean(torch.log(self.std_params)**2)\n",
    "        total_loss += self.log_std_squared_loss\n",
    "\n",
    "        total_loss.backward(retain_graph=True)\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "    def train(self, epochs, optimizer='Adam', num_samples=100, **kwargs):\n",
    "        if optimizer == 'Adam':\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), **kwargs)\n",
    "\n",
    "        elif optimizer == 'L-BFGS':\n",
    "            self.optimizer = torch.optim.LBFGS(self.parameters(), **kwargs)\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.step(self.closure)\n",
    "            if epoch % 1000 == 0:\n",
    "                loss_ic, loss_interior, loss_data, loss_bc = self.compute_losses()\n",
    "                total_loss = loss_ic + loss_interior + loss_data + loss_bc\n",
    "\n",
    "                scheduler.step(total_loss)\n",
    "\n",
    "                # Append losses to history\n",
    "                self.total_loss_history.append(total_loss.item())\n",
    "                self.loss_ic_history.append(loss_ic.item())\n",
    "                self.loss_interior_history.append(loss_interior.item())\n",
    "                self.loss_data_history.append(loss_data.item())\n",
    "                self.loss_bc_history.append(loss_bc.item())\n",
    "                self.loss_std_history.append(self.log_std_squared_loss.item())\n",
    "\n",
    "#                 print(f'Epoch({optimizer}):{epoch},  Total Loss:{total_loss.item():.2f}  ' \\\n",
    "#                         f'PDE Loss:{loss_interior.item():.2f}  ' \\\n",
    "#                         f'BC Loss:{loss_bc.item():.2f}  ' \\\n",
    "#                         f'IC Loss: {loss_ic.item():.2f}  ' \\\n",
    "#                         f'Predicted Param:{self.predicted_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
    "#                         f'Std Params:{self.std_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
    "#                         f'Std Log Loss:{self.log_std_squared_loss.item():.2f}')\n",
    "\n",
    "                print(f'Epoch({optimizer}):{epoch},  Total Loss:{total_loss.item():.2f}  ' \\\n",
    "                        f'PDE Loss:{loss_interior.item():.2f}  ' \\\n",
    "                        f'BC Loss:{loss_bc.item():.2f}  ' \\\n",
    "                        f'IC Loss: {loss_ic.item():.2f}  ' \\\n",
    "#                         f'Predicted Param:{self.predicted_params.detach().cpu().numpy()[0][0]:.2f}'\n",
    "                        f'Mean Predicted Param: {self.mean_predicted_params[0]:.2f} ' \\\n",
    "                        f'Std Params:{self.std_params.detach().cpu().numpy()[0]:.2f}  ' \\\n",
    "                        f'Std Log Loss:{self.log_std_squared_loss.item():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "qktqyo9XUm1y"
   },
   "outputs": [],
   "source": [
    "def sample_dataset(noise=0.1, numSamples=100, device='cpu'):\n",
    "    t = torch.linspace(0, 1, numSamples, device=device) # Time domain\n",
    "    x = torch.linspace(-1, 1, numSamples, device=device) # Space domain\n",
    "\n",
    "    T, X   = torch.meshgrid(t, x) # Time-Space domain\n",
    "\n",
    "    y_true = Experiment.exact_solution(T, X)\n",
    "    T      = T.reshape(-1, 1) # Reshape to 2D to 1D\n",
    "    X      = X.reshape(-1, 1) # Resahpe to 2D to 1D\n",
    "\n",
    "    sample_mean = y_true.reshape(-1, 1)\n",
    "    sample_var  = noise * torch.ones_like(sample_mean)\n",
    "    Y_noisy     = torch.normal(sample_mean, sample_var)\n",
    "\n",
    "    return T, X, Y_noisy\n",
    "\n",
    "\n",
    "def create_train_test_datasets(device='cpu', batch_size = batch_size):\n",
    "    t_train, x_train, y_train = sample_dataset(noise=0.01, numSamples=10, device=device)\n",
    "    t_test, x_test, y_test = sample_dataset(noise=0.0, numSamples=100, device=device)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    size = int(t_test.size(0)**0.5)\n",
    "\n",
    "\n",
    "    plt.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
    "                 x_test.view(size, size).cpu().detach().numpy(),\n",
    "                 y_test.view(size, size).cpu().detach().numpy())\n",
    "    plt.xlabel('t'); plt.ylabel('x');\n",
    "    plt.colorbar()\n",
    "\n",
    "    train_dataset = TensorDataset(t_train, x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(t_test, x_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def plot_residuals(net, device, noise=0.0):\n",
    "    t_test, x_test, y_test = sample_dataset(noise=noise, device=device)\n",
    "    mu   = net.forward(t_test, x_test)\n",
    "\n",
    "    fig  = plt.figure(figsize=(6, 3))\n",
    "    ax1  = plt.subplot(121)\n",
    "    size = int(np.sqrt(x_test.size(0)))\n",
    "\n",
    "    # Calculate the residuals (difference between predictions and ground truth)\n",
    "    absDifference  = torch.abs(mu - y_test)\n",
    "    residuals      = absDifference.view(size, size).cpu().detach().numpy()\n",
    "\n",
    "    # Create a contour plot of the residuals\n",
    "    cax = ax1.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
    "                       x_test.view(size, size).cpu().detach().numpy(),\n",
    "                       residuals)\n",
    "\n",
    "    plt.colorbar(cax)\n",
    "    plt.xlabel('t');plt.ylabel('x')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_plot(net, device):\n",
    "    t_test, x_test, y_test = sample_dataset(noise=0.0, device=device)\n",
    "    mu = net.forward(t_test, x_test)\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 3))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122, sharex=ax1)\n",
    "    size = int(np.sqrt(x_test.size(0)))\n",
    "\n",
    "    ax1.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
    "                 x_test.view(size, size).cpu().detach().numpy(),\n",
    "                 mu.view(size, size).cpu().detach().numpy())\n",
    "\n",
    "    plt.xlabel('t')\n",
    "    plt.ylabel('x')\n",
    "\n",
    "    ax2.contourf(t_test.view(size, size).cpu().detach().numpy(),\n",
    "                 x_test.view(size, size).cpu().detach().numpy(),\n",
    "                 y_test.view(size, size).cpu().detach().numpy())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_history(net):\n",
    "    epochs = range(len(net.total_loss_history))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs, net.total_loss_history, label='Total Loss')\n",
    "    plt.plot(epochs, net.loss_ic_history, label='IC Loss')\n",
    "    plt.plot(epochs, net.loss_interior_history, label='Interior Loss')\n",
    "    plt.plot(epochs, net.loss_data_history, label='Data Loss')\n",
    "    plt.plot(epochs, net.loss_bc_history, label='BC Loss')\n",
    "    plt.plot(epochs, net.loss_std_history, label='STD Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_history_log(net):\n",
    "    epochs = range(len(net.total_loss_history))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.semilogy(epochs, net.total_loss_history, label='Total Loss')\n",
    "    plt.semilogy(epochs, net.loss_ic_history, label='IC Loss')\n",
    "    plt.semilogy(epochs, net.loss_interior_history, label='Interior Loss')\n",
    "    plt.semilogy(epochs, net.loss_data_history, label='Data Loss')\n",
    "    plt.semilogy(epochs, net.loss_bc_history, label='BC Loss')\n",
    "    plt.semilogy(epochs, net.loss_std_history, label='STD Loss')\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Loss Over Epochs (log scale)')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxdtBGzUUphv",
    "outputId": "14c393d6-dcf6-43c9-b49d-3dbdfc1df607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Params: 2103\n"
     ]
    }
   ],
   "source": [
    "net = Experiment()\n",
    "net.to(net.device)\n",
    "print(\"Params:\", sum(p.numel() for p in net.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "yvco7AmeUrOF",
    "outputId": "99c9a061-d64e-49f1-96b2-2cb9df4bca8f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, test_loader = create_train_test_datasets(device = net.device, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "ivyI-KUlUsBV"
   },
   "outputs": [],
   "source": [
    "net.w_ic = 1\n",
    "net.w_data = 10\n",
    "net.w_bc   = 1\n",
    "net.w_int = 5\n",
    "net.w_param_std = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pXJ12CAPUvSp",
    "outputId": "7f4b368d-7262-4abc-db04-515a740040c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(Adam):0,  Total Loss:31.76  PDE Loss:31.02  BC Loss:0.02  IC Loss: 0.52  Mean Predicted Param: 0.04 Std Params:0.05  Std Log Loss:8.66\n",
      "Epoch(Adam):1000,  Total Loss:0.23  PDE Loss:0.08  BC Loss:0.08  IC Loss: 0.04  Mean Predicted Param: 1.35 Std Params:0.57  Std Log Loss:0.22\n",
      "Epoch(Adam):2000,  Total Loss:0.12  PDE Loss:0.06  BC Loss:0.04  IC Loss: 0.01  Mean Predicted Param: 1.13 Std Params:0.54  Std Log Loss:0.24\n",
      "Epoch(Adam):3000,  Total Loss:0.08  PDE Loss:0.02  BC Loss:0.02  IC Loss: 0.03  Mean Predicted Param: 1.22 Std Params:0.76  Std Log Loss:0.05\n",
      "Epoch(Adam):4000,  Total Loss:0.17  PDE Loss:0.11  BC Loss:0.03  IC Loss: 0.01  Mean Predicted Param: 1.38 Std Params:0.90  Std Log Loss:0.01\n",
      "Epoch(Adam):5000,  Total Loss:0.05  PDE Loss:0.01  BC Loss:0.02  IC Loss: 0.01  Mean Predicted Param: 1.26 Std Params:1.01  Std Log Loss:0.01\n",
      "Epoch(Adam):6000,  Total Loss:0.06  PDE Loss:0.02  BC Loss:0.02  IC Loss: 0.01  Mean Predicted Param: 1.25 Std Params:0.92  Std Log Loss:0.00\n",
      "Epoch(Adam):7000,  Total Loss:0.06  PDE Loss:0.02  BC Loss:0.02  IC Loss: 0.01  Mean Predicted Param: 1.12 Std Params:0.66  Std Log Loss:0.13\n",
      "Epoch(Adam):8000,  Total Loss:0.08  PDE Loss:0.02  BC Loss:0.02  IC Loss: 0.03  Mean Predicted Param: 1.34 Std Params:0.80  Std Log Loss:0.04\n",
      "Epoch(Adam):9000,  Total Loss:0.04  PDE Loss:0.01  BC Loss:0.02  IC Loss: 0.01  Mean Predicted Param: 1.21 Std Params:0.82  Std Log Loss:0.03\n",
      "Epoch(Adam):10000,  Total Loss:0.09  PDE Loss:0.02  BC Loss:0.02  IC Loss: 0.03  Mean Predicted Param: 1.17 Std Params:0.72  Std Log Loss:0.09\n",
      "Epoch(Adam):11000,  Total Loss:0.09  PDE Loss:0.01  BC Loss:0.03  IC Loss: 0.02  Mean Predicted Param: 1.48 Std Params:0.82  Std Log Loss:0.03\n",
      "Epoch(Adam):12000,  Total Loss:0.10  PDE Loss:0.06  BC Loss:0.01  IC Loss: 0.02  Mean Predicted Param: 1.32 Std Params:0.92  Std Log Loss:0.00\n",
      "Epoch(Adam):13000,  Total Loss:0.07  PDE Loss:0.02  BC Loss:0.02  IC Loss: 0.02  Mean Predicted Param: 1.30 Std Params:0.91  Std Log Loss:0.00\n",
      "Epoch(Adam):14000,  Total Loss:0.04  PDE Loss:0.01  BC Loss:0.02  IC Loss: 0.00  Mean Predicted Param: 1.27 Std Params:0.84  Std Log Loss:0.02\n",
      "Epoch(Adam):15000,  Total Loss:0.13  PDE Loss:0.05  BC Loss:0.03  IC Loss: 0.03  Mean Predicted Param: 1.45 Std Params:0.89  Std Log Loss:0.01\n",
      "Epoch(Adam):16000,  Total Loss:0.24  PDE Loss:0.06  BC Loss:0.08  IC Loss: 0.05  Mean Predicted Param: 1.87 Std Params:1.19  Std Log Loss:0.09\n",
      "Epoch(Adam):17000,  Total Loss:0.06  PDE Loss:0.01  BC Loss:0.02  IC Loss: 0.02  Mean Predicted Param: 1.27 Std Params:0.83  Std Log Loss:0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m25000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 258\u001b[0m, in \u001b[0;36mExperiment.train\u001b[0;34m(self, epochs, optimizer, num_samples, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    260\u001b[0m         loss_ic, loss_interior, loss_data, loss_bc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_losses()\n",
      "File \u001b[0;32m~/.pyenv/versions/torch/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/torch/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.pyenv/versions/torch/lib/python3.11/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 143\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    146\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[59], line 227\u001b[0m, in \u001b[0;36mExperiment.closure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 227\u001b[0m     loss_ic, loss_interior, loss_data, loss_bc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_ic \u001b[38;5;241m*\u001b[39m loss_ic \\\n\u001b[1;32m    229\u001b[0m                \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_int \u001b[38;5;241m*\u001b[39m loss_interior \\\n\u001b[1;32m    230\u001b[0m                \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_data \u001b[38;5;241m*\u001b[39m loss_data \\\n\u001b[1;32m    231\u001b[0m                \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_bc \u001b[38;5;241m*\u001b[39m loss_bc\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# Sample the parameter posterior and update self.predicted_params and self.std_params\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[59], line 181\u001b[0m, in \u001b[0;36mExperiment.compute_losses\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_losses\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    180\u001b[0m     loss_ic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_initial_condition()\n\u001b[0;32m--> 181\u001b[0m     loss_interior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_interior\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     loss_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_data()\n\u001b[1;32m    183\u001b[0m     loss_bc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_boundary_condition()\n",
      "Cell \u001b[0;32mIn[59], line 198\u001b[0m, in \u001b[0;36mExperiment.loss_interior\u001b[0;34m(self, num_samples)\u001b[0m\n\u001b[1;32m    196\u001b[0m interior_t_samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((num_samples, \u001b[38;5;241m1\u001b[39m),   device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    197\u001b[0m interior_x_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mrand((num_samples, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m--> 198\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPDE_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterior_t_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterior_x_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m loss_residual \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msquare(res))\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_residual\n",
      "Cell \u001b[0;32mIn[59], line 130\u001b[0m, in \u001b[0;36mExperiment.PDE_residual\u001b[0;34m(self, t, x)\u001b[0m\n\u001b[1;32m    128\u001b[0m dy_dt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_derivative(y, t, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    129\u001b[0m dy_dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_derivative(y, x, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m d2y_dx2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# print(\"\\n \\n\")\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# print(\"X shape:\" , x.shape)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# print(\"Y shape:\" , y.shape)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# print(\"dy_dt shape:\" , dy_dt.shape)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# print(\"d2y_dx2 shape:\" , d2y_dx2.shape)\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# print(\"predicted params\", self.predicted_params.shape)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m residual \u001b[38;5;241m=\u001b[39m  dy_dt \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultiply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_params[:,[\u001b[38;5;241m0\u001b[39m]], d2y_dx2) \\\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_params[:,[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m*\u001b[39m t) \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39msin(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m x) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msin(torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mpi) \u001b[38;5;241m*\u001b[39m x))\n",
      "Cell \u001b[0;32mIn[59], line 122\u001b[0m, in \u001b[0;36mExperiment.compute_derivative\u001b[0;34m(self, y, x, n)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     dy_dx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(y, x, torch\u001b[38;5;241m.\u001b[39mones_like(y, device\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    119\u001b[0m                                 create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m                                 allow_unused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdy_dx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 118\u001b[0m, in \u001b[0;36mExperiment.compute_derivative\u001b[0;34m(self, y, x, n)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     dy_dx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_derivative(dy_dx, x, n \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/torch/lib/python3.11/site-packages/torch/autograd/__init__.py:394\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    390\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    391\u001b[0m         grad_outputs_\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    405\u001b[0m         output\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (output, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, t_inputs)\n\u001b[1;32m    409\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "net.train(25000, optimizer='Adam', lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "yW09rE71cgx-"
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': net.state_dict(),\n",
    "    'optimizer_state_dict': net.optimizer.state_dict(),\n",
    "    'total_loss_history': net.total_loss_history,\n",
    "    'loss_ic_history': net.loss_ic_history,\n",
    "    'loss_interior_history': net.loss_interior_history,\n",
    "    'loss_data_history': net.loss_data_history,\n",
    "    'loss_bc_history': net.loss_bc_history,\n",
    "    'loss_std_history': net.loss_std_history\n",
    "    # Add other variables if needed\n",
    "}, f'model_checkpoint_trained.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsZSJLXXUy0H"
   },
   "outputs": [],
   "source": [
    "make_plot(net, device=net.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHjKd828U01l"
   },
   "outputs": [],
   "source": [
    "plot_residuals(net, device=net.device, noise=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCvV8otacgx_"
   },
   "outputs": [],
   "source": [
    "plot_loss_history(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1lf-DIxcgx_"
   },
   "outputs": [],
   "source": [
    "plot_loss_history_log(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyC72D9mU3GD"
   },
   "outputs": [],
   "source": [
    "samples = net.sample_parameter_posterior(num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AoeVq4xpU46Z"
   },
   "outputs": [],
   "source": [
    "samples.mean(axis=0), samples.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xwqkcv3vcgx_"
   },
   "outputs": [],
   "source": [
    "p1, p2 = samples.cpu().detach().unbind(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdGgVRYjU6yW"
   },
   "outputs": [],
   "source": [
    "g  = sns.kdeplot(p1, fill=True, palette=\"crest\",\n",
    "                 alpha=.5, linewidth=1)\n",
    "\n",
    "\n",
    "g  = sns.kdeplot(p2, fill=True, palette=\"crest\",\n",
    "                 alpha=.5, linewidth=1)\n",
    "g.legend( ['D', 'Alpha'])\n",
    "\n",
    "# Histograms of individual parameters with adjusted line color\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['p1'], kde=True, linewidth=2, edgecolor='black')  # Change edgecolor here\n",
    "plt.title('Parameter 1 Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['p2'], kde=True, linewidth=2, edgecolor='black')  # Change edgecolor here\n",
    "plt.title('Parameter 2 Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jDv4qKQcgx_"
   },
   "outputs": [],
   "source": [
    "samples = net.sample_parameter_posterior(num_samples=100).cpu().detach().numpy()\n",
    "\n",
    "df = pd.DataFrame(samples, columns=['Parameter 1', 'Parameter 2'])\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df['Parameter 1'], kde=True, linewidth=2, edgecolor='blue', color='black')\n",
    "plt.title('Parameter 1 Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df['Parameter 2'], kde=True, linewidth=2, edgecolor='blue', color='black')\n",
    "plt.title('Parameter 2 Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYqtbvIPU8eG"
   },
   "outputs": [],
   "source": [
    "x = torch.linspace(-1, 1, 100).reshape(-1, 1)\n",
    "t = torch.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "X, T = torch.meshgrid(x[:, 0], t[:, 0])\n",
    "\n",
    "y_true = Experiment.exact_solution(T, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDn5pHYpcgx_"
   },
   "source": [
    "#### Loading the saved model from the checkpoint\n",
    "We can load the state of model from the saved checkpoint like shown below and also other necessary states like optimizer state, losses etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfXKqV3ocgyA"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model_checkpoint_trained.pth')\n",
    "\n",
    "model = Experiment()  # Create an instance of your Experiment class\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.total_loss_history = checkpoint['total_loss_history']\n",
    "model.loss_ic_history = checkpoint['loss_ic_history']\n",
    "model.loss_interior_history = checkpoint['loss_interior_history']\n",
    "model.loss_data_history = checkpoint['loss_data_history']\n",
    "model.loss_bc_history = checkpoint['loss_bc_history']\n",
    "model.loss_std_history = checkpoint['loss_std_history']\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Load Optimizer's state if necessary\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49VuKDPpcgyA"
   },
   "source": [
    "#### Once we load the model, we can call the plot functions by passing the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hotxS-dcgyB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_plot(model, device)  # Call the make_plot method to plot the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfP2W3ixcgyB"
   },
   "outputs": [],
   "source": [
    "plot_loss_history_log(model)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
